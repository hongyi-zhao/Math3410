{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">3.1<\/span> <span class=\"title\">Orthogonal sets of vectors<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-326\">You may recall from elementary linear algebra, or a calculus class, that vectors in $\\R^2$ or $\\R^3$ are considered to be quantities with both <em class=\"emphasis\">magnitude<\/em> and <em class=\"emphasis\">direction<\/em>. Interestingly enough, neither of these properties is inherent to a general vector space. The vector space axioms specify only algebra; they say nothing about geometry. (What, for example, should be the “angle” between two polynomials?)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-327\">Because vector algebra is often introduced as a consequence of geometry (like the “tip-to-tail” rule), you may not have thought all that carefully about what, exactly, is responsible for making the connection between algebra and geometry. It turns out that the missing link is the humble dot product. This should be plausible after a bit of thought. After all, you probably encountered the following result, perhaps as a consequence of the law of cosines: for any two vectors $\\uu,\\vv\\in\\R^2\\text{,}$<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\uu\\dotp\\vv = \\len{\\uu}\\,\\len{\\vv}\\cos\\theta\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $\\theta$ is the angle between $\\uu$ and $\\vv\\text{.}$ Here we see both magnitude and direction (encoded by the angle) defined in terms of the dot product.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-328\">While it is possible to generalize the idea of the dot product to something called an <em class=\"emphasis\">inner product<\/em>, we will first focus on the basic dot product in $\\R^n\\text{.}$ Once we have a good understanding of things in that setting, we can move on to consider the abstract counterpart.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Subsection<\/span> <span class=\"codenumber\">3.1.1<\/span> <span class=\"title\">Basic definitions and properties<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-329\">For most of this chapter (primarily for typographical reasons) we will denote elements of $\\R^n$ as ordered $n$-tuples $(x_1,\\ldots, x_n)$ rather than as column vectors.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-dot-prod-norm\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-330\">Let $\\xx=(x_1,x_2,\\ldots, x_n)$ and $\\yy=(y_1,y_2,\\ldots, y_n)$ be vectors in $\\R^n\\text{.}$ The <dfn class=\"terminology\">dot product<\/dfn> of $\\xx$ and $\\yy\\text{,}$ denoted by $\\xx\\dotp\\yy$ is the scalar defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx\\dotp \\yy = x_1y_1+x_2y_2+\\cdots + x_ny_n\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">The <dfn class=\"terminology\">norm<\/dfn> of a vector $\\xx$ is denoted $\\len{\\xx}$ and defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\xx} = \\sqrt{x_1^2+x_2^2+\\cdots + x_n^2}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-331\">Note that both the dot product and the norm produce <em class=\"emphasis\">scalars<\/em>. Through the Pythagorean Theorem, we recognize the norm as the length of $\\xx\\text{.}$ The dot product can still be thought of as measuring the angle between vectors, although the simple geometric proof used in two dimensions is not that easily translated to $n$ dimensions. At the very least, the dot product lets us extend the notion of right angles to higher dimensions.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-orthogonal\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-332\">We say that two vectors $\\xx,\\yy\\in\\R^n$ are <dfn class=\"terminology\">orthogonal<\/dfn> if $\\xx\\dotp\\yy = 0\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-333\">It should be no surprise that all the familiar properties of the dot product work just as well in any dimension. The folowing properties are all easily confirmed by routine computation.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-dot-props\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-334\">For any vectors $\\xx,\\yy,\\zz\\in\\R^n\\text{,}$<\/p><ol class=\"decimal\"><li id=\"li-84\">$\\displaystyle \\xx\\dotp\\yy = \\yy\\dotp\\xx$<\/li><li id=\"li-85\">$\\displaystyle \\xx\\dotp(\\yy+\\zz)=\\xx\\dotp\\yy+\\xx\\dotp\\zz$<\/li><li id=\"li-86\"><p id=\"p-335\">For any scalar $c\\text{,}$ $\\xx\\dotp(c\\yy) = (c\\xx)\\dotp\\yy=c(\\xx\\dotp\\yy)$<\/p><\/li><li id=\"li-87\"><p id=\"p-336\">$\\xx\\dotp\\xx\\geq 0\\text{,}$ and $\\xx\\dotp\\xx=0$ if and only if $\\xx=\\mathbf{0}$<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-337\">The above properties, when properly abstracted, become the defining properties of a (real) inner product. (A complex inner product also involves complex conjugates.) For a general inner product, the requirement $\\xx\\dotp\\xx\\geq 0$ is referred to as being <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">positive-definite<\/em>, and the property that only the zero vector produces zero when dotted with itself is called <em class=\"emphasis\">nondegenerate<\/em>. Note that we have the following connection between norm and dot product:<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\xx}^2 = \\xx\\dotp \\xx\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">For a general inner product, this can be used as a <em class=\"emphasis\">definition<\/em> of the norm associated to an inner product.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-26\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-338\">Given that $\\len{\\xx}=3, \\len{\\yy}=1\\text{,}$ and $\\xx\\dotp\\yy=-2\\text{,}$ compute $(4\\xx-3\\yy)\\dotp (\\xx+5\\yy)\\text{.}$<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-21\" id=\"solution-21\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-21\"><div class=\"solution solution-like\"><p id=\"p-339\">Note that the distributive property, together with symmetry, let us handle this dot product using what is essentially “<abbr class=\"initialism\">FOIL<\/abbr>”:<\/p><div class=\"displaymath\">\n\\begin{align*}\n(4\\xx-3\\yy)\\dotp (\\xx+5\\yy)\\amp = (4\\xx)\\dotp \\xx+(4\\xx)\\dotp(5\\yy)+(-3\\yy)\\dotp \\xx+(-3\\yy)\\dotp(5\\yy)\\\\\n\\amp = 4(\\xx\\dotp\\xx)+(4\\cdot 5)(\\xx\\dotp \\yy)-3(\\yy\\dotp \\xx)+(-3\\cdot 5)(\\yy\\dotp\\yy)\\\\\n\\amp = 4\\len{\\xx}^2+20\\xx\\dotp\\yy-3\\xx\\dotp\\yy-15\\len{\\yy}^2\\\\\n\\amp = 4(9)+17(-2)-15(1) = -13\\text{.}\n\\end{align*}\n<\/div><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"ex-norm-sum-square\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-340\">Show that for any vectors $\\xx,\\yy\\in\\R^n\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\xx+\\yy}^2 = \\len{\\xx}^2+2\\xx\\dotp\\yy+\\len{\\yy}^2\\text{.}\n\\end{equation*}\n<\/div><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-22\" id=\"solution-22\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-22\"><div class=\"solution solution-like\"><p id=\"p-341\">This is simply an exercise in properties of the dot product. We have<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\len{\\xx+\\yy}^2 \\amp = (\\xx+\\yy)\\dotp (\\xx+\\yy) \\\\\n\\amp = \\xx\\dotp \\xx+\\xx\\dotp\\yy+\\yy\\dotp\\xx+\\yy\\dotp\\yy\\\\\n\\amp =\\len{\\xx}^2+2\\xx\\dotp\\yy+\\len{\\yy}^2\\text{.}\n\\end{align*}\n<\/div><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-28\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.6<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-342\">Suppose $\\mathbb{R}^n=\\spn\\{\\vv_1,\\vv_2,\\ldots, \\vv_k\\}\\text{.}$ Prove that $\\xx=\\mathbf{0}$ if and only if $\\xx\\dotp \\vv_i=0$ for each $i=1,2,\\ldots, k\\text{.}$<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-23\" id=\"solution-23\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-23\"><div class=\"solution solution-like\"><p id=\"p-343\">If $\\xx=\\mathbf{0}\\text{,}$ then the result follows immediately from the dot product formula in <a href=\"sec-orthogonal-sets.ipynb#def-dot-prod-norm\" class=\"internal\" title=\"Definition 3.1.1\">Definition 3.1.1<\/a>. Conversely, suppose $\\xx\\dotp \\vv_i=0$ for each $i\\text{.}$ Since the $\\vv_i$ span $\\R^n\\text{,}$ there must exist scalars $c_1,c_2,\\ldots, c_k$ such that $\\xx=c_1\\vv_1+c_2\\vv_2+\\cdots+c_k\\vv_k\\text{.}$ But then<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\xx\\dotp\\xx \\amp = \\xx\\dotp (c_1\\vv_1+c_2\\vv_2+\\cdots+c_k\\vv_k) \\\\\n\\amp = c_1(\\xx\\dotp \\vv_1)+ c_2(\\xx\\dotp \\vv_2)+\\cdots +c_k(\\xx\\dotp \\vv_k)\\\\\n\\amp = c_1(0)+c_2(0)+\\cdots + c_k(0)=0\\text{.}\n\\end{align*}\n<\/div><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-344\">There are two important inequalities associated to the dot product and norm. We state them both in the following theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-cauchy-triangle\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.7<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-345\">Let $\\xx,\\yy$ be any vectors in $\\R^n\\text{.}$ Then<\/p><ol class=\"decimal\"><li id=\"li-88\">$\\displaystyle \\lvert \\xx\\dotp \\yy\\rvert \\leq \\len{\\xx}\\len{\\yy}$<\/li><li id=\"li-89\">$\\displaystyle \\len{\\xx+\\yy}\\leq \\len{\\xx}+\\len{\\yy}$<\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-346\">The first of the above inequalities is called the <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">Cauchy-Schwarz inequality<\/em>, which be viewed as a manifestation of the formula<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx\\dotp \\yy = \\len{\\xx}\\len{\\yy}\\cos\\theta\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">since after all, $\\lvert \\cos\\theta\\rvert\\leq 1$ for any angle $\\theta\\text{.}$ For a direct proof, we note that for any scalars $r,s$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{r\\xx\\pm s\\yy}^2 = r^2\\len{\\xx}^2\\pm 2rs\\xx\\dotp\\yy+s^2\\len{\\yy}^2\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Putting $r=\\len{\\yy}, s=\\len{\\xx}$ gives<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{s\\xx\\pm r\\yy}^2 = 2rs(rs\\pm\\xx\\dotp\\yy)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Since the left-hand side is non-negative, we must have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx\\yy\\leq \\len{\\xx}\\len{\\yy} \\quad \\text{ and } -\\len{\\xx}\\len{\\yy}\\leq \\xx\\dotp\\yy\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and the result follows.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-347\">The second result, called the  <em class=\"emphasis\">triangle inequality<\/em>, follows immediately from the Cauchy-Scwarz inequality and <a href=\"sec-orthogonal-sets.ipynb#ex-norm-sum-square\" class=\"internal\" title=\"Exercise 3.1.5\">Exercise 3.1.5<\/a>. The triangle inequality gets its name from the “tip-to-tail” picture for vector addition. Essentially, it tells us that the length of any side of a triangle must be less than the sum of the lengths of the other two sides. The importance of the triangle inequality is that it tells us that the norm can be used to define distance.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-vector-distance\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.8<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-348\">For any vectors $\\xx,\\yy\\in \\R^n\\text{,}$ the <dfn class=\"terminology\">distance<\/dfn> from $\\xx$ to $\\yy$ is denoted $d(\\xx,\\yy)\\text{,}$ and defined as<\/p><div class=\"displaymath\">\n\\begin{equation*}\nd(\\xx,\\yy) = \\len{\\xx-\\yy}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-349\">Using properties of the norm, we can show that this distance function meets the criteria of what's called a <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">metric<\/em>. A metric is any function that takes a pair of vectors (or points) as input, and returns a number as output, with the following properties:<\/p><ol class=\"decimal\"><li id=\"li-90\"><p id=\"p-350\">$d(\\xx,\\yy)=d(\\yy,\\xx)$ for any $\\xx,\\yy$<\/p><\/li><li id=\"li-91\"><p id=\"p-351\">$d(\\xx,\\yy)\\geq 0\\text{,}$ and $d(\\xx,\\yy)=0$ if and only if $\\xx=\\yy$<\/p><\/li><li id=\"li-92\"><p id=\"p-352\">$d(\\xx,\\yy)\\leq d(\\xx,\\zz)+d(\\zz,\\yy)$ for any $\\xx,\\yy,\\zz$<\/p><\/li><\/ol><p data-braille=\"continuation\">We leave it as an exercise to confirm that the distance function defined above is a metric.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-353\">In more advanced courses (e.g. topology or analysis) you might go into detailed study of these structures. There are three interrelated structures: inner products, norms, and metrics. You might consider questions like: does every norm come from an inner product? Does every metric come from a norm? (No.) Things get even more interesting for infinite-dimensional spaces. Of special interest are spaces such as <em class=\"emphasis\">Hilbert spaces<\/em> (a special type of infinite-dimensional inner product space) and <em class=\"emphasis\">Banach spaces<\/em> (a special type of infinite-dimensional normed space).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Subsection<\/span> <span class=\"codenumber\">3.1.2<\/span> <span class=\"title\">Orthogonal sets of vectors<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-354\">In earlier chapters, we've seen that among different sets of vectors one could consider, independent sets and spanning sets are both worthy of study. One of the main themes of this chapter is that <em class=\"emphasis\">orthogonal<\/em> sets are equally worthy, and in many cases, easier to work with.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-ortho-set\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.9<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-355\">A set of vectors $\\{\\vv_1,\\vv_2,\\ldots, \\vv_k\\}$ in $\\R^n$ is called <dfn class=\"terminology\">orthogonal<\/dfn> if:<\/p><ul class=\"disc\"><li id=\"li-93\"><p id=\"p-356\">$\\vv_i\\neq \\mathbf{0}$ for each $i=1,2\\ldots, n$<\/p><\/li><li id=\"li-94\"><p id=\"p-357\">$\\vv_i\\dotp\\vv_j = 0$ for all $i\\neq j$<\/p><\/li><\/ul><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"ex-orthogonal-set\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.10<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-358\">Show that the following is an orthogonal subset of $\\R^4\\text{.}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\{(1,0,1,0), (-1,0,1,1), (1,1,-1,2)\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Can you find a fourth vector that is orthogonal to each vector in this set?<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-24\" id=\"solution-24\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-24\"><div class=\"solution solution-like\"><p id=\"p-359\">Clearly, all three vectors are nonzero. To confirm the set is orthogonal, we simply compute dot products:<\/p><div class=\"displaymath\">\n\\begin{align*}\n(1,0,1,0)\\dotp (-1,0,1,1)\\amp =-1+0+1+0=0\\\\\n(-1,0,1,1)\\dotp (1,1,-1,2)\\amp =-1+0-1+2=0\\\\\n(1,0,1,0)\\dotp (1,1,-1,2) \\amp = 1+0-1+0=0\\text{.}\n\\end{align*}\n<\/div><p id=\"p-360\">To find a fourth vector, we proceed as follows. Let $\\xx=(a,b,c,d)\\text{.}$ We want $\\xx$ to be orthogonal to the three vectors in our set. Computing dot products, we must have:<\/p><div class=\"displaymath\">\n\\begin{align*}\n(a,b,c,d)\\dotp (1,0,1,0) \\amp = a+c=0 \\\\\n(a,b,c,d)\\dotp (-1,0,1,1) \\amp = -a+c+d=0 \\\\\n(a,b,c,d)\\dotp (1,1,-1,2) \\amp = a+b-c+2d=0\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">This is simply a homogeneous system of three equations in four variables. Using the Sage cell below, we find that our vector must satisfy $a=\\frac12 d, b = -3d, c=-\\frac12 d\\text{.}$<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA=Matrix(3,4,[1,0,1,0,-1,0,1,1,1,1,-1,2])\nA.rref()"], "outputs":[]}<p id=\"p-361\">One possible nonzero solution is to take $d=2\\text{,}$ giving $\\xx=(1,-6,-1,2)\\text{.}$ We'll leave the verification that this vector works as an easy exercise.<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-362\">The requirement that the vectors in an orthogonal set be nonzero is partly because the alternative would be boring, and partly because it lets us state the following theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-ortho-independent\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.11<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-363\">Any orthogonal set of vectors is linearly independent.<\/p><\/article><article class=\"proof\" id=\"proof-15\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-364\">Suppose $S=\\{\\vv_1,\\vv_2,\\ldots, \\vv_k\\}$ is orthogonal, and suppose<\/p><div class=\"displaymath\">\n\\begin{equation*}\nc_1\\vv_1+c_2\\vv_2+\\cdots + c_k\\vv_k = \\mathbf{0}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">for scalars $c_1,c_2,\\ldots, c_k\\text{.}$ Taking the dot product of both sides of the above equation with $\\vv_1$ gives<\/p><div class=\"displaymath\">\n\\begin{align*}\nc_1(\\vv_1\\dotp \\vv_1)+c_2(\\vv_1\\dotp \\vv_2)+\\cdots +c_k(\\vv_1\\dotp \\vv_k) \\amp =\\vv_1\\dotp \\mathbf{0}\\\\\nc_1\\len{\\vv_1}^2+0+\\cdots + 0\\amp = 0 \\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">Since $\\len{\\vv_1}^2\\neq 0\\text{,}$ we must have $c_1=0\\text{.}$ We similarly find that all the remaining scalars are zero by taking the dot product with $\\vv_2,\\ldots, \\vv_k\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-365\">Another useful consequence of orthogonality: recall that in two dimensions, we have the Pythagorean Theorem for right-angled triangles, but have to settle for the Law of Cosines otherwise. In $n$ dimensions, we have the following, which follows from the fact that all “cross terms” (dot products of different vectors) will vanish.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-pythagoras\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.12<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-366\">For any orthogonal set of vectors $\\{\\xx_1,\\ldots, \\xx_k\\}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\xx_1+\\cdots +\\xx_k}^2 = \\len{\\xx_1}^2+\\cdots + \\len{\\xx_k}^2\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-367\">Our final initial result about orthogonal sets of vectors relates to span. In general, we know that if $\\yy\\in\\spn\\{\\xx_1,\\ldots, \\xx_k\\}\\text{,}$ then it is possible to solve for scalars $c_1,\\ldots, c_k$ such that $\\yy=c_1\\xx_1+\\cdots+ c_k\\xx_k\\text{.}$ The trouble is that finding these scalars generally involves setting up, and then solving, a system of linear equations. The great thing about orthogonal sets of vectors is that we can provide explicit formulas for the scalars.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-fourier-expansion\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.13<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Fourier expansion theorem.<\/span><\/h6><p id=\"p-368\">Let $=\\{\\vv_1,\\vv_2,\\ldots, \\vv_k\\}$ be an orthogonal set of vectors. For any $\\yy\\in \\spn S\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\yy = \\left(\\frac{\\yy\\dotp\\mathbf{v}_1}{\\vv_1\\dotp\\vv_1}\\right)\\vv_1+\n\\left(\\frac{\\yy\\dotp\\mathbf{v}_2}{\\vv_2\\dotp\\vv_2}\\right)\\vv_2+\\cdots +\n\\left(\\frac{\\yy\\dotp\\mathbf{v}_k}{\\vv_k\\dotp\\vv_k}\\right)\\vv_k\\text{.}\n\\end{equation*}\n<\/div><\/article><article class=\"proof\" id=\"proof-16\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-369\">Let $\\yy=c_1\\vv_1+\\cdots + c_k\\vv_k\\text{.}$ Taking the dot product of both sides of this equation with $\\vv_i$ gives<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vv_i\\dotp\\yy = c_i(\\vv_i\\dotp\\vv_i)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">since the dot product of $\\vv_i$ with $\\vv_j$ for $i\\neq j$ is zero.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-370\">One use of <a href=\"sec-orthogonal-sets.ipynb#thm-fourier-expansion\" class=\"internal\" title=\"Theorem 3.1.13: Fourier expansion theorem\">Theorem 3.1.13<\/a> is determining whether or not a given vector is in the span of an orthogonal set. If it is in the span, then its coefficients must satisfy the Fourier expansion formula. Therefore, if we compute the right hand side of the above formula and do not get our original vector, then that vector must not be in the span.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"ex-test-span\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.14<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-371\">Determine whether or not the vectors $\\vv=(1,-4,3,-11), \\ww=(3,1,-4,2)$ belong to the span of the vectors $\\xx_1=(1,0,1,0), \\xx_2=(-1,0,1,1), \\xx_3=(1,1,-1,2)\\text{.}$ (We confirmed that these vectors form an orthogonal set in <a href=\"sec-orthogonal-sets.ipynb#ex-orthogonal-set\" class=\"internal\" title=\"Exercise 3.1.10\">Exercise 3.1.10<\/a>.)<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-25\" id=\"solution-25\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-25\"><div class=\"solution solution-like\"><p id=\"p-372\">We compute<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\left(\\frac{\\vv\\dotp\\xx_1}{\\len{\\xx_1}^2}\\right)\\mathbf{x}_1\n\\amp +\\left(\\frac{\\vv\\dotp\\xx_2}{\\len{\\xx_2}^2}\\right)\\mathbf{x}_2\n+\\left(\\frac{\\vv\\dotp\\xx_3}{\\len{\\xx_3}^2}\\right)\\mathbf{x}_3\\\\\n\\amp = \\frac{4}{2}\\xx_1+\\frac{-9}{3}\\xx_2+\\frac{-28}{7}\\\\\n\\amp = 2(1,0,1,0)-3(-1,0,1,1)-4(1,1,-1,2)\\\\\n\\amp = (1,-4,3,-11) = \\vv\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">so $\\vv\\in\\spn\\{\\xx_1,\\xx_2,\\xx_3\\}\\text{.}$<\/p><p id=\"p-373\">On the other hand, repeating the same calculation with $\\ww\\text{,}$ we find<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\left(\\frac{\\vv\\dotp\\xx_1}{\\len{\\xx_1}^2}\\right)\\xx_1\n\\amp +\\left(\\frac{\\vv\\dotp\\xx_2}{\\len{\\xx_2}^2}\\right)\\xx_2\n+\\left(\\frac{\\vv\\dotp\\xx_3}{\\len{\\xx_3}^2}\\right)\\xx_3\\\\\n\\amp =\\frac12 (1,0,1,0)-\\frac53 (-1,0,1,1) +\\frac47 (1,1,-1,2)\\\\\n\\amp = \\left(\\frac{73}{42},\\frac47,-\\frac{115}{42},-\\frac{11}{21}\\right)\\neq \\ww\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">so $\\ww\\notin\\spn\\{\\xx_1,\\xx_2,\\xx_3\\}\\text{.}$<\/p><p id=\"p-374\">Soon, we'll see that the quantity we computed when showing that $\\ww\\notin\\spn\\{\\xx_1,\\xx_2,\\xx_3\\}$ is, in fact, the <em class=\"emphasis\">orthogonal projection<\/em> of $\\ww$ onto the subspace $\\spn\\{\\xx_1,\\xx_2,\\xx_3\\}\\text{.}$<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-375\">The Fourier expansion is especially simple if our basis vectors have norm one, since the denominators in each coefficient disappear. Recall (from elementary linear algebra) that for any nonzero vector $\\vv\\text{,}$ a <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">unit vector<\/em> (that is, a vector of norm one) in the direction of $\\vv$ is given by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\hat{u} = \\frac{1}{\\len{\\vv}}\\vv\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">We often say that the vector $\\uu$ is <em class=\"emphasis\">normalized<\/em>. (The convention of using a “hat” for unit vectors is common but not universal.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-onb\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.15<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-376\">A basis $B$ of $\\R^n$ is called an <dfn class=\"terminology\">orthonormal basis<\/dfn> if $B$ is orthogonal, and all the vectors in $B$ are unit vectors.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"example example-like\" id=\"example-5\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Example<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.1.16<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-377\">In <a href=\"sec-orthogonal-sets.ipynb#ex-orthogonal-set\" class=\"internal\" title=\"Exercise 3.1.10\">Exercise 3.1.10<\/a> we saw that the set<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\{(1,0,1,0), (-1,0,1,1), (1,1,-1,2),(1,-6,-1,2)\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">is orthogonal. Since it's orthogonal, it must be independent, and since it's a set of four independent vectors in $\\R^4\\text{,}$ it must be a basis. To get an orthonormal basis, we normalize each vector:<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\hat{u}_1 \\amp = \\frac{1}{\\sqrt{1^2+0^2+1^2+0^2}}(1,0,1,0) = \\frac{1}{\\sqrt{2}}(1,0,1,0)\\\\\n\\hat{u}_2 \\amp = \\frac{1}{\\sqrt{(-1)^2+0^2+1^2+1^2}}(-1,0,1,1,) = \\frac{1}{\\sqrt{3}}(-1,0,1,1)\\\\\n\\hat{u}_3 \\amp = \\frac{1}{\\sqrt{1^2+1^2+(-1)^2+2^2}}(1,1,-1,2) = \\frac{1}{\\sqrt{7}}(1,1,-1,2)\\\\\n\\hat{u}_4 \\amp = \\frac{1}{\\sqrt{1^2+(-6)^2+(-1)^2+2^2}}(1,-6,-1,2) = \\frac{1}{\\sqrt{42}}(1,-6,-1,2)\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">The set $\\{\\hat{u}_1,\\hat{u}_2,\\hat{u}_3,\\hat{u}_4\\}$ is then an orthonormal basis of $\\R^4\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-378\">The process of creating unit vectors does typically introduce square root coefficients in our vectors. This can seem undesirable, but there remains value in having an orthonormal basis. For example, suppose we wanted to write the vector $\\vv=(3,5,-1,2)$ in terms of our basis. We can quickly compute<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{align*}\n\\vv\\dotp\\hat{u}_1 \\amp = \\frac{3}{\\sqrt{2}}-\\frac{1}{\\sqrt{2}}=\\sqrt{2}\\\\\n\\vv\\dotp\\hat{u}_2 \\amp = -\\frac{3}{\\sqrt{3}}-\\frac{1}{\\sqrt{3}}+\\frac{2}{\\sqrt{3}}=-\\frac{2}{\\sqrt{3}}\\\\\n\\vv\\dotp\\hat{u}_3 \\amp = \\frac{3}{\\sqrt{7}}+\\frac{5}{\\sqrt{7}}+\\frac{1}{\\sqrt{7}}+\\frac{4}{\\sqrt{7}} = \\frac{11}{\\sqrt{7}}\\\\\n\\vv\\dotp\\hat{u}_4 \\amp = \\frac{3}{\\sqrt{42}}-\\frac{30}{\\sqrt{42}}+\\frac{1}{\\sqrt{42}}+\\frac{4}{\\sqrt{42}} = -\\frac{22}{\\sqrt{42}}\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">and so<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vv = \\sqrt{2}\\hat{u}_1-\\frac{2}{\\sqrt{3}}\\hat{u}_2+\\frac{11}{\\sqrt{7}}\\hat{u}_3-\\frac{22}{\\sqrt{42}}\\hat{u}_4\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">There's still work to be done, but it is comparatively simpler than solving the corresponding system of equations.<\/p><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-orthogonal-sets.ipynb"}
}