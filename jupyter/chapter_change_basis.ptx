<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-change-basis">
  <title>Change of Basis</title>
  <section xml:id="sec-matrix-of-transformation">
    <title>The matrix of a linear transformation</title>
    <p>
      Recall from <xref ref="ex-matrix-trans">Example</xref> in <xref ref="ch-linear-trans">Chapter</xref>
      that given any <m>m\times n</m> matrix <m>A</m>, we can define the matrix transformation
      <m>T_A:\R^n\to \R^m</m> by <m>T_A(\xx)=A\xx</m>,
      where we view <m>\xx\in\R^n</m> as an <m>n\times 1</m> column vector.
    </p>

    <p>
      Conversely, given any linear map <m>T:\R^n\to \R^m</m>,
      if we let <m>\basis{e}{n}</m> denote the standard basis of <m>\R^n</m>,
      then the matrix
      <me>
        A = \bbm T(\mathbf{e}_1) \amp T(\mathbf{e}_2) \amp \cdots \amp T(\mathbf{e}_n)\ebm
      </me>
      is such that <m>T=T_A</m>.
    </p>

    <p>
      We have already discussed the fact that this idea generalizes:
      given a linear transformation <m>T:V\to W</m>,
      where <m>V</m> and <m>W</m> are finite-dimensional vector spaces,
      it is possible to represent <m>T</m> as a matrix transformation.
    </p>

    <p>
      The representation depends on choices of bases for both <m>V</m> and <m>W</m>.
      Recall the definition of the coefficient isomorphism,
      from <xref ref="def-coefficient-iso">Definition</xref> in <xref ref="sec-isomorphism">Section</xref>.
      If <m>\dim V=n</m> and <m>\dim W=m</m>,
      this gives us isomorphisms <m>C_B:V\to \R^n</m> and <m>C_D:W\to \R^m</m>
      depending on the choice of a basis <m>B</m> for <m>V</m> and a basis <m>D</m> for <m>W</m>.
      These isomorphisms define a matrix transformation <m>T_A:\R^n\to \R^m</m>
      according to the diagram we gave in <xref ref="fig_transformation_matrix">Figure</xref>.
    </p>

    <p>
      We should stress one important point about the coefficient isomorphism, however.
      It depends on the choice of basis, but also on the <em>order</em> of the basis elements.
      Thus, we generally will work with an <em>ordered basis</em> in this chapter.
      That is, rather than simply thinking of our basis as a set,
      we will think of it as an ordered list.
      Order matters, since given a basis <m>B=\basis{e}{n}</m>,
      we rely on the fact that we can write any vector <m>\vv</m> uniquely as
      <me>
        \vv = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n
      </me>
      in order to make the assignment <m>C_B(\vv) = \bbm c_1\\\vdots \\c_n\ebm</m>.
    </p>

    <exercise>
      <statement>
        <p>
          Show that the coefficient isomorphism is, indeed,
          a linear isomorphism from <m>V</m> to <m>\R^n</m>.
        </p>
      </statement>
      <solution>
        <p>
          It's clear that <m>C_B(\mathbf{0})=\vec{0}</m>,
          since the only way to write the zero vector in <m>V</m> in terms of <m>B</m>
          (or, indeed, any independent set) is to set all the scalars equal to zero.
        </p>

        <p>
          If we have two vectors <m>\vv,\ww</m> given by
          <md>
            <mrow>\vv \amp = a_1\mathbf{e}_1+\cdots + a_n\mathbf{e}_n </mrow>
            <mrow>\ww \amp = b_1\mathbf{e}_1+\cdots + b_n\mathbf{e}_n</mrow>
          </md>,
          then
          <me>
            \vv+\ww = (a_1+b_1)\mathbf{e}_1+\cdots + (a_n+b_n)\mathbf{e}_n
          </me>,
          so
          <md>
            <mrow>C_B(\vv+\ww) \amp = \bbm a_1+b_1\\\vdots \\ a_n+b_n\ebm </mrow>
            <mrow> \amp = \bbm a_1\\\vdots\\a_n\ebm +\bbm b_1\\\vdots \\b_n\ebm</mrow>
            <mrow>  \amp = C_B(\vv)+C_B(\ww)</mrow>
          </md>.
        </p>

        <p>
          Finally, for any scalar <m>c</m>, we have
          <md>
            <mrow>C_B(c\vv) \amp = C_B((ca_1)\mathbf{e}_1+\cdots +(ca_n)\mathbf{e}_n)</mrow>
            <mrow> \amp = \bbm ca_1\\\vdots \\ca_n\ebm</mrow>
            <mrow>  \amp =c\bbm a_1\\\vdots \\a_n\ebm</mrow>
            <mrow>  \amp =cC_B(\vv)</mrow>
          </md>.
        </p>

        <p>
          This shows that <m>C_B</m> is linear.
          To see that <m>C_B</m> is an isomorphism, we can simply note that <m>C_B</m>
          takes the basis <m>B</m> to the standard basis of <m>\R^n</m>.
          Alternatively, we can give the inverse: <m>C_B^{-1}:\R^n\to V</m> is given by
          <me>
            C_B^{-1}\bbm c_1\\\vdots \\c_n\ebm = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n
          </me>.
        </p>
      </solution>
    </exercise>

    <p>
      Given <m>T:V\to W</m> and coefficient isomorphisms <m>C_B:V\to \R^n, C_D:W\to \R^m</m>,
      the map <m>C_DTC_B^{-1}:\R^n\to \R^m</m> is a linear transformation,
      and the matrix of this transformation gives a representatioon of <m>T</m>.
      Explicitly, let <m>B = \basis{v}{n}</m> be an ordered basis for <m>V</m>,
      and let <m>D=\basis{w}{m}</m> be an ordered basis for <m>W</m>.
      Since <m>T(\vv_i)\in W</m> for each <m>\vv_i\in B</m>,
      there exist unique scalars <m>a_{ij}</m>, with <m>1\leq i\leq m</m>
      and <m>1\leq j\leq n</m> such that
      <me>
        T(\vv_j) = a_{1j}\ww_1+a_{2j}\ww_2+\cdots + a_{mj}\ww_m
      </me>
      for <m>j=1,\ldots, n</m>. This gives us the <m>m\times n</m> matrix <m>A = [a_{ij}]</m>.
      Notice that the first column of <m>A</m> is <m>C_D(T(\vv_1))</m>,
      the second column is <m>C_D(T(\vv_2))</m>, and so on.
    </p>

    <p>
      Given <m>\xx\in V</m>, write <m>\xx = c_1\vv_1+\cdots + c_n\vv_n</m>,
      so that <m>C_B(\xx) = \bbm c_1\\\vdots \\c_n\ebm</m>. Then
      <me>
        T_A(C_B(\xx)) = \bbm a_{11}\amp a_{12} \amp \cdots \amp a_{1n}\\
                             a_{21}\amp a_{22} \amp \cdots \amp a_{2n}\\
                             \vdots \amp \vdots \amp \ddots \amp \vdots\\
                             a_{m1}\amp a_{m2} \amp \cdots \amp a_{mn}\ebm\bbm c_1\\c_2\\ \vdots \\c_n\ebm
                             = \bbm a_{11}c_1+a_{12}c_2+\cdots +a_{1n}c_n\\
                                    a_{21}c_1+a_{22}c_2+\cdots +a_{2n}c_n\\
                                    \vdots
                                    a_{m1}c_1+a_{m2}c_2+\cdots +a_{mn}c_n\ebm
      </me>.
    </p>

    <p>
      On the other hand,
      <md>
        <mrow>T(\xx) \amp = T(c_1\vv_1+\cdots + c_n\vv_n) </mrow>
        <mrow> \amp = c_1T(\vv_1)+\cdots + c_nT(\vv_n)</mrow>
        <mrow> \amp = c_1(a_{11}\ww_1+\cdots + a_{m1}\ww_m)+\cdots c_n(a_{1n}\ww_1+\cdots + a_{mn}\ww_m)</mrow>
        <mrow> \amp = (c_1a_{11}+\cdots + c_na_{1n})\ww_1 + \cdots + (c_1a_{m1}+\cdots + c_na_{mn})\ww_m</mrow>
      </md>.
      Therefore,
      <me>
        C_D(T(\xx)) = \bbm c_1a_{11}+\cdots + c_na_{1n}\\ \vdots \\ c_1a_{m1}+\cdots + c_na_{mn}\ebm = T_A(C_B(\xx))
      </me>.
      Thus, we see that <m>C_DT = T_AC_B</m>, or <m>T_A = C_DTC_B^{-1}</m>, as expected.
    </p>

    <definition xml:id="def-transformation-matrix">
      <title>The matrix <m>M_{DB}(T)</m> of a linear map</title>

      <statement>
        <p>
          Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces,
          and let <m>T:V\to W</m> be a linear map.
          Let <m>B=\basis{v}{n}</m> and <m>D=\basis{w}{m}</m> be ordered bases for <m>V</m> and <m>W</m>,
          respectively. Then the <term>matrix</term> <m>M_{DB}(T)</m> of <m>T</m> with respect to the bases <m>B</m> and <m>D</m>
          is defined by
          <me>
            M_{DB}(T) = \bbm C_D(T(\vv_1)) \amp C_D(T(\vv_2)) \amp \cdots \amp C_D(T(\vv_n))\ebm
          </me>.
        </p>
      </statement>
    </definition>

    <p>
      In other words, <m>A=M_{DB}(T)</m> is the unique <m>m\times n</m> matrix such that <m>C_DT = T_AC_B</m>.
      This gives the defining property
      <me>
        C_D(T(\vv)) = M_{DB}(T)C_B(\vv)  \text{ for all } \vv\in V
      </me>,
      as was demonstrated above.
    </p>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:P_2(\R)\to \R^2</m> is given by
          <me>
            T(a+bx+cx^2) = (a+c,2b)
          </me>.
          Compute the matrix of <m>T</m> with respect to the bases
          <m>B = \{1,1-x,(1-x)^2\}</m> of <m>P_2(\R)</m> and
          <m>D = \{(1,0),(1,-1)\}</m> of <m>\R^2</m>.
        </p>
      </statement>
      <solution>
        <p>
          We have
          <md>
            <mrow>T(1) \amp = (1,0) = 1(1,0)+0(1,-1) </mrow>
            <mrow>T(1-x) \amp= (1,-2) = -1(1,0)+2(1,-1) </mrow>
            <mrow> T((1-x)^2) = T(1-2+x^2) \amp = (2, -4) = -2(1,0)+4(1,-1)</mrow>
          </md>.
          Thus,
          <md>
            <mrow>M_{DB}(T) \amp = \bbm C_D(T(1))\amp C_D(T(1-x)) \amp C_D(T((1-x)^2))\ebm</mrow>
            <mrow>\amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm</mrow>
          </md>.
          To confirm, note that
          <md>
            <mrow>M_{DB}(T)C_B\amp (a+bx+cx^2)</mrow>
            <mrow> \amp = M_{DB}(T)C_B((a+b+c)-(b+2c)(1-x)+c(1-x)^2)</mrow>
            <mrow> \amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm\bbm a+b+c\\ -b-2c\\c\ebm</mrow>
            <mrow> \amp \bbm (a+b+c)+(b+2c)-2c\\0-2(b+2c)+4c\ebm = \bbm a+2b+c\\-2b\ebm</mrow>
          </md>,
          while on the other hand,
          <md>
            <mrow>C_D(T(a+bx+cx^2)) \amp= C_D(a+c,2b)</mrow>
            <mrow>\amp = C_D((a+2b+c)(1,0)-2b(1,-1))</mrow>
            <mrow>\amp = \bbm a+2b+c\\-2b\ebm</mrow>
          </md>.

        </p>
      </solution>
    </exercise>

    <p>
      When we compute the matrix of a transformation with respect to a non-standard basis,
      we don't have to worry about how to write vectors in the domain in terms of that basis.
      Instead, we simply plug the basis vectors into the transformation,
      and then determine how to write the output in terms of the basis of the codomain.
      However, if we want to <em>use</em> this matrix to compute values of <m>T:V\to W</m>,
      then we need a systematic way of writing elements of <m>V</m> in terms of the given basis.
    </p>

    <example>
      <title>Working with the matrix of a transformation</title>


      <statement>
        <p>
          Let <m>T:P_2(\R)\to \R^2</m> be a linear transformation whose matrix is given by
          <me>
            M(T) = \bbm 3\amp 0 \amp 3\\-1\amp -2\amp 2\ebm
          </me>
          with respect to the ordered bases <m>B = \{1+x, 2-x, 2x+x^2\}</m> of  <m>P_2(\R)</m>
          and <m>D = \{(0,1),(-1,1)\}</m> of <m>\R^2</m>. Find the value of <m>T(2+3x-4x^2)</m>.
        </p>
      </statement>
      <solution>
        <p>
          We need to write the input <m>2+3x-4x^2</m> in terms of the basis <m>B</m>.
          This amounts to solving the system of equations given by
          <me>
            a(1+x)+b(2-x)+c(2x+x^2)=2+3x-4x^2
          </me>.
          Of course, we can easily set up and solve this system,
          but let's try to be systematic, and obtain a more useful result for future problems.
          Since we can easily determine how to write any polynomial in terms of the standard basis <m>\{1,x,x^2\}</m>,
          it suffices to know how to write these three polynomials in terms of our basis.
        </p>

        <p>
          At first, this seems like more work. After all, we now have three systems to solve:
          <md>
            <mrow>a_1(x+1)+b_1(2-x)+c_1(2x+x^2) \amp =1</mrow>
            <mrow>a_2(x+1)+b_2(2-x)+c_2(2x+x^2) \amp =x</mrow>
            <mrow>a_3(x+1)+b_3(2-x)+c_3(2x+x^2) \amp x^2</mrow>
          </md>.
          However, all three systems have the same coefficient matrix,
          so we can solve them simultaneously, by adding three <q>constants</q>
          columns to our augmented matrix.
        </p>

        <p>
          We get the matrix
          <me>
            \left[\begin{matrix}1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\end{matrix}
            \right\rvert\left.\begin{matrix}1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\end{matrix}\right]
          </me>.
          But this is exactly the augmented matrix we'd right down if we were trying to find the inverse of the matrix
          <me>
            P=\bbm 1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\ebm
          </me>
          whose columns are the coefficient representations of our given basis vectors in terms of the standard basis.
        </p>

        <p>
          We compute (using the Sage cell below) that
          <me>
            P^{-1}=\frac13\bbm 1\amp 2\amp -4\\1\amp -1\amp 2\\0\amp 0\amp 3\ebm
          </me>,
          and
          <me>
            M(T)P^{-1}=\bbm 1\amp 2\amp -1\\-1\amp 0\amp 2\ebm
          </me>.
          This matrix first converts the coefficient vector for a polynomial <m>p(x)</m>
          with respect to the standard basis into the coefficient vector for our given basis <m>B</m>,
          and then multiplies by the matrix representing our transformation.
          The result will be the coefficient vector for <m>T(p(x))</m> with repect to the basis <m>D</m>.
        </p>

        <p>
          The polynomial <m>p(x) = 2+3x-4x^2</m> has coefficient vector <m>\bbm 2\\3\\-4\ebm</m>
          with respect to the standard basis. We find that <m>M(T)P^{-1}\bbm 2\\3\\-4\ebm = \bbm 12\\-10\ebm</m>.
          The coefficients <m>12</m> and <m>-10</m> are the coefficients of <m>T(p(x))</m>
          with repsect to the basis <m>D</m>. Thus,
          <me>
            T(2+3x-4x^2) = 12(0,1)-10(-1,1) = (10,2)
          </me>.
          Note that in the last step we gave the <q>simplified</q> answer <m>(10,2)</m>,
          which is simplified primarily in that it is expressed with respect to the standard basis.
        </p>

        <p>
          Note that we can also introduce the matrix <m>Q = \bbm 0\amp -1\\1\amp 1\ebm</m>
          whose columns are the coefficient vectors of the vectors in the basis <m>D</m>
          with respect to the standard basis.
          The effect of multiplying by <m>Q</m> is to convert from coefficients with respect to <m>D</m>
          into a coefficient vector with respect to the standard basis.
          We can then write a new matrix <m>\hat{M}(T) = QM(T)P^{-1}</m>;
          this new matrix is now the matrix representation of <m>T</m> with respect to the
          <em>standard</em> bases of <m>P_2(\R)</m> and <m>\R^2</m>.
          We check that
          <me>
            \hat{M}(T)\bbm 2\\3\\-4\ebm = \bbm 10\\2\ebm
          </me>,
          as before.
        </p>

        <p>
          We find that <m>\tilde{M}(T) = \bbm 1\amp 0\amp -2\\0\amp 2\amp 1\ebm</m>.
          This lets us determine that for a general polynomial <m>p(x) = a+bx+cx^2</m>,
          <me>
            \hat{M}(T)\bbm a\\b\\c\ebm = \bbm a-2c\\2b+c\ebm
          </me>,
          and therefore, our original transformation must have been
          <me>
            T(a+bx+cx^2)=(a-2c,2b+c)
          </me>.

        </p>
      </solution>
    </example>

    <sage>
      <input>
        from sympy import *
        init_printing()
        P = Matrix(3,3,[1,2,0,1,-1,2,0,0,1])
        P**-1
      </input>
    </sage>

    <sage>
      <input>
        M = Matrix(2,3,[3,0,3,-1,-2,2])
        v = Matrix([2,3,-4])
        M*P**-1,v,(M*P**-1)*v
      </input>
    </sage>

    <sage>
      <input>
        Q = Matrix(2,2,[0,-1,1,1])
        Q*M*P**-1
      </input>
    </sage>

    <p>
      The previous example illustrated some important observations that are true in general.
      We won't give the general proof, but we sum up the results in a theorem.
    </p>

    <theorem xml:id="thm-change-basis-transformation">
      <statement>
        <p>
          Suppose <m>T:V\to W</m> is a linear transformation,
          and suppose <m>M_0 = M_{D_0B_0}(T)</m> is the matrix of <m>T</m> with resepct to bases <m>B_0</m> of <m>V</m> and <m>D_0</m> of <m>W</m>.
          Let <m>B_1=\basis{v}{n}</m> and <m>D_1\basis{w}{m}</m> be any other choice of basis for <m>V</m> and <m>W</m>, respectively.
          Let
          <md>
            <mrow>P \amp =\bbm C_{B_0}(\vv_1) \amp C_{B_0}(\vv_2) \amp \cdots \amp C_{B_0}(\vv_n)\ebm</mrow>
            <mrow>Q \amp =\bbm C_{B_0}(\ww_1) \amp C_{B_0}(\ww_2) \amp \cdots \amp C_{B_0}(\ww_n)\ebm</mrow>
          </md>
          be matrices whose columns are the coefficient vectors of the vectors in <m>B_1,D_1</m> with respect to <m>B_0,D_0</m>.
          Then the matrix of <m>T</m> with respect to the bases <m>B_1</m> and <m>D_1</m> is
          <me>
            M_{D_1B_1}(T) = QM_{D_0B_0}(T)P^{-1}
          </me>.
        </p>
      </statement>
    </theorem>

    <p>
      The relationship between the different maps is illustrated in <xref ref="fig-basis-cube"/> below.
      In this figure, the maps <m>V\to V</m> and <m>W\to W</m> are the identity maps,
      corresponding to representating the same vector with respect to two different bases.
      The vertical arrows are the coefficient isomorphisms <m>C_{B_0},C_{B_1},C_{D_0},C_{D_1}</m>.
    </p>

    <figure xml:id="fig-basis-cube">
      <caption>Diagramming matrix of a transformation with respect to two different choices of basis</caption>
      <image width="47%" source="basis-cube.svg"/>
    </figure>

    <p>
      We generally apply <xref ref="thm-change-basis-transformation"/> in the case that <m>B_0,D_0</m>
      are the <em>standard</em> bases for <m>V,W</m>, since in this case,
      the matrices <m>M_0, P, Q</m> are easy to determine,
      and we can use a computer to calculate <m>P^{-1}</m> and the product <m>QM_0P^{-1}</m>.
    </p>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:M_{22}(\R)\to P_2(\R)</m> has the matrix
          <me>
            M_{DB}(T) = \bbm 2\amp -1\amp 0\amp 3\\0\amp 4\amp -5\amp 1\\-1\amp 0\amp 3\amp -2\ebm
          </me>
          with respect to the bases
          <me>
            B = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 0\\0\amp 1\ebm\right\}
          </me>
          of <m>M_{22}(\R)</m> and <m>D=\{1,x,x^2\}</m> of <m>P_2(\R)</m>.
          Determine a formula for <m>T</m> in terms of a general input <m>X=\bbm a\amp b\\c\amp d\ebm</m>.
        </p>
      </statement>
      <solution>
        <p>
          We must first write our general input in terms of the given basis.
          With respect to the standard basis
          <me>
            B_0 = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}
          </me>,
          we have the matrix <m>P = \bbm 1\amp 0\amp 0\amp 1\\0\amp 1\amp 1\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 1\amp 0\amp 1\ebm</m>,
          representing the change from the basis <m>B</m> the basis <m>B_0</m>.
          The basis <m>D</m> of <m>P_2(\R)</m> is already the standard basis.
          For a matrix <m>X = \bbm a\amp b\\c\amp d\ebm</m> we find
          <me>
            M_{DB}(T)P^{-1}C_{B_0}(X)=\bbm 2\amp -2\amp 2\amp 1\\0\amp 3\amp -8\amp 1\\-1\amp 1\amp 2\amp -1\ebm\bbm a\\b\\c\\d\ebm =
            \bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm
          </me>.
          But this is equal to <m>C_D(T(X))</m>, so
          <md>
            <mrow>T\left(\bbm a\amp b\\c\amp d\ebm\right) \amp = C_D^{-1}\bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm</mrow>
            <mrow>\amp = (2a-2b+2c+d)+(3b-8c+d)x+(-a+b+2c-d)x^2</mrow>
          </md>.

        </p>
      </solution>
    </exercise>

    <sage>
      <input>
        M = Matrix(3,4,[2,-1,0,3,0,4,-5,1,-1,0,3,-2])
        P = Matrix(4,4,[1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,1])
        M*P**-1
      </input>
    </sage>

    <p>
      In textbooks such as Sheldon Axler's <em>Linear Algebra Done Right</em> that focus primarily on linear transfomrations,
      the above construction of the matrix of a transformation with respect to choices of bases can be used as a primary motivation for introducing matrices,
      and determining their algebraic properties. In particular, the rule for matrix multiplication, which can seem peculiar at first,
      can be seen as a consequence of the composition of linear maps.
    </p>

    <theorem xml:id="thm-matrix-multiplication">
      <statement>
        <p>
          Let <m>U,V,W</m> be finite-dimensional vectors spaces,
          with ordered bases <m>B_1,B_2,B_3</m>, respectively.
          Let <m>T:U\to V</m> and <m>S:V\to W</m> be linear maps. Then
          <me>
            M_{B_3B_1}(ST) = M_{B_3B_2}(S)M_{B_2B_1}(T)
          </me>.
        </p>
      </statement>
      <proof>
        <p>
          Let <m>\xx\in U</m>. Then <m>C_{B_3}(ST(\xx)) = M_{B_3B_1}(ST)C_{B_1}(\xx)</m>.
          On the other hand,
          <md>
            <mrow> M_{B_3B_2}(S)M_{B_2B_1}(T)C_{B_1}(\xx) \amp  = M_{B_3B_2}(S)(C_{B_2}(T(\xx)))</mrow>
            <mrow> \amp = C_{B_3}(S(T(\xx))) = C_{B_3}(ST(\xx))</mrow>
          </md>.
          Since <m>C_{B_3}</m> is invertible, the result follows.
        </p>
      </proof>

    </theorem>

    <p>
      Being able to express a general linear transformation in terms of a matrix is useful,
      since questions about linear transformations can be converted into questions about matrices that we already know how to solve.
      In particular,
      <ul>
        <li>
          <p>
            <m>T:V\to W</m> is an isomorphism if and only if <m>M_{DB}(T)</m> is invertible for some
            (and hence, all) choice of bases <m>B</m> of <m>V</m> and <m>D</m> of <m>W</m>.
          </p>
        </li>

        <li>
          <p>
            The rank of <m>T</m>  is equal to the rank of <m>M_{DB}(T)</m> (and this does not depend on the choice of basis).
          </p>
        </li>

        <li>
          <p>
            The kernel of <m>T</m> is isomorphic to the nullspace of <m>M_{DB}(T)</m>.
          </p>
        </li>
      </ul>
    </p>

    <p>
      Next, we will want to look at two topics in particular.
      First, if <m>T:V\to V</m> is a linear operator, then it makes sense to consider the matrix <m>M_B(T)=M_{BB}(T)</m>
      obtained by using the same basis for both domain and codomain.
      Second, we will want to know how this matrix changes if we change the choice of basis.
    </p>
  </section>

  <section xml:id="sec-matrix-operator">
    <title>The matrix of a linear operator</title>
    <p>
      Recall that a linear transformation <m>T:V\to V</m> is referred to as a <term>linear operator</term>.
      Recall also that two matrices <m>A</m> and <m>B</m> are <term>similar</term>
      if there exists an invertible matrix <m>P</m> such that <m>B = PAP^{1-}</m>,
      and that similar matrices have a lot of properties in common.
      In particular, if <m>A</m> is similar to <m>B</m>, then <m>A</m> and <m>B</m>
      have the same trace, determinant, and eigenvalues.
      One way to understand this is the realization that two matrices are similar if they are representations of the
      <em>same</em> operator, with respect to <em>different</em> bases.
    </p>

    <p>
      Since the domain and codomain of a linear operator are the same,
      we can consider the matrix <m>M_{DB}(T)</m> where <m>B</m> and <m>D</m> are the <em>same</em> ordered basis.
      This leads to the next definition.
    </p>

    <definition xml:id="def-operator-matrix">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator,
          and let <m>B=\basis{b}{n}</m> be an ordered basis of <m>V</m>.
          The matrix <m>M_B(T)=M_{BB}(T)</m> is called the <term><m>B</m>-matrix</term> of <m>T</m>.
        </p>
      </statement>
    </definition>

    <p>
      The following result collects several useful properties of the <m>B</m>-matrix of an operator.
      Most of these were already encountered for the matrix <m>M_{DB}(T)</m> of a transformation,
      although not all were stated formally. (Formal statements can be found in the textbook by Nicholson.)
    </p>

    <theorem xml:id="thm-bmatrix-properties">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator, and let <m>B=\basis{b}{n}</m> be a basis for <m>V</m>.
          Then
          <ol>
            <li><m>C_B(T(\vv))=M_B(T)C_B(\vv)</m> for all <m>\vv\in V</m>.</li>
            <li>If <m>S:V\to V</m> is another operator, then <m>M_B(ST)=M_B(S)M_B(T)</m>.</li>
            <li><m>T</m> is an isomorphism if and only if <m>M_B(T)</m> is invertible.</li>
            <li>If <m>T</m> is an isomorphism, then <m>M_B(T^{-1}) = [M_B(T)]^{-1}</m>.</li>
            <li><m>M_B(T)=\bbm C_B(\mathbf{b}_1) \amp \cdots \amp C_B(\mathbf{b}_n)\ebm</m>.</li>
          </ol>
        </p>
      </statement>
    </theorem>


    <example>
      <statement>
        <p>
          Find the <m>B</m>-matrix of the operator <m>T:P_2(\R)\to P_2(\R)</m> given by
          <m>T(p(x))=p(0)(1+x^2)+p(1)x</m>, with respect to the ordered basis
          <me>
            B = \{1-x, x+3x^2, 2-x^2\}
          </me>.
        </p>
      </statement>
      <solution>
        <p>
          We compute
          <md>
            <mrow>T(1-x) \amp = 1(1+x^2)+0(x) = 1+x^2</mrow>
            <mrow>T(x+3x^2) \amp = 0(1+x^2)+4x=4x</mrow>
            <mrow>T(2-x^2) \amp = 2(1+x^2)+1(x) = 2+x+2x^2</mrow>
          </md>.
          We now need to write each of these in terms of the basis <m>B</m>.
          We can do this by working out how to write each polynomial above in terms of <m>B</m>.
          Or we can be systematic.
        </p>

        <p>
          Let <m>P = \bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm</m>
          be the matrix whose columns are given by the coefficient representations of the polynomials in <m>B</m>
          with respect to the <em>standard basis</em> <m>B_0=\{1,x,x^2\}</m>.
          For <m>T(1-x)=1+x^2</m> we need to solve the equation
          <me>
            a(1-x)+b(x+3x^2)+c(2-x^2)=1+x^2
          </me>
          for scalars <m>a,b,c</m>. But this is equivalent to the system
          <md>
            <mrow>a+2c \amp =1</mrow>
            <mrow>-a+b \amp =0</mrow>
            <mrow>3b-c \amp =1</mrow>
          </md>,
          which, in turn, is equivalent to the matrix equation
          <me>
            \bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm\bbm a\\b\\c\ebm = \bbm 1\\0\\1\ebm
          </me>;
          that is, <m>PC_B(1+x^2) = C_{B_0}(1+x^2)</m>. Thus,
          <me>
            C_B(1+x^2) = \bbm a\\b\\c\ebm = P^{-1}C_{B_0}(1+x^2) = P^{-1}\bbm 1\\0\\1\ebm
          </me>.
          Similarly, <m>C_B(4x)=P^{-1}\bbm 0\\4\\0\ebm = P^{-1}C_{B_0}(4x)</m>, and
          <m>C_B(2+x+2x^2)=P^{-1}\bbm 2\\1\\2\ebm = P^{-1}C_{B_0}(2+x+2x^2)</m>.
          Thus, (using the Sage code below)
          <md>
            <mrow>M_B(T) \amp =\bbm C_B(T(1-x)) \amp C_B(T(x+3x^2)) \amp C_B(T(2-x^2))\ebm</mrow>
            <mrow> \amp =\bbm P^{-1}C_{B_0}T(1-x) \amp P^{-1} C_{B_0}T(x+3x^2) \amp P^{-1}C_{B_0}(T(2-x^2))\ebm</mrow>
            <mrow> \amp =P^{-1}\bbm C_{B_0}(1+x^2) \amp C_{B_0}(4x)\amp C_{B_0}(2+x+2x^2)\ebm</mrow>
            <mrow> \amp =\bbm 1/7\amp -6/7\amp 3/7\\1/7\amp 1/7\amp 2/7\\3/7\amp 3/7\amp -1/7\ebm
                         \bbm 1\amp 0\amp 2\\0\amp 4\amp 1\\1\amp 0\amp 2\ebm</mrow>
            <mrow> \amp = \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm</mrow>
          </md>.
        </p>

        <p>
          Let's confirm that this works. Suppose we have
          <md>
            <mrow>p(x) \amp = C_B^{-1}\bbm a\\b\\c\ebm = a(1-x)+b(x+3x^2)+c(2-x^2)</mrow>
            <mrow>\amp = (a+2c)+(-a+b)x+(3b-c)x^2</mrow>
          </md>.
          Then <m>T(p(x))=(a+2c)(1+x^2)+(4b+c)x</m>, and we find
          <me>
            C_B(T(p(x))) = P^{-1}\bbm a+2c\\4b+c\\a+2c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm
          </me>.
          On the other hand,
          <me>
            M_B(T) = \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\bbm a\\b\\c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm
          </me>.
          The results agree, but possibly leave us a little confused.
        </p>
      </solution>
    </example>

    <sage>
      <input>
        from sympy import *
        init_printing()
        P = Matrix(3,3,[1,0,2,-1,1,0,0,3,-1])
        M = Matrix(3,3,[1,0,2,0,4,1,1,0,2])
        P**-1, P**-1*M
      </input>
    </sage>

    <p>
      In general, given an ordered basis <m>B=\basis{b}{n}</m> for a vector space <m>V</m>
      with standard basis <m>B_0 = \basis{e}{n}</m>, if we let
      <me>
        P = \bbm C_{B_0}(\mathbf{b}_1) \amp \cdots \amp C_{B_0}(\mathbf{b}_n)\ebm
      </me>,
      then
      <md>
        <mrow>M_B(T) \amp = \bbm C_B(T(\mathbf{b}_1)) \amp \cdots \amp C_B(T(\mathbf{b_n})\ebm</mrow>
        <mrow> \amp = P^{-1}\bbm C_{B_0}(T(\mathbf{b}_1)) \amp \cdots \amp C_{B_0}(T(\mathbf{b_n})\ebm</mrow>
      </md>.
    </p>

    <p>
      As we saw above, this gives us the result, but doesn't shed much light on the problem,
      unless we have an easy way to write vectors in terms of the basis <m>B</m>.
      Let's revisit the problem. Instead of using the given basis <m>B</m>,
      let's use the standard basis <m>B_0 = \{1,x,x^2\}</m>.
      We quickly find
      <me>
        T(1)=1+x+x^2, T(x) = x, \text{ and } T(x^2)=x
      </me>,
      so with respect to the standard basis, <m>M_{B_0}(T) = \bbm 1\amp 0\amp 0\\1\amp 1\amp 1\\1\amp 0\amp 0\ebm</m>.
      Now, recall that
      <me>
        M_{B}(T)=P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm
      </me>
      and note that for any polynomial <m>p(x)</m>, <m>C_{B_0}T(p(x)) = M_{B_0}C_{B_0}(p(x))</m>. But
      <me>
        \bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm = P
      </me>,
      so we get
      <md>
        <mrow>M_B(T) \amp = P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm</mrow>
        <mrow> \amp = P^{-1}\bbm M_{B_0}C_{B_0}(1-x)\amp M_{B_0}C_{B_0}(x+3x^2)\amp M_{B_0}C_{B_0}(2-x^2)\ebm </mrow>
        <mrow> \amp = P^{-1}M_{B_0}\bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm</mrow>
        <mrow> \amp = P^{-1}M_{B_0}P</mrow>
      </md>.
    </p>

    <p>
      Now we have a much more efficient method for arriving at the matrix <m>M_B(T)</m>.
      The matrix <m>M_{B_0}(T)</m> is easy to determine, the matrix <m>P</m> is easy to determine,
      and with the help of the computer, it's easy to compute <m>P^{-1}M_{B_0}P = M_B(T)</m>.
    </p>

    <sage>
      <input>
        M0 = Matrix(3,3,[1,0,0,1,1,1,1,0,0])
        P**-1*M0*P
      </input>
    </sage>

    <exercise>
      <statement>
        <p>
          Determine the matrix of the operator <m>T:\R^3\to \R^3</m> given by
          <me>
            T(x,y,z) = (3x-2y+4z,x-5y,2y-7z)
          </me>
          with respect to the ordered basis
          <me>
            B = \{(1,2,0),(0,-1,2),(1,2,1)\}
          </me>.
          (You may want to use the Sage cell below for computational assistance.)
        </p>
      </statement>
      <solution>
        <p>
          With respect to the standard basis, we have
          <me>
            M_0=M_{B_0}(T) = \bbm 3\amp -2\amp 4\\1\amp -5\amp 0\\0\amp 2\amp -7\ebm
          </me>,
          and the matrix <m>P</m> is given by <m>P = \bbm 1\amp 3\amp 1\\2\amp -1\amp 2\\0\amp 2\amp-5\ebm</m>.
          Thus, we find
          <me>
            M_B(T)=P^{-1}M_0P=\bbm 9\amp 56\amp 36\\7\amp 15\amp 15\\-10\amp -46\amp -33\ebm
          </me>.
        </p>
      </solution>
    </exercise>

    <sage>
      <input>

      </input>
    </sage>

    <p>
      The matrix <m>P</m> used in the above examples is known as a <em>change</em> matrix.
      If the columns of <m>P</m> are the coefficient vectors of <m>B=\basis{b}{n}</m>
      with respect to another basis <m>D</m>, then we have
      <md>
        <mrow>P \amp= \bbm C_D(\mathbf{b}_1)\amp\cdots \amp C_D(\mathbf{b}_n)\ebm</mrow>
        <mrow>\amp = \bbm C_D(1_V(\mathbf{b}_1))\amp \cdots \amp C_D(1_V(\mathbf{b}_n))\ebm</mrow>
        <mrow>\amp = M_{DB}(1_V)</mrow>
      </md>.
      In other words, <m>P</m> is the matrix of the identity transformation <m>1_V:V\to V</m>,
      where we use the basis <m>B</m> for the domain, and the basis <m>D</m> for the codomain.
    </p>

    <definition xml:id="def-change-matrix">
      <statement>
        <p>
          The <term>change matrix</term> with respect to ordered bases <m>B,D</m> of <m>V</m>
          is denoted <m>P_{D\leftarrow B}</m>, and defined by
          <me>
            P_{D\leftarrow B} = M_{DB}(1_V)
          </me>.
        </p>
      </statement>
    </definition>

    <theorem xml:id="thm-change-matrix">
      <statement>
        <p>
          Let <m>B=\basis{b}{n}</m> and <m>D</m> be two ordered bases of <m>V</m>.
          Then
          <me>
            P_{D\leftarrow B} = \bbm C_D(\mathbf{b}_1)\amp \cdots \amp C_D(\mathbf{b}_n)\ebm
          </me>,
          and satisfies <m>C_D(\vv) = P_{D\leftarrow B}C_B(\vv)</m> for all <m>\vv\in V</m>.
        </p>

        <p>
          The matrix <m>P_{D\leftarrow B}</m> is invertible, and <m>(P_{D\leftarrow B})^{-1} = P_{B\leftarrow D}</m>.
          Moreover, if <m>E</m> is a third ordered basis, then
          <me>
            P_{E\leftarrow D}P_{D\leftarrow B} = P_{E\leftarrow B}
          </me>.
        </p>
      </statement>
    </theorem>

    <example>
      <statement>
        <p>
          Let <m>B = \{1,x,x^2\}</m> and let <m>D = \{1+x,x+x^2,2-3x+x^2\}</m> be orderd bases of <m>P_2(\R)</m>.
          Find the change matrix <m>P_{D\leftarrow B}</m>.
        </p>
      </statement>
      <solution>
        <p>
          Finding this matrix requires us to first write the vectors in <m>B</m> in terms of the vectors in <m>D</m>.
          However, it's much easier to do this the other way around. We easily find
          <me>
            P_{B\leftarrow D} = \bbm 1\amp 0\amp 2\\1\amp 1\amp -3\\0\amp 1\amp 1\ebm
          </me>,
          and by <xref ref="thm-change-matrix"/>, we have
          <me>
            P_{D\leftarrow B} = (P_{B\leftarrow D})^{-1} = \frac16\bbm 4\amp 2\amp -2\\-1\amp 1\amp 5\\1\amp -1\amp 1\ebm
          </me>.
        </p>
      </solution>
    </example>

    <p>
      The above results give a straightforward procedure for determining the matrix of any operator,
      with respect to any basis, if we let <m>D</m> be the standard basis.
      The importance of these results is not just their computational simplicity, however.
      The most important outcome of the above is that if <m>M_B(T)</m> and <m>M_D(T)</m>
      give the matrix of <m>T</m> with respect to two different bases, then
      <me>
        M_B(T) = (P_{D\leftarrow B})^{-1}M_B(T)P_{D\leftarrow B}
      </me>,
      so that the two matrices are <em>similar</em>.
    </p>

    <p>
      Recall from <xref ref="thm-similar-properties">Theorem</xref>
      that similar matrices have the same determinant, trace, and eigenvalues.
      This means that we can unambiguously define the determinant and trace of an  <em>operator</em>,
      and that we can compute eigenvalues of an operator using any matrix representation of that operator.
    </p>
  </section>

  <section xml:id="sec-direct-sum">
    <title>Direct Sums and Invariant Subspaces</title>
    <introduction>
      <p>
        Much of this section has been mentioned previously in the course (and these notes),
        but we will follow the organization of Nicholson's textbook,
        and reprise these concepts in more detail than previously.
      </p>
    </introduction>

    <subsection xml:id="subsec-invariant">
      <title>Invariant subspaces</title>
      <definition xml:id="def-invariant-subspace">
        <statement>
          <p>
            Given an operator <m>T:V\to V</m>, we say that a subspace <m>U\subseteq V</m>
            is <m>T</m>-<term>invariant</term> if <m>T(\uu)\in U</m> for all <m>\uu\in U</m>.
          </p>
        </statement>
      </definition>

      <p>
        In other words, a subspace <m>U</m> is <m>T</m>-invariant if <m>T</m> does not map any vectors in <m>U</m> outside of <m>U</m>.
        Notice that if we shrink the domain of <m>T</m> to <m>U</m>, then we get an operator from <m>U</m> to <m>U</m>,
        since the image <m>T(U)</m> is contained in <m>U</m>.
      </p>

      <p>
        Given a basis <m>B=\basis{u}{k}</m> of <m>U</m>,
        note that <m>U</m> is <m>T</m>-invariant if and only if <m>T(\uu_i)\in U</m> for each <m>i=1,2,\ldots, k</m>.
      </p>

      <p>
        For any operator <m>T:V\to V</m>, there are four subspaces that are always <m>T</m>-invariant:
        <me>
          \{\mathbf{0}\}, V, \ker T, \text{ and } \im T
        </me>.
        Of course, some of these subspaces might be the same; for example,
        if <m>T</m> is invertible, then <m>\ker T = \{\mathbf{0}\}</m> and <m>\im T = V</m>.
        (We will skip the proof that <m>\ker T</m> and <m>\im T</m> are <m>T</m>-invariant,
        since this has been assigned as homework!)
      </p>

      <definition xml:id="def-restriction">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
            The <term>restriction</term> of <m>T</m> to <m>U</m>, denoted <m>T|_U</m>,
            is the operator <m>T|_U:U\to U</m> defined by <m>T|_U(\uu)=T(\uu)</m> for all <m>\uu\in U</m>.
          </p>
        </statement>
      </definition>

      <p>
        Notice that the restriction <m>T|_U</m> is defined by the same <q>rule</q> as <m>T</m>,
        but its domain is the subspace <m>U</m> instead of the entire vector space <m>V</m>.
      </p>

      <p>
        A lot can be learned by studying the restrictions of an operator to invariant subspaces.
        Indeeed, the textbook by Axler does almost everything from this point of view.
        One reason to study invariant subspaces is that they allow us to put the matrix of <m>T</m> into simpler forms.
      </p>

      <theorem xml:id="thm-invariant-block-triangular">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
            Let <m>B_U = \basis{u}{k}</m> be a basis of <m>U</m>, and extend this to a basis
            <me>
              B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_{n-k}\}
            </me>
            of <m>V</m>. Then the matrix <m>M_B(T)</m> with respect to this basis has the block-triangular form
            <me>
              M_B(T) = \bbm M_{B_U}(T_U) \amp P\\0 \amp Q\ebm
            </me>
            for some <m>(n-k)\times (n-k)</m> matrix <m>Z</m>.
          </p>
        </statement>
      </theorem>

      <p>
        Reducing a matrix to block triangular form is useful,
        because it simplifies computations such as determinants and eigenvalues
        (and determinants and eigenvalues are computationally expensive).
        In particular, if a matrix <m>A</m> has the block form
        <me>
          A = \bbm A_{11} \amp A_{12} \amp \cdots A_{1n}\\
                   0\amp A_{22} \amp \cdots A_{2n}\\
                   \vdots \amp \vdots \amp \ddots \amp \vdots\\
                   0 \amp 0 \amp \cdots \amp A_{nn}\ebm
        </me>,
        where the diagonal blocks are square matrices,
        then <m>\det(A) = \det(A_{11})\det(A_{22})\cdots \det(A_{nn})</m>
        and <m>c_A(x) = c_{A_{11}}(x)c_{A_{22}}(x)\cdots C_{A_{nn}}(x)</m>.
      </p>
    </subsection>

    <subsection xml:id="subsec-eigenspace">
      <title>Eigenspaces</title>
      <p>
        An important source of invariant subspaces is eigenspaces.
        Recall that for any real number <m>\lambda</m>,
        and any operator <m>T:V\to V</m>, we define
        <me>
          E_\lambda(T) = \ker(T-\lambda 1_V) = \{\vv\in V \,|\, T(\vv) = \lambda\vv\}
        </me>.
        For most values of <m>\lambda</m>, we'll have <m>E_\lambda(T)=\{\mathbf{0}\}</m>.
        The values of <m>\lambda</m> for which <m>E_\lambda(T)</m> is non-trivial are precisely the eigenvalues of <m>T</m>.
        Note that since similar matrices have the same characteristic polynomial any matrix representation <m>M_B(T)</m>
        will have the same eigenvalues. They do <em>not</em> generally have the same eigenspaces,
        but we do have the following.
      </p>

      <theorem xml:id="thm-eigenspace-invariant">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator. For any scalar <m>\lambda</m>,
            the eigenspace <m>E_\lambda(T)</m> is <m>T</m>-invariant.
            Moreover, for any ordered basis <m>B</m> of <m>V</m>,
            the coefficient isomorphism <m>C_B:V\to \R^n</m> induces an isomorphism
            <me>
              C_B|_{E_\lambda(T)}:E_\lambda(T)\to E_{\lambda}(M_B(T))
            </me>.
          </p>
        </statement>
      </theorem>
    </subsection>

    <subsection xml:id="subsec-direct-sum">
      <title>Direct Sums</title>
      <p>
        Recall that for any subspaces <m>U,W</m> of a vector space <m>V</m>,
        the sets
        <md>
          <mrow>U+W \amp =\{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}</mrow>
          <mrow>U\cap W \amp = \{\vv \in V \,|\, \vv\in U \text{ and } \vv\in W\}</mrow>
        </md>
        are subspaces of <m>V</m>. Saying that <m>\vv\in U+W</m> means that <m>\vv</m>
        can be written as a sum of a vector in <m>U</m> and a vector in <m>W</m>.
        However, this sum may not be unique. If <m>\vv\in U\cap W</m>, <m>\uu\in U</m> and <m>\ww\in W</m>,
        then we can write <m>(\uu+\vv)+\ww = \uu + (\vv+\ww)</m>,
        giving two different representations of a vector as an element of <m>U+W</m>.
      </p>

      <theorem xml:id="thm-unique-sum">
        <statement>
          <p>
            Let <m>U</m> and <m>W</m> be subspaces of a vector space <m>V</m>.
            Let <m>\vv\in U+W</m>. Then there exist <em>unique</em> vectors
            <m>\uu\in U, \ww\in W</m> such that <m>\vv = \uu+\ww</m>
            if and only if <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>
        </statement>

        <proof>
          <p>
            Suppose <m>U+W</m> has the property that if <m>\vv\in U+W</m>,
            then there exist unique <m>\uu\in U,\ww\in W</m> such that <m>\vv=\uu+\ww</m>.
            Suppose <m>\mathbf{x}\in U\cap W</m>. Then <m>\xx\in U</m> and <m>\xx\in W</m>,
            which implies that <m>-\xx\in W</m>, since <m>W</m> is a subspace.
            Then we can write
            <me>
              \mathbf{0} = \xx + (-\xx)
            </me>,
            with <m>\xx\in U</m> and <m>-\xx\in W</m>. But we also know that <m>\mathbf{0}\in U</m> and <m>\mathbf{0}\in W</m>,
            so we can also write <m>\mathbf{0}=\mathbf{0}+\mathbf{0}</m>.
            Since we can only write <m>\mathbf{0}</m> in one way as a sum of a vector in <m>U</m> and a vector in <m>W</m>,
            we must have <m>\xx=\mathbf{0}</m>, showing that <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>

          <p>
            Conversely, suppose that <m>U\cap W = \{\zer\}</m>, and let <m>\vv\in U+W</m>.
            Suppose that there exist vectors <m>\uu_1,\uu_2\in U</m> and <m>\ww_1,\ww_2\in W</m>
            such that <m>\vv = \uu_1+\ww_1=\uu_2+\ww_2</m>. But then <m>\uu_1-\uu_2=\ww_2-\ww_1</m>,
            and since <m>\uu_1-\uu_2\in U</m> and <m>\ww_2-\ww_1\in W</m>,
            we have <m>\uu_1-\uu_2=\ww_2-\ww_1\in U\cap W</m>. Since <m>U\cap W=\{\zer\}</m>,
            we have <m>\uu_1-\uu_2=\zer</m> and <m>\ww_2-\ww_1=\zer</m>,
            so <m>\uu_1=\uu_2</m> and <m>\ww_1=\ww_2</m>.
          </p>
        </proof>

      </theorem>

      <definition xml:id="def-direct-sum">
        <statement>
          <p>
            We say that a sum <m>U+W</m> is a <term>direct sum</term>,
            and write this sum as <m>U\oplus W</m>, if <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>
        </statement>
      </definition>

      <p>
        Typically we are interested in the case that the two subspaces sum to <m>V</m>.
        If <m>V = U\oplus W</m>, we say that <m>W</m> is a <term>complement</term> of <m>U</m>,
        and that <m>U\oplus W</m> is a direct sum decomposition of <m>V</m>.
        Of course, the orthogonal complement <m>U^\bot</m> of a subspace <m>U</m>
        is a complement in this sense, if <m>V</m> is equipped with an inner product.
        (Without an inner product we have no concept of <q>orthogonal</q>.)
        But even if we don't have an inner product, finding a complement is not too difficult,
        as the next example shows.
      </p>

      <example xml:id="eg-direct-sum-basis">
        <title>Finding a complement by extending a basis</title>

        <statement>
          <p>
            The easiest way to determine a direct sum decomposition (or equivalently, a complement)
            is through the use of a basis. Suppose <m>U</m> is a subspace of <m>V</m> with basis
            <m>\basis{e}{k}</m>, and extend this to a basis
            <me>
              B = \{\mathbf{e}_1,\ldots, \mathbf{e}_k,\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}
            </me>
            of <m>V</m>. Let <m>W = \spn\{\mathbf{e}_{k+1},\ldots, \mathbf{e}_n\}</m>.
            Then clearly <m>U+W=V</m>, and <m>U\cap W=\{\zer\}</m>,
            since if <m>\vv\in U\cap W</m>, then <m>\vv\in U</m> and <m>\vv\in W</m>, so we have
            <me>
              \vv = a_1\mathbf{e}_1+\cdots + a_k\mathbf{e_k} = b_1\mathbf{e}_{k+1}+\cdots+b_{n-k}e_{n}
            </me>,
            which gives
            <me>
              a_1\mathbf{e}_1+\cdots + a_k\mathbf{e}_k-b_1\mathbf{e}_{k+1}-\cdots - b_{n-k}\mathbf{e}_n=\zer
            </me>,
            so <m>a_1=\cdots b_{n-k}=0</m> by the linear indpendence of <m>B</m>, showing that <m>\vv=\zer</m>.
          </p>

          <p>
            Conversely, if <m>V=U\oplus W</m>, and we have bases <m>\basis{u}{k}</m> of <m>U</m>
            and <m>\basis{v}{l}</m> of <m>W</m>, then
            <me>
              B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_l\}
            </me>
            is a basis for <m>V</m>. Indeed, <m>B</m> spans <m>V</m>,
            since every element of <m>V</m> can be written as <m>\vv=\uu+\ww</m> with <m>\uu\in U,\ww\in W</m>.
            Independence follows by reversing the argument above: if
            <me>
              a_1\uu_1+\cdots + a_k\uu_k+b_1\ww_1+\cdots b_l\ww_l=\zer
            </me>
            then <m>a_1\uu_1+\cdots + a_k\uu_k = -b_1\ww_1-\cdots -b_l\ww_l</m>,
            and equality is only possible if both sides belong to <m>U\cap W = \{\zer\}</m>.
            Since <m>\basis{u}{k}</m> is independent, the <m>a_i</m> have to be zero,
            and since <m>\basis{w}{l}</m> is independent, the <m>b_j</m> have to be zero.
          </p>
        </statement>
      </example>

      <p>
        The argument given in the second part of <xref ref="eg-direct-sum-basis"/> has an immediate,
        but important consequence.
      </p>

      <theorem xml:id="thm-direct-sum-dimension">
        <statement>
          <p>
            Suppose <m>V=U\oplus W</m>, where <m>\dim U = m</m> and <m>\dim W = n</m>.
            Then <m>V</m> is finite-dimensional, and <m>\dim V = m+n</m>.
          </p>
        </statement>
      </theorem>

      <example xml:id="eg-invariant-block">
        <statement>
          <p>
            Suppose <m>V=U\oplus W</m>, where <m>U</m> and <m>W</m> are <m>T</m>-invariant subspaces
            for some operator <m>T:V\to V</m>. Let <m>B_U=\basis{u}{m}</m> and let <m>B_W = \basis{w}{n}</m>
            be bases for <m>U</m> and <m>W</m>, respectively.
            Determine the matrix of <m>T</m> with respect to the basis <m>B=B_U\cup B_W</m> of <m>V</m>.
          </p>
        </statement>
        <solution>
          <p>
            Since we don't know the map <m>T</m> or anything about the bases <m>B_U,B_W</m>,
            we're looking for a fairly general statement here.
            Since <m>U</m> is <m>T</m>-invariant, we must have <m>T(\uu_i)\in U</m> for each <m>i=1,\ldots, m</m>.
            Similarly, <m>T(\ww_j)\in W</m> for each <m>j=1,\ldots, n</m>. This means that we have
            <md>
              <mrow>T(\uu_1) \amp = a_{11}\uu_1 + \cdots + a_{m1}\uu_m + 0\ww_1+\cdots + 0\ww_n</mrow>
              <mrow>  \amp \vdots </mrow>
              <mrow>T(\uu_m) \amp = a_{1m}\uu_1 + \cdots + a_{mm}\uu_m+0\ww_1+\cdots + 0\ww_n</mrow>
              <mrow>T(\ww_1) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{11}\ww_1 + \cdots + b_{n1}\ww_n </mrow>
              <mrow> \amp \vdots </mrow>
              <mrow>T(\ww_n) \amp = 0\uu_1 + \cdots + 0\uu_m+b_{1n}\ww_1 + \cdots + b_{nn}\ww_n</mrow>
            </md>
            for some scalars <m>a_{ij},b_{ij}</m>. If we set <m>A = [a_{ij}]_{m\times m}</m>
            and <m>B = [b_{ij}]_{n\times n}</m>, then we have
            <me>
              M_B(T) = \bbm A \amp 0\\0\amp B\ebm
            </me>.
            Moreover, we can also see that <m>A = M_{B_U}(T|_U)</m>, and <m>B = M_{B_W}(T|_W)</m>.
          </p>
        </solution>
      </example>

    </subsection>

  </section>

  <section xml:id="sec-gen-eigen">
    <title>Generalized eigenspaces</title>
    <p>
      Example <xref ref="eg-invariant-block"/> showed us that if <m>V=U\oplus W</m>,
      where <m>U</m> and <m>W</m> are <m>T</m>-invariant,
      then the matrix <m>M_B(T)</m> has block diagonal form <m>\bbm A \amp 0\\0\amp B\ebm</m>,
      as long as the basis <m>B</m> is the union of bases of <m>U</m> and <m>W</m>.
    </p>

    <p>
      We want to take this idea further. If <m>V = U_1\oplus U_2\oplus \cdots \oplus U_k</m>,
      where each subspace <m>U_j</m> is <m>T</m>-invariant,
      then with repsect to a basis <m>B</m> consisting of basis vectors for each subspace,
      we will have
      <me>
        M_B(T)=\bbm A_1 \amp 0 \amp \cdots \amp 0\\
                    0 \amp A_2 \amp \cdots \amp 0\\
                    \vdots \amp \vdots \amp \ddots \amp \vdots\\
                    0 \amp 0 \amp \cdots \amp A_k\ebm
      </me>,
      where each <m>A_j</m> is the matrix of <m>T|_{U_j}</m> with respect to some basis of <m>U_j</m>.
    </p>

    <p>
      Our goal moving forward is twofold: one, to make the blocks as small as possible,
      so that <m>M_B(T)</m> is as close to diagonal as possible,
      and two, to make the blocks as simple as possible.
      Of course, if <m>T</m> is diagonalizable, then we can get all blocks down to size <m>1\times 1</m>,
      but this is not always possible.
    </p>

    <p>
      Recall from <xref ref="subsec-eigen-basics">Section</xref> that if the characteristic polynomial of <m>T</m>
      (or equivalently, any matrix representation <m>A</m> of <m>T</m>) is
      <me>
        c_T(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}
      </me>,
      then <m>\dim E_{\lambda_j}(T)\leq m_j</m> for each <m>j=1,\ldots, k</m>,
      and <m>T</m> is diagonalizable if and only if we have equality for each <m>j</m>.
      (This guarantees that we have sufficiently many independent eigenvectors to form a basis of <m>V</m>.)
    </p>

    <p>
      Since eigenspaces are <m>T</m>-invariant, we see that being able to diagonalize <m>T</m> is equivalent to having the direct sum decomposition
      <me>
        V = E_{\lambda_1}(T)\oplus E_{\lambda_2}(T)\oplus \cdots \oplus E_{\lambda_k}(T)
      </me>.
      If <m>T</m> cannot be diagonalized, it's because we came up short on the number of eigenvectors,
      and the direct sum of all eigenspaces only produces some subspace of <m>V</m> of lower dimension.
      We now consider how one might enlarge a set of independent eigenvectors in some standard,
      and ideally optimal, way.
    </p>

    <p>
      First, we note that for any operator <m>T</m>, the restriction of <m>T</m> to <m>\ker T</m> is the zero operator,
      since by definition, <m>T(\vv)=\zer</m> for all <m>\vv\in\ker T</m>.
      Since we define <m>E_{\lambda}(T)=\ker (T-\lambda I)</m>,
      it follows that <m>T-\lambda I</m> restricts to the zero operator on the eigenspace <m>E_\lambda(T)</m>.
      The idea is to relax the condition <q>identically zero</q> to something that will allow us to potentially enlarge some of our eigenspaces,
      so that we end up with enough vectors to span <m>V</m>.
    </p>

    <p>
      It turns out that the correct replacement for <q>identically zero</q> is <q>nilpotent</q>.
      What we would like to find is some subspace <m>G_\lambda(T)</m> such that the restriction of <m>T-\lambda I</m>
      to <m>G_\lambda(T)</m> will be nilpotent.
      (Recall that this means <m>(T-\lambda I)^k = 0</m> for some integer <m>k</m> when restricted to <m>G_\lambda(T)</m>.)
      The only problem is that we don't (yet) know what this subspace should be.
      To figure it out, we rely on some ideas you may have explored in your last assignment.
    </p>

    <theorem xml:id="thm-nullspace-power">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator. Then:
          <ol>
            <li>
              <m>\{\zer\}\subseteq \ker T \subseteq \ker T^2 \subseteq \cdots \subseteq \ker T^k\subseteq \cdots</m>
            </li>
            <li>
              <p>
                If <m>\ker T^{k+1}=\ker T^k</m> for some <m>k</m>, then <m>\ker T^{k+m}=\ker T^k</m> for all <m>m\geq 0</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>n=\dim V</m>, then <m>\ker T^{n+1} = \ker T^n</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>

    <p>
      In other words, for any operator <m>T</m>, the kernels of successive powers of <m>T</m> can get bigger,
      but the moment the kernel doesn't change for the next highest power,
      it stops changing for all further powers of <m>T</m>.
      That is, we have a sequence of kernels of strictly greater dimension until we reach a maximum,
      at which point the kernels stop growing. And of course, the maximum dimension cannot be more than the dimension of <m>V</m>.
    </p>

    <definition xml:id="def-generalized-eigenspace">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator, and let <m>\lambda</m> be an eigenvalue of <m>T</m>.
          The <term>generalized eigenspace</term> of <m>T</m> associated to the eigenvalue <m>\lambda</m>
          is denoted <m>G_\lambda(T)</m>, and defined as
          <me>
            G_\lambda(T) = \ker (T-\lambda I)^n
          </me>,
          where <m>n=\dim V</m>.
        </p>
      </statement>
    </definition>

    <p>
      Some remarks are in order. First, we can actually define <m>G_\lambda(T)</m> for any scalar <m>\lambda</m>.
      But this space will be trivial if <m>\lambda</m> is not an eigenvalue.
      Second, it is possible to show (although we will not do so here)
      that if <m>\lambda</m> is an eigenvalue with multiplicity <m>m</m>,
      then <m>G_\lambda(T)=\ker (T-\lambda I)^m</m>.
      (The kernel will usually have stopped growing well before we hit <m>n=\dim V</m>,
      but we know they're all eventually equal, so using <m>n</m> guarantees we have everything).
    </p>

    <p>
      We will not prove it here (see Nicholson, or Axler),
      but the advantage of using generalized eigenspaces is that they're just big enough to cover all of <m>V</m>.
    </p>

    <theorem xml:id="thm-gen-eigen-decomp">
      <statement>
        <p>
          Let <m>V</m> be a complex vector space, and let <m>T:V\to V</m> be a linear operator.
          (We can take <m>V</m> to be real if we assume that <m>T</m> has all real eigenvalues.)
          Let <m>\lambda_1,\ldots, \lambda_k</m> be the distinct eigenvalues of <m>T</m>.
          Then each generalized eigenspace <m>G_{\lambda_j}(T)</m> is <m>T</m>-invariant,
          and we have the direct sum decomposition
          <me>
            V = G_{\lambda_1}(T)\oplus G_{\lambda_2}(T)\oplus \cdots \oplus G_{\lambda_k}(T)
          </me>.
        </p>
      </statement>
    </theorem>


    <p>
      For each eigenvalue <m>\lambda_j</m> of <m>T</m>, let <m>l_j</m> denote the <em>smallest</em>
      integer power such that <m>G_{\lambda_j}(T) = (T-\lambda_j I)^{l_j}</m>.
      Then certainly we have <m>l_j\leq m_j</m> for each <m>j</m>.
      (Note also that if <m>l_j=1</m>, then <m>G_{\lambda_j}(T)=E_{\lambda_j}(T)</m>.)
    </p>


    <p>
      The polynomial <m>m_T(x) = (x-\lambda_1)^{l_1}(x-\lambda_2)^{l_2}\cdots (x-\lambda_k)^{l_k}</m>
      is the polynomial of <em>smallest degree</em> such that <m>m_T(T)=0</m>.
      The polynomial <m>m_T(x)</m> is called the <term>minimal polynomial</term> of <m>T</m>.
      Note that <m>T</m> is diagonalizable if and only if the minimal polynomial of <m>T</m> has no repeated roots.
    </p>

    <p>
      In <xref ref="sec-jordan-form"/>, we'll explore a systematic method for determining the generalized eigenspaces of a matrix,
      and in particular, for computing a basis for each generalized eigenspace,
      with respect to which the corresponding block in the block-diagonal form is especially simple.
    </p>
  </section>

  <section xml:id="sec-jordan-form">
    <title>Jordan Canonical Form</title>
    <p>
      The results of <xref ref="thm-nullspace-power"/> and <xref ref="thm-gen-eigen-decomp"/>
      tell us that for an eigenvalue <m>\lambda</m> of <m>T:V\to V</m> with multiplicity <m>m</m>,
      we have a sequence of subspace inclusions
      <me>
        E_\lambda(T) = \ker (T-\lambda I)\subseteq \ker (T-\lambda I)^2 \subseteq \cdots \subseteq \ker (T-\lambda I)^m = G_\lambda(T)
      </me>.
      Not all subspaces in this sequence are necessarily distinct.
      Indeed, it is entirely possible that <m>\dim E_\lambda(T)=m</m>,
      in which case <m>E_\lambda(T)=G_\lambda(T)</m>.
      In geeral there will be some <m>l\leq m</m> such that <m>\ker (T-\lambda I)^l=G_\lambda(T)</m>.
    </p>

    <p>
      Our goal in this section is to determine a basis for <m>G_\lambda(T)</m> in a standard way.
      We begin with a couple of important results, which we state without proof.
      The first can be found in Axler's book; the second in Nicholson's.
    </p>

    <theorem xml:id="thm-gen-eigen-props">
      <statement>
        <p>
          Suppose <m>V</m> is a complex vector space, and <m>T:V\to V</m> is a linear operator.
          Let <m>\lambda_1,\ldots, \lambda_k</m> denote the distinct eigenvalues of <m>T</m>.
          (We can assume <m>V</m> is real if we also assume that all eigenvalues of <m>V</m> are real.)
          Then:
          <ol>
            <li>
              <p>
                Generalized eigenvectors corresponding to <em>distinct</em> eigenvalues are linearly independent.
              </p>
            </li>
            <li><m>V = G_{\lambda_1}(T)\oplus G_{\lambda_2}(T)\oplus \cdots \oplus G_{\lambda_k}(T)</m></li>
            <li>
              <p>
                Each generalize eigenspace <m>G_{\lambda_j}(T)</m> is <m>T</m>-invariant.
              </p>
            </li>
            <li>
              <p>
                Each restriction <m>(T-\lambda_j)|_{G_{\lambda_j}(T)}</m> is nilpotent.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>

    <theorem xml:id="thm-block-eigen">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator. If the characteristic polynomial of <m>T</m> is given by
          <me>
            c_T(x) = (x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}
          </me>,
          then <m>\dim G_{\lambda_j}(T)=m_j</m> for each <m>j=1,\ldots, k</m>.
        </p>

        <p>
          Moreover, if we let <m>B=B_1\cup B_2\cup\cdots \cup B_k</m>,
          where <m>B_j</m> is any basis for <m>G_{\lambda_j}(T)</m> for <m>j=1,\ldots, k</m>,
          then <m>B</m> is a basis for <m>V</m> (this follows immediately from <xref ref="thm-gen-eigen-props"/>)
          and the matrix of <m>T</m> with respect to this basis has the block-diagonal form
          <me>
            M_B(T) = \bbm A_1 \amp 0 \amp \cdots \amp 0\\
                          0 \amp A_2 \amp \cdots \amp 0\\
                          \vdots\amp\vdots\amp\ddots\amp\vdots\\
                          0 \amp 0 \amp \cdots \amp A_k\ebm
          </me>,
          where each <m>A_j</m> has size <m>m_j\times m_j</m>.
        </p>
      </statement>
    </theorem>

    <p>
      A few remarks are called for here.
      <ul>
        <li>
          <p>
            One of the ways to see that <m>\dim G_{\lambda_j}(T)=m_j</m> is to consider <m>(M_B(T)-\lambda_j I_n)^{m_j}</m>.
            This will have the form <m>\diag(U_1^{m_j}, U_2^{m_j},\ldots, U_k^{m_j})</m>,
            where <m>U_i</m> is the matrix of <m>(T-\lambda_j)^{m_j}</m>,
            restricted to <m>G_{\lambda_i}(T)</m>.
            If <m>i\neq j</m>, <m>T-\lambda_j I</m> restricts to an invertible operator on <m>G_{\lambda_i}(T)</m>,
            but its restricton to <m>G_{\lambda_j}(T)</m> is nilpotent, by <xref ref="thm-gen-eigen-props"/>.
            So <m>U_j</m> is nilpotent (with <m>U_j^{m_j}=0</m>), and has size <m>m_j\times m_j</m>,
            while <m>U_i</m> is invertible if <m>i\neq j</m>.
            The matrix <m>(M_B(T)-\lambda_j I)^{m_j}</m> thus ends up with a <m>m_j\times m_j</m> block of zeros,
            so <m>\dim \ker (T-\lambda_j I)^{m_j}=m_j</m>.
          </p>
        </li>

        <li>
          <p>
            If the previous point wasn't clear, note that with an appropriate choice of basis,
            the block <m>A_i</m> in <xref ref="thm-block-eigen"/> has the form
            <me>
              A_i = \bbm \lambda_i \amp \ast \amp \cdots \amp \ast\\
                         0 \amp \lambda_i \amp \cdots \amp \ast\\
                         \vdots \amp \vdots \amp \ddots\amp \vdots\\
                         0 \amp 0 \amp \cdots \amp \lambda_i\ebm
            </me>.
            Thus, <m>M_B(T)-\lambda_j I</m> will have blocks that are upper triangular,
            with diagonal entries <m>\lambda_i-\lambda_j\neq 0</m> when <m>i\neq j</m>,
            but when <m>i=j</m> we get a matrix that is strictly upper triangular,
            and therefore nilpotent, since its diagonal entries will be <m>\lambda_j-\lambda_j=0</m>.
          </p>
        </li>

        <li>
          <p>
            if <m>l_j</m> is the <em>least</em> integer such that <m>\ker (A-\lambda_j I)^{l_j}=G_{\lambda_j}(T)</m>,
            then it is possible to choose the basis of <m>G_{\lambda_j}(T)</m> so that <m>A_j</m> is itself block-diagonal,
            with the largest block having size <m>l_j\times l_j</m>.
            The remainder of this section is devoted to determining how to choose such a basis.
          </p>
        </li>
      </ul>
    </p>

    <p>
      The basic principle for choosing a basis for each generalized eigenspace is as follows.
      We know that <m>E_{\lambda}(T)\subseteq G_\lambda(T)</m> for each eigenvalue <m>\lambda</m>.
      So we start with a basis for <m>E_\lambda(T)</m>, by finding eigenvectors as usual.
      If <m>\ker (T-\lambda I)^2 = \ker (T-\lambda I)</m>, then we're done: <m>E_\lambda(T)=G_\lambda(T)</m>.
      Otherwise, we enlarge the basis for <m>E_\lambda(T)</m> to a basis of <m>\ker T(-\lambda I)^2</m>.
      If <m>\ker T(-\lambda I)^3=\ker (T-\lambda I)^2</m>, then we're done,
      and <m>G_\lambda(T) = \ker (T-\lambda I)^2</m>.
      If not, we enlarge our existing basis to a basis of <m>\ker (T-\lambda I)^3</m>.
      We continue this process until we reach some power <m>l</m> such that <m>\ker (T-\lambda I)^l = \ker (T-\lambda I)^{l+1}</m>.
      (This is guaranteed to happen by <xref ref="thm-nullspace-power"/>.)
      We then conclude that <m>G_\lambda(T) = \ker (T-\lambda I)^l</m>.
    </p>

    <p>
      The above produces <em>a</em> basis for <m>G_\lambda(T)</m>,
      but we want what is, in some sense, the <q>best</q> basis.
      For our purposes, the best basis is the one in which the matrix of <m>T</m> restricted to each generalized eigenspace is block diaganal,
      where each block is a <em>Jordan block</em>.
    </p>

    <definition xml:id="def-jordan-block">
      <statement>
        <p>
          Let <m>\lambda</m> be a scalar. A <term>Jordan block</term> is an <m>m\times m</m> matrix of the form
          <me>
            J(m,\lambda) = \bbm \lambda \amp 1 \amp 0 \amp \cdots \amp 0\\
                                0 \amp \lambda \amp 1 \amp \cdots \amp 0\\
                                \vdots \amp \vdots \amp \ddots \amp \ddots \amp \vdots\\
                                0 \amp 0 \amp \cdots \amp \lambda \amp 1\\
                                0 \amp 0 \amp 0 \amp \cdots \amp \lambda\ebm
          </me>.
          That is <m>J(m,\lambda)</m> has each diagaonal entry equal to <m>\lambda</m>,
          and each <q>superdiagonal</q> entry (those just above the diagaonal) equal to 1,
          with all other entries equal to zero.
        </p>
      </statement>
    </definition>

    <example>
      <statement>
        <p>
          The following are examples of Jordan blocks:
          <me>
            J(2,4)=\bbm 4 \amp 1\\ 0\amp 4\ebm, J(3,\sqrt{2})=\bbm \sqrt{2} \amp 1\amp 0\\0\amp \sqrt{2}\amp 1\\0\amp 0\amp \sqrt{2}\ebm,
            J(4,2i)=\bbm 2i \amp 1\amp 0\amp 0\\0\amp 2i\amp 1\amp 0\\0\amp 0\amp 2i\amp 1\\0\amp 0\amp 0\amp 2i\ebm
          </me>.
        </p>
      </statement>
    </example>

    <insight xml:id="jordan-chain-basis">
      <title>Finding a chain basis </title>
      <p>
        A Jordan block corresponds to basis vectors <m>\vv_1,\vv_2,\ldots, \vv_m</m> with the following properties:
        <md>
          <mrow>T(\vv_1) \amp = \lambda \vv_1</mrow>
          <mrow>T(\vv_2) \amp = \vv_1+\lambda \vv_2</mrow>
          <mrow>T(\vv_3) \amp = \vv_2+\lambda \vv_3</mrow>
        </md>,
        and so on. Notice that <m>\vv_1</m> is an eigenvector, and for each <m>j=2,\ldots, m</m>,
        <me>
          (T-\lambda I)\vv_{j} = \vv_{j-1}
        </me>.
      </p>

      <p>
        Notice also that if we set <m>N=T-\lambda I</m>, then
        <me>
          \vv_1 = N\vv_2, \vv_2 = N\vv_3, \ldots, \vv_{m-1} = N\vv_m
        </me>
        so our basis for <m>G_\lambda(T)</m> is of the form
        <me>
          \vv, N\vv, N^2\vv, \ldots, N^{m-1}\vv
        </me>,
        where <m>\vv = \vv_m</m>, and <m>\vv_1=N^{m-1}\vv</m> is an eigenvector.
        (Note that <m>N^m\vv = (T-\lambda I)\vv_1=\zer</m>,
        and indeed <m>N^m\vv_j=\zer</m> for each <m>j=1,\ldots, m</m>.)
        Such a basis is known as a <term>chain basis</term>.
      </p>

      <p>
        If <m>\dim E_\lambda(T)\gt 1</m> we might have to repeat this process for each eigenvector in a basis for the eigenspace.
        The full matrix of <m>T</m> might have several Jordan blocks of possibly different sizes for each eigenvalue.
      </p>
    </insight>

    <example xml:id="ex-jordan-form1">
      <statement>
        <p>
          Determine a Jordan basis for the operator <m>T:\R^5\to \R^5</m>
          whose matrix with respect to the standard basis is given by
          <me>
            A = \bbm 7\amp 1 \amp -3\amp 2\amp 1\\
                    -6\amp 2\amp 4\amp -2\amp -2\\
                    0 \amp 1\amp 3 \amp 1\amp -1\\
                    -8\amp -1\amp 6\amp 0 \amp-3\\
                    -4\amp 0\amp 3\amp -1\amp 1\ebm
          </me>
        </p>
      </statement>
      <solution>
        <p>
          The characteristic polynomial of <m>A</m> is given by
          <me>
            c_A(x)=(x-2)^2(x-3)^3
          </me>.
          (See the Sage cell below to confirm this.)
          We thus have two eigenvalues: <m>2</m>, of multiplicity <m>2</m>,
          and <m>3</m>, of multiplicity <m>3</m>.
          We find
          <me>
            E_2(A)=\nll(A-2I) = \spn\{\xx_1\}, \text{ where } \xx_1=\bbm -1\\0\\-1\\1\\0\ebm
          </me>,
          so we have only one independent eigenvector, which means that <m>G_2(A)=\nll(A-2I)^2</m>.
        </p>

        <p>
          Following <xref ref="jordan-chain-basis"/>, we extend <m>\{\xx_1\}</m>
          to a basis of <m>G_2(A)</m> by solving the system
          <me>
            (A-2I)\xx = \xx_1
          </me>.
          Performing Gaussian elimination, we find a general solution
          <me>
            \xx = \bbm -t\\-1\\-t\\t\\0\ebm = t\bbm -1\\0\\-1\\1\\0\ebm + \bbm 0\\-1\\0\\0\\0\ebm
          </me>.
          Note that our solution is of the form <m>\xx = t\xx_1+\xx_2</m>.
          We set <m>t=0</m>, and get <m>\xx_2 = \bbm 0\amp -1\amp 0\amp 0\amp 0\ebm^T</m>.
        </p>

        <p>
          Next, we consider the eigenvalue <m>\lambda=3</m>. We find
          <me>
            E_3(A) = \nll(A-3I) = \spn\{\yy_1,\yy_2\}, \text{ where } \yy_1 = \bbm 1\\-2\\2\\2\\0\ebm, \yy_2 = \bbm -1\\2\\0\\0\\2\ebm
          </me>.
          Again, we're one eigenvector short of the multiplicity, so we need to consider <m>G_3(A)=\nll(A-3I)^3</m>.
          Unfortunately, the systems <m>(A-3I)\yy = \yy_1</m> and <m>(A-3I)\yy=\yy_2</m> are both inconsistent!
        </p>

        <p>
          We can salvage the situation by replacing the eigenvector <m>\yy_2</m> by some linear combination
          <m>\zz_2 = a\yy_1+b\yy_2</m>. We row-reduce, and look for values of <m>a</m> and <m>b</m> that give a consistent system.
          We find that <m>a=b</m> does the job, so we set
          <me>
            \zz_2 = \yy_1+\yy_2 = \bbm 0\\0\\2\\2\\2\ebm
          </me>.
          Solving the system <m>(A-3I)\zz = \yy_1+\yy_2</m>, we find
          <md>
            <mrow>\zz \amp = \bbm \frac12 +\frac12 s-\frac12 t\\1-s+t\\1+s\\s\\t\ebm</mrow>
            <mrow>\amp = \bbm \frac12\\1\\1\\0\\0\ebm + s\bbm\frac12\\-1\\1\\1\\0\ebm+t\bbm -\frac12\\1\\0\\0\\1\ebm</mrow>
            <mrow>\amp = \bbm \frac12\\1\\1\\0\\0\ebm \frac{s}{2}\yy_1+\frac{t}{2}\yy_2</mrow>
          </md>.
          We let <m>\zz_3 = \bbm \frac 1\\2\\2\\0\\0\ebm</m>, and check that
          <me>
            A\zz_3 = 3\zz_3+\zz_2
          </me>,
          as required.
        </p>

        <p>
          This gives us the basis <m>B = \{\xx_1,\xx_2,\yy_1,\zz_2,\zz_3\}</m> for <m>\R^5</m>,
          and with respect to this basis, we have the Jordan canonical form
          <me>
            M_B(T) = \bbm 2 \amp 1\amp 0\amp 0 \amp 0\\
                          0 \amp 2\amp 0\amp 0 \amp 0\\
                          0 \amp 0\amp 3\amp 0 \amp 0\\
                          0 \amp 0\amp 0\amp 3 \amp 1\\
                          0 \amp 0\amp 0\amp 0 \amp 3\ebm
          </me>.
        </p>
      </solution>
    </example>

    <p>
      The following Sage cells perform the computations needed for <xref ref="ex-jordan-form1"/>.
      First, we get the characteristic polynomial.
    </p>
    <sage>
      <input>
        from sympy import *
        init_printing()
        A = Matrix([[7,1,-3,2,1],[-6,2,4,-2,-2],[0,1,3,1,-1],[-8,-1,6,0,-3],[-4,0,3,-1,1]])
        p = A.charpoly()
        factor(p)
      </input>
    </sage>

    <p>
      Next, we determine a basis for the <m>E_2(A)</m> eigenspace.
    </p>
    <sage>
      <input>
        N2 = A-2*eye(5)
        E2 = N2.nullspace()
        E2
      </input>
    </sage>

    <p>
      Next, we solve the system <m>(A-2I)\xx_2=\xx_1</m>.
    </p>

    <sage>
      <input>
        B2 = N2.col_insert(5,E2[0])
        B2.rref()
      </input>
    </sage>

    <p>
      Now, we repeat for the eigenvalue <m>\lambda=3</m>.
      Make sure you've run the previous cells to define the matrix <m>A</m>.
    </p>

    <sage>
      <input>
        N3 = A-3*eye(5)
        E3 = N3.nullspace()
        E3
      </input>
    </sage>

    <p>
      In the next cell, note that we doubled the eigenvectors in <c>E3</c> to avoid fractions.
      To follow the solution in our example, we append <c>2*E3[0]</c>.
      You should find that using the eigenvector <m>\yy_1</m> corresponding to <c>E3[0]</c>
      leads to an inconsistent system. Once you confirm this, replace <c>E3[0]</c> with <c>E3[1]</c>
      and re-run the cell to see that we get an inconsistent system using <m>\yy_2</m> as well!
    </p>
    <sage>
      <input>
        B3 = N3.col_insert(5,2*E3[0])
        B3.rref()
      </input>
    </sage>

    <p>
      We now want to try a linear combination <m>a\yy_1+b\yy_2</m>,
      and see what values of <m>a</m> and <m>b</m> give a consistent system.
      The <c>rref</c> command takes things a bit farther than we'd like,
      but adding the optional argument <c>simplify=True</c> leaves us with a clue:
      SymPy has to divide by <m>32a-32b</m> to get to RREF, so if <m>a=b</m>,
      something interesing happens.
    </p>

    <sage>
      <input>
        a = Symbol('a')
        b = Symbol('b')
        C3 = N3.col_insert(5,a*E3[0]+b*E3[1])
        C3.rref(simplify=True)
      </input>
    </sage>

    <sage>
      <input>
        D3 = N3.col_insert(5,E3[0]+E3[1])
        D3.rref()
      </input>
    </sage>

    <p>
      Finally, we check that our last vector works.
    </p>

    <sage>
      <input>
        Z3 = Matrix([1,2,2,0,0])
        A-3*Z3-2*(E3[0]+E3[1])
      </input>
    </sage>

    <p>
      Now that we've done all the work required for <xref ref="ex-jordan-form1"/>,
      we should confess that there was an easier way all along:
    </p>

    <sage>
      <input>
        A.jordan_form()
      </input>
    </sage>

    <p>
      The <c>jordan_form()</c> command returns a pair <m>P,J</m>,
      where <m>J</m> is the Jordan canonical form of <m>A</m>,
      and <m>P</m> is an invertible matrix such that <m>P^{-1}AP=J</m>.
      You might find that the computer's answer is not quite the same as ours.
      This is because the Jordan canonical form is only unique up to permuation of the Jordan blocks.
      Changing the order of the blocks amounts to changing the order of the columns of <m>P</m>,
      which are given by a basis of (generalized eigenvectors).
    </p>
  </section>
</chapter>
