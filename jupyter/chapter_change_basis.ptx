<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-change-basis">
  <title>Change of Basis</title>
  <section xml:id="sec-matrix-of-transformation">
    <title>The matrix of a linear transformation</title>
    <p>
      Recall from <xref ref="ex-matrix-trans">Example</xref> in <xref ref="ch-linear-trans">Chapter</xref>
      that given any <m>m\times n</m> matrix <m>A</m>, we can define the matrix transformation
      <m>T_A:\R^n\to \R^m</m> by <m>T_A(\xx)=A\xx</m>,
      where we view <m>\xx\in\R^n</m> as an <m>n\times 1</m> column vector.
    </p>

    <p>
      Conversely, given any linear map <m>T:\R^n\to \R^m</m>,
      if we let <m>\basis{e}{n}</m> denote the standard basis of <m>\R^n</m>,
      then the matrix
      <me>
        A = \bbm T(\mathbf{e}_1) \amp T(\mathbf{e}_2) \amp \cdots \amp T(\mathbf{e}_n)\ebm
      </me>
      is such that <m>T=T_A</m>.
    </p>

    <p>
      We have already discussed the fact that this idea generalizes:
      given a linear transformation <m>T:V\to W</m>,
      where <m>V</m> and <m>W</m> are finite-dimensional vector spaces,
      it is possible to represent <m>T</m> as a matrix transformation.
    </p>

    <p>
      The representation depends on choices of bases for both <m>V</m> and <m>W</m>.
      Recall the definition of the coefficient isomorphism,
      from <xref ref="def-coefficient-iso">Definition</xref> in <xref ref="sec-isomorphism">Section</xref>.
      If <m>\dim V=n</m> and <m>\dim W=m</m>,
      this gives us isomorphisms <m>C_B:V\to \R^n</m> and <m>C_D:W\to \R^m</m>
      depending on the choice of a basis <m>B</m> for <m>V</m> and a basis <m>D</m> for <m>W</m>.
      These isomorphisms define a matrix transformation <m>T_A:\R^n\to \R^m</m>
      according to the diagram we gave in <xref ref="fig_transformation_matrix">Figure</xref>.
    </p>

    <p>
      We should stress one important point about the coefficient isomorphism, however.
      It depends on the choice of basis, but also on the <em>order</em> of the basis elements.
      Thus, we generally will work with an <em>ordered basis</em> in this chapter.
      That is, rather than simply thinking of our basis as a set,
      we will think of it as an ordered list.
      Order matters, since given a basis <m>B=\basis{e}{n}</m>,
      we rely on the fact that we can write any vector <m>\vv</m> uniquely as
      <me>
        \vv = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n
      </me>
      in order to make the assignment <m>C_B(\vv) = \bbm c_1\\\vdots \\c_n\ebm</m>.
    </p>

    <exercise>
      <statement>
        <p>
          Show that the coefficient isomorphism is, indeed,
          a linear isomorphism from <m>V</m> to <m>\R^n</m>.
        </p>
      </statement>
      <solution>
        <p>
          It's clear that <m>C_B(\mathbf{0})=\vec{0}</m>,
          since the only way to write the zero vector in <m>V</m> in terms of <m>B</m>
          (or, indeed, any independent set) is to set all the scalars equal to zero.
        </p>

        <p>
          If we have two vectors <m>\vv,\ww</m> given by
          <md>
            <mrow>\vv \amp = a_1\mathbf{e}_1+\cdots + a_n\mathbf{e}_n </mrow>
            <mrow>\ww \amp = b_1\mathbf{e}_1+\cdots + b_n\mathbf{e}_n</mrow>
          </md>,
          then
          <me>
            \vv+\ww = (a_1+b_1)\mathbf{e}_1+\cdots + (a_n+b_n)\mathbf{e}_n
          </me>,
          so
          <md>
            <mrow>C_B(\vv+\ww) \amp = \bbm a_1+b_1\\vdots \\ a_n+b_n\ebm </mrow>
            <mrow> \amp = \bbm a_1\\\vdots\\a_n\ebm +\bbm b_1\\\vdots \\b_n\ebm</mrow>
            <mrow>  \amp = C_B(\vv)+C_B(\ww)</mrow>
          </md>.
        </p>

        <p>
          Finally, for any scalar <m>c</m>, we have
          <md>
            <mrow>C_B(c\vv) \amp = C_B((ca_1)\mathbf{e}_1+\cdots +(ca_n)\mathbf{e}_n)</mrow>
            <mrow> \amp = \bbm ca_1\\\vdots \\ca_n\ebm</mrow>
            <mrow>  \amp =c\bbm a_1\\\vdots \\a_n\ebm</mrow>
            <mrow>  \amp =cC_B(\vv)</mrow>
          </md>.
        </p>

        <p>
          This shows that <m>C_B</m> is linear.
          To see that <m>C_B</m> is an isomorphism, we can simply note that <m>C_B</m>
          takes the basis <m>B</m> to the standard basis of <m>\R^n</m>.
          Alternatively, we can give the inverse: <m>C_B^{-1}:\R^n\to V</m> is given by
          <me>
            C_B^{-1}\bbm c_1\\\vdots \\c_n\ebm = c_1\mathbf{e}_1+\cdots +c_n\mathbf{e}_n
          </me>.
        </p>
      </solution>
    </exercise>

    <p>
      Given <m>T:V\to W</m> and coefficient isomorphisms <m>C_B:V\to \R^n, C_D:W\to \R^m</m>,
      the map <m>C_DTC_B^{-1}:\R^n\to \R^m</m> is a linear transformation,
      and the matrix of this transformation gives a representatioon of <m>T</m>.
      Explicitly, let <m>B = \basis{v}{n}</m> be an ordered basis for <m>V</m>,
      and let <m>D=\basis{w}{m}</m> be an ordered basis for <m>W</m>.
      Since <m>T(\vv_i)\in W</m> for each <m>\vv_i\in B</m>,
      there exist unique scalars <m>a_{ij}</m>, with <m>1\leq i\leq m</m>
      and <m>1\leq j\leq n</m> such that
      <me>
        T(\vv_j) = a_{1j}\ww_1+a_{2j}\ww_2+\cdots + a_{mj}\ww_m
      </me>
      for <m>j=1,\ldots, n</m>. This gives us the <m>m\times n</m> matrix <m>A = [a_{ij}]</m>.
      Notice that the first column of <m>A</m> is <m>C_D(T(\vv_1))</m>,
      the second column is <m>C_D(T(\vv_2))</m>, and so on.
    </p>

    <p>
      Given <m>\xx\in V</m>, write <m>\xx = c_1\vv_1+\cdots + c_n\vv_n</m>,
      so that <m>C_B(\xx) = \bbm c_1\\\vdots \\c_n\ebm</m>. Then
      <me>
        T_A(C_B(\xx)) = \bbm a_{11}\amp a_{12} \amp \cdots \amp a_{1n}\\
                             a_{21}\amp a_{22} \amp \cdots \amp a_{2n}\\
                             \vdots \amp \vdots \amp \ddots \amp \vdots\\
                             a_{m1}\amp a_{m2} \amp \cdots \amp a_{mn}\ebm\bbm c_1\\c_2\\ \vdots \\c_n\ebm
                             = \bbm a_{11}c_1+a_{12}c_2+\cdots +a_{1n}c_n\\
                                    a_{21}c_1+a_{22}c_2+\cdots +a_{2n}c_n\\
                                    \vdots
                                    a_{m1}c_1+a_{m2}c_2+\cdots +a_{mn}c_n\ebm
      </me>.
    </p>

    <p>
      On the other hand,
      <md>
        <mrow>T(\xx) \amp = T(c_1\vv_1+\cdots + c_n\vv_n) </mrow>
        <mrow> \amp = c_1T(\vv_1)+\cdots + c_nT(\vv_n)</mrow>
        <mrow> \amp = c_1(a_{11}\ww_1+\cdots + a_{m1}\ww_m)+\cdots c_n(a_{1n}\ww_1+\cdots + a_{mn}\ww_m)</mrow>
        <mrow> \amp = (c_1a_{11}+\cdots + c_na_{1n})\ww_1 + \cdots + (c_1a_{m1}+\cdots + c_na_{mn})\ww_m</mrow>
      </md>.
      Therefore,
      <me>
        C_D(T(\xx)) = \bbm c_1a_{11}+\cdots + c_na_{1n}\\ \vdots \\ c_1a_{m1}+\cdots + c_na_{mn}\ebm = T_A(C_B(\xx))
      </me>.
      Thus, we see that <m>C_DT = T_AC_B</m>, or <m>T_A = C_DTC_B^{-1}</m>, as expected.
    </p>

    <definition xml:id="def-transformation-matrix">
      <title>The matrix <m>M_{DB}(T)</m> of a linear map</title>

      <statement>
        <p>
          Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces,
          and let <m>T:V\to W</m> be a linear map.
          Let <m>B=\basis{v}{n}</m> and <m>D=\basis{w}{m}</m> be ordered bases for <m>V</m> and <m>W</m>,
          respectively. Then the <term>matrix</term> <m>M_{DB}(T)</m> of <m>T</m> with respect to the bases <m>B</m> and <m>D</m>
          is defined by
          <me>
            M_{DB}(T) = \bbm C_D(T(\vv_1)) \amp C_D(T(\vv_2)) \amp \cdots \amp C_D(T(\vv_n))\ebm
          </me>.
        </p>
      </statement>
    </definition>

    <p>
      In other words, <m>A=M_{DB}(T)</m> is the unique <m>m\times n</m> matrix such that <m>C_DT = T_AC_B</m>.
      This gives the defining property
      <me>
        C_D(T(\vv)) = M_{DB}(T)C_B(\vv)  \text{ for all } \vv\in V
      </me>,
      as was demonstrated above.
    </p>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:P_2(\R)\to \R^2</m> is given by
          <me>
            T(a+bx+cx^2) = (a+c,2b)
          </me>.
          Compute the matrix of <m>T</m> with respect to the bases
          <m>B = \{1,1-x,(1-x)^2\}</m> of <m>P_2(\R)</m> and
          <m>D = \{(1,0),(1,-1)\}</m> of <m>\R^2</m>.
        </p>
      </statement>
      <solution>
        <p>
          We have
          <md>
            <mrow>T(1) \amp = (1,0) = 1(1,0)+0(1,-1) </mrow>
            <mrow>T(1-x) \amp= (1,-2) = -1(1,0)+2(1,-1) </mrow>
            <mrow> T((1-x)^2) = T(1-2+x^2) \amp = (2, -4) = -2(1,0)+4(1,-1)</mrow>
          </md>.
          Thus,
          <md>
            <mrow>M_{DB}(T) \amp = \bbm C_D(T(1))\amp C_D(T(1-x)) \amp C_D(T((1-x)^2))\ebm</mrow>
            <mrow>\amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm</mrow>
          </md>.
          To confirm, note that
          <md>
            <mrow>M_{DB}(T)C_B\amp (a+bx+cx^2)</mrow>
            <mrow> \amp = M_{DB}(T)C_B((a+b+c)-(b+2c)(1-x)+c(1-x)^2)</mrow>
            <mrow> \amp = \bbm 1\amp -1\amp -2\\0\amp 2\amp 4\ebm\bbm a+b+c\\ -b-2c\\c\ebm</mrow>
            <mrow> \amp \bbm (a+b+c)+(b+2c)-2c\\0-2(b+2c)+4c\ebm = \bbm a+2b+c\\-2b\ebm</mrow>
          </md>,
          while on the other hand,
          <md>
            <mrow>C_D(T(a+bx+cx^2)) \amp= C_D(a+c,2b)</mrow>
            <mrow>\amp = C_D((a+2b+c)(1,0)-2b(1,-1))</mrow>
            <mrow>\amp = \bbm a+2b+c\\-2b\ebm</mrow>
          </md>.

        </p>
      </solution>
    </exercise>

    <p>
      When we compute the matrix of a transformation with respect to a non-standard basis,
      we don't have to worry about how to write vectors in the domain in terms of that basis.
      Instead, we simply plug the basis vectors into the transformation,
      and then determine how to write the output in terms of the basis of the codomain.
      However, if we want to <em>use</em> this matrix to compute values of <m>T:V\to W</m>,
      then we need a systematic way of writing elements of <m>V</m> in terms of the given basis.
    </p>

    <example>
      <title>Working with the matrix of a transformation</title>


      <statement>
        <p>
          Let <m>T:P_2(\R)\to \R^2</m> be a linear transformation whose matrix is given by
          <me>
            M(T) = \bbm 3\amp 0 \amp 3\\-1\amp -2\amp 2\ebm
          </me>
          with respect to the ordered bases <m>B = \{1+x, 2-x, 2x+x^2\}</m> of  <m>P_2(\R)</m>
          and <m>D = \{(0,1),(-1,1)\}</m> of <m>\R^2</m>. Find the value of <m>T(2+3x-4x^2)</m>.
        </p>
      </statement>
      <solution>
        <p>
          We need to write the input <m>2+3x-4x^2</m> in terms of the basis <m>B</m>.
          This amounts to solving the system of equations given by
          <me>
            a(1+x)+b(2-x)+c(2x+x^2)=2+3x-4x^2
          </me>.
          Of course, we can easily set up and solve this system,
          but let's try to be systematic, and obtain a more useful result for future problems.
          Since we can easily determine how to write any polynomial in terms of the standard basis <m>\{1,x,x^2\}</m>,
          it suffices to know how to write these three polynomials in terms of our basis.
        </p>

        <p>
          At first, this seems like more work. After all, we now have three systems to solve:
          <md>
            <mrow>a_1(x+1)+b_1(2-x)+c_1(2x+x^2) \amp =1</mrow>
            <mrow>a_2(x+1)+b_2(2-x)+c_2(2x+x^2) \amp =x</mrow>
            <mrow>a_3(x+1)+b_3(2-x)+c_3(2x+x^2) \amp x^2</mrow>
          </md>.
          However, all three systems have the same coefficient matrix,
          so we can solve them simultaneously, by adding three <q>constants</q>
          columns to our augmented matrix.
        </p>

        <p>
          We get the matrix
          <me>
            \left[\begin{matrix}1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\end{matrix}
            \right\rvert\left.\begin{matrix}1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\end{matrix}\right]
          </me>.
          But this is exactly the augmented matrix we'd right down if we were trying to find the inverse of the matrix
          <me>
            P=\bbm 1\amp 2\amp 0\\1\amp -1\amp 2\\0\amp 0\amp 1\ebm
          </me>
          whose columns are the coefficient representations of our given basis vectors in terms of the standard basis.
        </p>

        <p>
          We compute (using the Sage cell below) that
          <me>
            P^{-1}=\frac13\bbm 1\amp 2\amp -4\\1\amp -1\amp 2\\0\amp 0\amp 3\ebm
          </me>,
          and
          <me>
            M(T)P^{-1}=\bbm 1\amp 2\amp -1\\-1\amp 0\amp 2\ebm
          </me>.
          This matrix first converts the coefficient vector for a polynomial <m>p(x)</m>
          with respect to the standard basis into the coefficient vector for our given basis <m>B</m>,
          and then multiplies by the matrix representing our transformation.
          The result will be the coefficient vector for <m>T(p(x))</m> with repect to the basis <m>D</m>.
        </p>

        <p>
          The polynomial <m>p(x) = 2+3x-4x^2</m> has coefficient vector <m>\bbm 2\\3\\-4\ebm</m>
          with respect to the standard basis. We find that <m>M(T)P^{-1}\bbm 2\\3\\-4\ebm = \bbm 12\\-10\ebm</m>.
          The coefficients <m>12</m> and <m>-10</m> are the coefficients of <m>T(p(x))</m>
          with repsect to the basis <m>D</m>. Thus,
          <me>
            T(2+3x-4x^2) = 12(0,1)-10(-1,1) = (10,2)
          </me>.
          Note that in the last step we gave the <q>simplified</q> answer <m>(10,2)</m>,
          which is simplified primarily in that it is expressed with respect to the standard basis.
        </p>

        <p>
          Note that we can also introduce the matrix <m>Q = \bbm 0\amp -1\\1\amp 1\ebm</m>
          whose columns are the coefficient vectors of the vectors in the basis <m>D</m>
          with respect to the standard basis.
          The effect of multiplying by <m>Q</m> is to convert from coefficients with respect to <m>D</m>
          into a coefficient vector with respect to the standard basis.
          We can then write a new matrix <m>\widetilde{M}(T) = QM(T)P^{-1}</m>;
          this new matrix is now the matrix representation of <m>T</m> with respect to the
          <em>standard</em> bases of <m>P_2(\R)</m> and <m>\R^2</m>.
          We check that
          <me>
            \widetilde{M}(T)\bbm 2\\3\\-4\ebm = \bbm 10\\2\ebm
          </me>,
          as before.
        </p>

        <p>
          We find that <m>\tilde{M}(T) = \bbm 1\amp 0\amp -2\\0\amp 2\amp 1\ebm</m>.
          This lets us determine that for a general polynomial <m>p(x) = a+bx+cx^2</m>,
          <me>
            \widetilde{M}(T)\bbm a\\b\\c\ebm = \bbm a-2c\\2b+c\ebm
          </me>,
          and therefore, our original transformation must have been
          <me>
            T(a+bx+cx^2)=(a-2c,2b+c)
          </me>.

        </p>
      </solution>
    </example>

    <sage>
      <input>
        from sympy import *
        init_printing()
        P = Matrix(3,3,[1,2,0,1,-1,2,0,0,1])
        P**-1
      </input>
    </sage>

    <sage>
      <input>
        M = Matrix(2,3,[3,0,3,-1,-2,2])
        v = Matrix([2,3,-4])
        M*P**-1,v,(M*P**-1)*v
      </input>
    </sage>

    <sage>
      <input>
        Q = Matrix(2,2,[0,-1,1,1])
        Q*M*P**-1
      </input>
    </sage>

    <p>
      The previous example illustrated some important observations that are true in general.
      We won't give the general proof, but we sum up the results in a theorem.
    </p>

    <theorem xml:id="thm-change-basis-transformation">
      <statement>
        <p>
          Suppose <m>T:V\to W</m> is a linear transformation,
          and suppose <m>M_0 = M_{D_0B_0}(T)</m> is the matrix of <m>T</m> with resepct to bases <m>B_0</m> of <m>V</m> and <m>D_0</m> of <m>W</m>.
          Let <m>B_1=\basis{v}{n}</m> and <m>D_1\basis{w}{m}</m> be any other choice of basis for <m>V</m> and <m>W</m>, respectively.
          Let
          <md>
            <mrow>P \amp =\bbm C_{B_0}(\vv_1) \amp C_{B_0}(\vv_2) \amp \cdots \amp C_{B_0}(\vv_n)\ebm</mrow>
            <mrow>Q \amp =\bbm C_{B_0}(\ww_1) \amp C_{B_0}(\ww_2) \amp \cdots \amp C_{B_0}(\ww_n)\ebm</mrow>
          </md>
          be matrices whose columns are the coefficient vectors of the vectors in <m>B_1,D_1</m> with respect to <m>B_0,D_0</m>.
          Then the matrix of <m>T</m> with respect to the bases <m>B_1</m> and <m>D_1</m> is
          <me>
            M_{D_1B_1}(T) = QM_{D_0B_0}(T)P^{-1}
          </me>.
        </p>
      </statement>
    </theorem>

    <p>
      The relationship between the different maps is illustrated in <xref ref="fig-basis-cube"/> below.
      In this figure, the maps <m>V\to V</m> and <m>W\to W</m> are the identity maps,
      corresponding to representating the same vector with respect to two different bases.
      The vertical arrows are the coefficient isomorphisms <m>C_{B_0},C_{B_1},C_{D_0},C_{D_1}</m>.
    </p>

    <figure xml:id="fig-basis-cube">
      <image width="47%" source="basis-cube.svg"/>
    </figure>

    <p>
      We generally apply <xref ref="thm-change-basis-transformation"/> in the case that <m>B_0,D_0</m>
      are the <em>standard</em> bases for <m>V,W</m>, since in this case,
      the matrices <m>M_0, P, Q</m> are easy to determine,
      and we can use a computer to calculate <m>P^{-1}</m> and the product <m>QM_0P^{-1}</m>.
    </p>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:M_{22}(\R)\to P_2(\R)</m> has the matrix
          <me>
            M_{DB}(T) = \bbm 2\amp -1\amp 0\amp 3\\0\amp 4\amp -5\amp 1\\-1\amp 0\amp 3\amp -2\ebm
          </me>
          with respect to the bases
          <me>
            B = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 0\\0\amp 1\ebm\right\}
          </me>
          of <m>M_{22}(\R)</m> and <m>D=\{1,x,x^2\}</m> of <m>P_2(\R)</m>.
          Determine a formula for <m>T</m> in terms of a general input <m>X=\bbm a\amp b\\c\amp d\ebm</m>.
        </p>
      </statement>
      <solution>
        <p>
          We must first write our general input in terms of the given basis.
          With respect to the standard basis
          <me>
            B_0 = \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}
          </me>,
          we have the matrix <m>P = \bbm 1\amp 0\amp 0\amp 1\\0\amp 1\amp 1\amp 0\\0\amp 0\amp 1\amp 0\\0\amp 1\amp 0\amp 1\ebm</m>,
          representing the change from the basis <m>B</m> the basis <m>B_0</m>.
          The basis <m>D</m> of <m>P_2(\R)</m> is already the standard basis.
          For a matrix <m>X = \bbm a\amp b\\c\amp d\ebm</m> we find
          <me>
            M_{DB}(T)P^{-1}C_{B_0}(X)=\bbm 2\amp -2\amp 2\amp 1\\0\amp 3\amp -8\amp 1\\-1\amp 1\amp 2\amp -1\ebm\bbm a\\b\\c\\d\ebm =
            \bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm
          </me>.
          But this is equal to <m>C_D(T(X))</m>, so
          <md>
            <mrow>T\left(\bbm a\amp b\\c\amp d\ebm\right) \amp = C_D^{-1}\bbm 2a-2b+2c+d\\3b-8c+d\\-a+b+2c-d\ebm</mrow>
            <mrow>\amp = (2a-2b+2c+d)+(3b-8c+d)x+(-a+b+2c-d)x^2</mrow>
          </md>.

        </p>
      </solution>
    </exercise>

    <sage>
      <input>
        M = Matrix(3,4,[2,-1,0,3,0,4,-5,1,-1,0,3,-2])
        P = Matrix(4,4,[1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,1])
        M*P**-1
      </input>
    </sage>

    <p>
      In textbooks such as Sheldon Axler's <em>Linear Algebra Done Right</em> that focus primarily on linear transfomrations,
      the above construction of the matrix of a transformation with respect to choices of bases can be used as a primary motivation for introducing matrices,
      and determining their algebraic properties. In particular, the rule for matrix multiplication, which can seem peculiar at first,
      can be seen as a consequence of the composition of linear maps.
    </p>

    <theorem xml:id="thm-matrix-multiplication">
      <statement>
        <p>
          Let <m>U,V,W</m> be finite-dimensional vectors spaces,
          with ordered bases <m>B_1,B_2,B_3</m>, respectively.
          Let <m>T:U\to V</m> and <m>S:V\to W</m> be linear maps. Then
          <me>
            M_{B_3B_1}(ST) = M_{B_3B_2}(S)M_{B_2B_1}(T)
          </me>.
        </p>
      </statement>
      <proof>
        <p>
          Let <m>\xx\in U</m>. Then <m>C_{B_3}(ST(\xx)) = M_{B_3B_1}(ST)C_{B_1}(\xx)</m>.
          On the other hand,
          <md>
            <mrow> M_{B_3B_2}(S)M_{B_2B_1}(T)C_{B_1}(\xx) \amp  = M_{B_3B_2}(S)(C_{B_2}(T(\xx)))</mrow>
            <mrow> \amp = C_{B_3}(S(T(\xx))) = C_{B_3}(ST(\xx))</mrow>
          </md>.
          Since <m>C_{B_3}</m> is invertible, the result follows.
        </p>
      </proof>

    </theorem>

    <p>
      Being able to express a general linear transformation in terms of a matrix is useful,
      since questions about linear transformations can be converted into questions about matrices that we already know how to solve.
      In particular,
      <ul>
        <li>
          <p>
            <m>T:V\to W</m> is an isomorphism if and only if <m>M_{DB}(T)</m> is invertible for some
            (and hence, all) choice of bases <m>B</m> of <m>V</m> and <m>D</m> of <m>W</m>.
          </p>
        </li>

        <li>
          <p>
            The rank of <m>T</m>  is equal to the rank of <m>M_{DB}(T)</m> (and this does not depend on the choice of basis).
          </p>
        </li>

        <li>
          <p>
            The kernel of <m>T</m> is isomorphic to the nullspace of <m>M_{DB}(T)</m>.
          </p>
        </li>
      </ul>
    </p>

    <p>
      Next, we will want to look at two topics in particular.
      First, if <m>T:V\to V</m> is a linear operator, then it makes sense to consider the matrix <m>M_B(T)=M_{BB}(T)</m>
      obtained by using the same basis for both domain and codomain.
      Second, we will want to know how this matrix changes if we change the choice of basis.
    </p>
  </section>

  <section xml:id="sec-matrix-operator">
    <title>The matrix of a linear operator</title>
    <p>
      Recall that a linear transformation <m>T:V\to V</m> is referred to as a <term>linear operator</term>.
      Recall also that two matrices <m>A</m> and <m>B</m> are <term>similar</term>
      if there exists an invertible matrix <m>P</m> such that <m>B = PAP^{1-}</m>,
      and that similar matrices have a lot of properties in common.
      In particular, if <m>A</m> is similar to <m>B</m>, then <m>A</m> and <m>B</m>
      have the same trace, determinant, and eigenvalues.
      One way to understand this is the realization that two matrices are similar if they are representations of the
      <em>same</em> operator, with respect to <em>different</em> bases.
    </p>

    <p>
      Since the domain and codomain of a linear operator are the same,
      we can consider the matrix <m>M_{DB}(T)</m> where <m>B</m> and <m>D</m> are the <em>same</em> ordered basis.
      This leads to the next definition.
    </p>

    <definition xml:id="def-operator-matrix">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator,
          and let <m>B=\basis{b}{n}</m> be an ordered basis of <m>V</m>.
          The matrix <m>M_B(T)=M_{BB}(T)</m> is called the <term><m>B</m>-matrix</term> of <m>T</m>.
        </p>
      </statement>
    </definition>

    <p>
      The following result collects several useful properties of the <m>B</m>-matrix of an operator.
      Most of these were already encountered for the matrix <m>M_{DB}(T)</m> of a transformation,
      although not all were stated formally. (Formal statements can be found in the textbook by Nicholson.)
    </p>

    <theorem xml:id="thm-bmatrix-properties">
      <statement>
        <p>
          Let <m>T:V\to V</m> be a linear operator, and let <m>B=\basis{b}{n}</m> be a basis for <m>V</m>.
          Then
          <ol>
            <li><m>C_B(T(\vv))=M_B(T)C_B(\vv)</m> for all <m>\vv\in V</m>.</li>
            <li>If <m>S:V\to V</m> is another operator, then <m>M_B(ST)=M_B(S)M_B(T)</m>.</li>
            <li><m>T</m> is an isomorphism if and only if <m>M_B(T)</m> is invertible.</li>
            <li>If <m>T</m> is an isomorphism, then <m>M_B(T^{-1}) = [M_B(T)]^{-1}</m>.</li>
            <li><m>M_B(T)=\bbm C_B(\mathbf{b}_1) \amp \cdots \amp C_B(\mathbf{b}_n)\ebm</m>.</li>
          </ol>
        </p>
      </statement>
    </theorem>


    <example>
      <statement>
        <p>
          Find the <m>B</m>-matrix of the operator <m>T:P_2(\R)\to P_2(\R)</m> given by
          <m>T(p(x))=p(0)(1+x^2)+p(1)x</m>, with respect to the ordered basis
          <me>
            B = \{1-x, x+3x^2, 2-x^2\}
          </me>.
        </p>
      </statement>
      <solution>
        <p>
          We compute
          <md>
            <mrow>T(1-x) \amp = 1(1+x^2)+0(x) = 1+x^2</mrow>
            <mrow>T(x+3x^2) \amp = 0(1+x^2)+4x=4x</mrow>
            <mrow>T(2-x^2) \amp = 2(1+x^2)+1(x) = 2+x+2x^2</mrow>
          </md>.
          We now need to write each of these in terms of the basis <m>B</m>.
          We can do this by working out how to write each polynomial above in terms of <m>B</m>.
          Or we can be systematic.
        </p>

        <p>
          Let <m>P = \bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm</m>
          be the matrix whose columns are given by the coefficient representations of the polynomials in <m>B</m>
          with respect to the <em>standard basis</em> <m>B_0=\{1,x,x^2\}</m>.
          For <m>T(1-x)=1+x^2</m> we need to solve the equation
          <me>
            a(1-x)+b(x+3x^2)+c(2-x^2)=1+x^2
          </me>
          for scalars <m>a,b,c</m>. But this is equivalent to the system
          <md>
            <mrow>a+2c \amp =1</mrow>
            <mrow>-a+b \amp =0</mrow>
            <mrow>3b-c \amp =1</mrow>
          </md>,
          which, in turn, is equivalent to the matrix equation
          <me>
            \bbm 1\amp 0\amp 2\\-1\amp 1\amp 0\\0\amp 3\amp -1\ebm\bbm a\\b\\c\ebm = \bbm 1\\0\\1\ebm
          </me>;
          that is, <m>PC_B(1+x^2) = C_{B_0}(1+x^2)</m>. Thus,
          <me>
            C_B(1+x^2) = \bbm a\\b\\c\ebm = P^{-1}C_{B_0}(1+x^2) = P^{-1}\bbm 1\\0\\1\ebm
          </me>.
          Similarly, <m>C_B(4x)=P^{-1}\bbm 0\\4\\0\ebm = P^{-1}C_{B_0}(4x)</m>, and
          <m>C_B(2+x+2x^2)=P^{-1}\bbm 2\\1\\2\ebm = P^{-1}C_{B_0}(2+x+2x^2)</m>.
          Thus, (using the Sage code below)
          <md>
            <mrow>M_B(T) \amp =\bbm C_B(T(1-x)) \amp C_B(T(x+3x^2)) \amp C_B(T(2-x^2))\ebm</mrow>
            <mrow> \amp =\bbm P^{-1}C_{B_0}T(1-x) \amp P^{-1} C_{B_0}T(x+3x^2) \amp P^{-1}C_{B_0}(T(2-x^2))\ebm</mrow>
            <mrow> \amp =P^{-1}\bbm C_{B_0}(1+x^2) \amp C_{B_0}(4x)\amp C_{B_0}(2+x+2x^2)\ebm</mrow>
            <mrow> \amp =\bbm 1/7\amp -6/7\amp 3/7\\1/7\amp 1/7\amp 2/7\\3/7\amp 3/7\amp -1/7\ebm
                         \bbm 1\amp 0\amp 2\\0\amp 4\amp 1\\1\amp 0\amp 2\ebm</mrow>
            <mrow> \amp \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm</mrow>
          </md>.
        </p>

        <p>
          Let's confirm that this works. Suppose we have
          <me>
            p(x)= C_B^{-1}\bbm a\\b\\c\ebm = a(1-x)+b(x+3x^2)+c(2-x^2) = (a+2c)+(-a+b)x+(3b-c)x^2
          </me>.
          Then <m>T(p(x))=(a+2c)(1+x^2)+(4b+c)x</m>, and we find
          <me>
            C_B(T(p(x))) = P^{-1}\bbm a+2c\\4b+c\\a+2c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm
          </me>.
          On the other hand,
          <me>
            M_B(T) = \bbm 3/7\amp -24/7\amp 0\\3/7\amp 4/7\amp 1\\2/7\amp 12/7\amp 1\ebm\bbm a\\b\\c\ebm = \bbm \frac37 a-\frac{24}{7}b\\\frac37 a+\frac47 b+c\\\frac27 a+\frac{12}{7}b+c\ebm
          </me>.
          The results agree, but possibly leave us a little confused.
        </p>
      </solution>
    </example>

    <sage>
      <input>
        from sympy import *
        init_printing()
        P = Matrix(3,3,[1,0,2,-1,1,0,0,3,-1])
        M = Matrix(3,3,[1,0,2,0,4,1,1,0,2])
        P**-1, P**-1*M
      </input>
    </sage>

    <p>
      In general, given an ordered basis <m>B=\basis{b}{n}</m> for a vector space <m>V</m>
      with standard basis <m>B_0 = \basis{e}{n}</m>, if we let
      <me>
        P = \bbm C_{B_0}(\mathbf{b}_1) \amp \cdots \amp C_{B_0}(\mathbf{b}_n)\ebm
      </me>,
      then
      <md>
        <mrow>M_B(T) \amp = \bbm C_B(T(\mathbf{b}_1)) \amp \cdots \amp C_B(T(\mathbf{b_n})\ebm</mrow>
        <mrow> \amp = P^{-1}\bbm C_{B_0}(T(\mathbf{b}_1)) \amp \cdots \amp C_{B_0}(T(\mathbf{b_n})\ebm</mrow>
      </md>.
    </p>

    <p>
      As we saw above, this gives us the result, but doesn't shed much light on the problem,
      unless we have an easy way to write vectors in terms of the basis <m>B</m>.
      Let's revisit the problem. Instead of using the given basis <m>B</m>,
      let's use the standard basis <m>B_0 = \{1,x,x^2\}</m>.
      We quickly find
      <me>
        T(1)=1+x+x^2, T(x) = x, \text{ and } T(x^2)=x
      </me>,
      so with respect to the standard basis, <m>M_{B_0}(T) = \bbm 1\amp 0\amp 0\\1\amp 1\amp 1\\1\amp 0\amp 0\ebm</m>.
      Now, recall that
      <me>
        M_{B}(T)=P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm
      </me>
      and note that for any polynomial <m>p(x)</m>, <m>C_{B_0}T(p(x)) = M_{B_0}C_{B_0}(p(x))</m>. But
      <me>
        \bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm = P
      </me>,
      so we get
      <md>
        <mrow>M_B(T) \amp = P^{-1}\bbm C_{B_0}T(1-x)\amp C_{B_0}(T(x+3x^2)\amp C_{B_0}(T(2-x^2))\ebm</mrow>
        <mrow> \amp = P^{-1}\bbm M_{B_0}C_{B_0}(1-x)\amp M_{B_0}C_{B_0}(x+3x^2)\amp M_{B_0}C_{B_0}(2-x^2)\ebm </mrow>
        <mrow> \amp = P^{-1}M_{B_0}\bbm C_{B_0}(1-x) \amp C_{B_0}(x+3x^2)\amp C_{B_0}(2-x^2)\ebm</mrow>
        <mrow> \amp = P^{-1}M_{B_0}P</mrow>
      </md>.
    </p>

    <p>
      Now we have a much more efficient method for arriving at the matrix <m>M_B(T)</m>.
      The matrix <m>M_{B_0}(T)</m> is easy to determine, the matrix <m>P</m> is easy to determine,
      and with the help of the computer, it's easy to compute <m>P^{-1}M_{B_0}P = M_B(T)</m>.
    </p>

    <sage>
      <input>
        M0 = Matrix(3,3,[1,0,0,1,1,1,1,0,0])
        P**-1*M0*P
      </input>
    </sage>

    <exercise>
      <statement>
        <p>
          Determine the matrix of the operator <m>T:\R^3\to \R^3</m> given by
          <me>
            T(x,y,z) = (3x-2y+4z,x-5y,2y-7z)
          </me>
          with respect to the ordered basis
          <me>
            B = \{(1,2,0),(0,-1,2),(1,2,1)\}
          </me>.
          (You may want to use the Sage cell below for computational assistance.)
        </p>
      </statement>
      <solution>
        <p>
          With respect to the standard basis, we have
          <me>
            M_0=M_{B_0}(T) = \bbm 3\amp -2\amp 4\\1\amp -5\amp 0\\0\amp 2\amp -7\ebm
          </me>,
          and the matrix <m>P</m> is given by <m>P = \bbm 1\amp 3\amp 1\\2\amp -1\amp 2\\0\amp 2\amp-5\ebm</m>.
          Thus, we find
          <me>
            M_B(T)=P^{-1}M_0P=\bbm 9\amp 56\amp 36\\7\amp 15\amp 15\\-10\amp -46\amp -33\ebm
          </me>.
        </p>
      </solution>
    </exercise>

    <sage>
      <input>

      </input>
    </sage>

    <p>
      The matrix <m>P</m> used in the above examples is known as a <em>change</em> matrix.
      If the columns of <m>P</m> are the coefficient vectors of <m>B=\basis{b}{n}</m>
      with respect to another basis <m>D</m>, then we have
      <md>
        <mrow>P \amp= \bbm C_D(\mathbf{b}_1)\amp\cdots \amp C_D(\mathbf{b}_n)\ebm</mrow>
        <mrow>\amp = \bbm C_D(1_V(\mathbf{b}_1))\amp \cdots \amp C_D(1_V(\mathbf{b}_n))\ebm</mrow>
        <mrow>\amp = M_{DB}(1_V)</mrow>
      </md>.
      In other words, <m>P</m> is the matrix of the identity transformation <m>1_V:V\to V</m>,
      where we use the basis <m>B</m> for the domain, and the basis <m>D</m> for the codomain.
    </p>

    <definition xml:id="def-change-matrix">
      <statement>
        <p>
          The <term>change matrix</term> with respect to ordered bases <m>B,D</m> of <m>V</m>
          is denoted <m>P_{D\leftarrow B}</m>, and defined by
          <me>
            P_{D\leftarrow B} = M_{DB}(1_V)
          </me>.
        </p>
      </statement>
    </definition>

    <theorem xml:id="thm-change-matrix">
      <statement>
        <p>
          Let <m>B=\basis{b}{n}</m> and <m>D</m> be two ordered bases of <m>V</m>.
          Then
          <me>
            P_{D\leftarrow B} = \bbm C_D(\mathbf{b}_1)\amp \cdots \amp C_D(\mathbf{b}_n)\ebm
          </me>,
          and satisfies <m>C_D(\vv) = P_{D\leftarrow B}C_B(\vv)</m> for all <m>\vv\in V</m>.
        </p>

        <p>
          The matrix <m>P_{D\leftarrow B}</m> is invertible, and <m>(P_{D\leftarrow B})^{-1} = P_{B\leftarrow D}</m>.
          Moreover, if <m>E</m> is a third ordered basis, then
          <me>
            P_{E\leftarrow D}P_{D\leftarrow B} = P_{E\leftarrow B}
          </me>.
        </p>
      </statement>
    </theorem>

    <example>
      <statement>
        <p>
          Let <m>B = \{1,x,x^2\}</m> and let <m>D = \{1+x,x+x^2,2-3x+x^2\}</m> be orderd bases of <m>P_2(\R)</m>.
          Find the change matrix <m>P_{D\leftarrow B}</m>.
        </p>
      </statement>
      <solution>
        <p>
          Finding this matrix requires us to first write the vectors in <m>B</m> in terms of the vectors in <m>D</m>.
          However, it's much easier to do this the other way around. We easily find
          <me>
            P_{B\leftarrow D} = \bbm 1\amp 0\amp 2\\1\amp 1\amp -3\\0\amp 1\amp 1\ebm
          </me>,
          and by <xref ref="thm-change-matrix"/>, we have
          <me>
            P_{D\leftarrow B} = (P_{B\leftarrow D})^{-1} = \frac16\bbm 4\amp 2\amp -2\\-1\amp 1\amp 5\\1\amp -1\amp 1\ebm
          </me>.
        </p>
      </solution>
    </example>

    <p>
      The above results give a straightforward procedure for determining the matrix of any operator,
      with respect to any basis, if we let <m>D</m> be the standard basis.
      The importance of these results is not just their computational simplicity, however.
      The most important outcome of the above is that if <m>M_B(T)</m> and <m>M_D(T)</m>
      give the matrix of <m>T</m> with respect to two different bases, then
      <me>
        M_B(T) = (P_{D\leftarrow B})^{-1}M_B(T)P_{D\leftarrow B}
      </me>,
      so that the two matrices are <em>similar</em>.
    </p>

    <p>
      Recall that similar matrices have the same determinant, trace, and eigenvalues.
      This means that we can unambiguously define the determinant and trace of an  <em>operator</em>,
      and that we can compute eigenvalues of an operator using any matrix representation of that operator.
    </p>
  </section>

  <section xml:id="sec-direct-sum">
    <title>Direct Sums and Invariant Subspaces</title>
    <introduction>
      <p>
        Much of this section has been mentioned previously in the course (and these notes),
        but we will follow the organization of Nicholson's textbook,
        and reprise these concepts in more detail than previously.
      </p>
    </introduction>

    <subsection xml:id="subsec-invariant">
      <title>Invariant subspaces</title>
      <definition xml:id="def-invariant-subspace">
        <statement>
          <p>
            Given an operator <m>T:V\to V</m>, we say that a subspace <m>U\subseteq V</m>
            is <m>T</m>-<term>invariant</term> if <m>T(\uu)\in U</m> for all <m>\uu\in U</m>.
          </p>
        </statement>
      </definition>

      <p>
        In other words, a subspace <m>U</m> is <m>T</m>-invariant if <m>T</m> does not map any vectors in <m>U</m> outside of <m>U</m>.
        Notice that if we shrink the domain of <m>T</m> to <m>U</m>, then we get an operator from <m>U</m> to <m>U</m>,
        since the image <m>T(U)</m> is contained in <m>U</m>.
      </p>

      <p>
        Given a basis <m>B=\basis{u}{k}</m> of <m>U</m>,
        note that <m>U</m> is <m>T</m>-invariant if and only if <m>T(\uu_i)\in U</m> for each <m>i=1,2,\ldots, k</m>.
      </p>

      <p>
        For any operator <m>T:V\to V</m>, there are four subspaces that are always <m>T</m>-invariant:
        <me>
          \{\mathbf{0}\}, V, \ker T, \text{ and } \im T
        </me>.
        Of course, some of these subspaces might be the same; for example,
        if <m>T</m> is invertible, then <m>\ker T = \{\mathbf{0}\}</m> and <m>\im T = V</m>.
        (We will skip the proof that <m>\ker T</m> and <m>\im T</m> are <m>T</m>-invariant,
        since this has been assigned as homework!)
      </p>

      <definition xml:id="def-restriction">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
            The <term>restriction</term> of <m>T</m> to <m>U</m>, denoted <m>T|_U</m>,
            is the operator <m>T|_U:U\to U</m> defined by <m>T|_U(\uu)=T(\uu)</m> for all <m>\uu\in U</m>.
          </p>
        </statement>
      </definition>

      <p>
        Notice that the restriction <m>T|_U</m> is defined by the same <q>rule</q> as <m>T</m>,
        but its domain is the subspace <m>U</m> instead of the entire vector space <m>V</m>.
      </p>

      <p>
        A lot can be learned by studying the restrictions of an operator to invariant subspaces.
        Indeeed, the textbook by Axler does almost everything from this point of view.
        One reason to study invariant subspaces is that they allow us to put the matrix of <m>T</m> into simpler forms.
      </p>

      <theorem xml:id="thm-invariant-block-triangular">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator, and let <m>U</m> be a <m>T</m>-invariant subspace.
            Let <m>B_U = \basis{u}{k}</m> be a basis of <m>U</m>, and extend this to a basis
            <me>
              B = \{\uu_1,\ldots, \uu_k,\ww_1,\ldots, \ww_{n-k}\}
            </me>
            of <m>V</m>. Then the matrix <m>M_B(T)</m> with respect to this basis has the block-triangular form
            <me>
              M_B(T) \bbm M_{B_U}(T_U) \amp P\\0 \amp Q\ebm
            </me>
            for some <m>(n-k)\times (n-k)</m> matrix <m>Z</m>.
          </p>
        </statement>
      </theorem>

      <p>
        Reducing a matrix to block triangular form is useful,
        because it simplifies computations such as determinants and eigenvalues
        (and determinants and eigenvalues are computationally expensive).
        In particular, if a matrix <m>A</m> has the block form
        <me>
          A = \bbm A_{11} \amp A_{12} \amp \cdots A_{1n}\\
                   0\amp A_{22} \amp \cdots A_{2n}\\
                   \vdots \amp \vdots \amp \ddots \amp \vdots\\
                   0 \amp 0 \amp \cdots \amp A_{nn}\ebm
        </me>,
        where the diagonal blocks are square matrices,
        then <m>\det(A) = \det(A_{11})\det(A_{22})\cdots \det(A_{nn})</m>
        and <m>c_A(x) = c_{A_{11}}(x)c_{A_{22}}(x)\cdots C_{A_{nn}}(x)</m>.
      </p>
    </subsection>

    <subsection xml:id="subsec-eigenspace">
      <title>Eigenspaces</title>
      <p>
        An important source of invariant subspaces is eigenspaces.
        Recall that for any real number <m>\lambda</m>,
        and any operator <m>T:V\to V</m>, we define
        <me>
          E_\lambda(T) = \ker(T-\lambda 1_V) = \{\vv\in V \,|\, T(\vv) = \lambda\vv\}
        </me>.
        For most values of <m>\lambda</m>, we'll have <m>E_\lambda(T)=\{\mathbf{0}\}</m>.
        The values of <m>\lambda</m> for which <m>E_\lambda(T)</m> is non-trivial are precisely the eigenvalues of <m>T</m>.
        Note that since similar matrices have the same characteristic polynomial any matrix representation <m>M_B(T)</m>
        will have the same eigenvalues. They do <em>not</em> generally have the same eigenspaces,
        but we do have the following.
      </p>

      <theorem xml:id="thm-eigenspace-invariant">
        <statement>
          <p>
            Let <m>T:V\to V</m> be a linear operator. For any scalar <m>\lambda</m>,
            the eigenspace <m>E_\lambda(T)</m> is <m>T</m>-invariant.
            Moreover, for any ordered basis <m>B</m> of <m>V</m>,
            the coefficient isomorphism <m>C_B:V\to \R^n</m> induces an isomorphism
            <me>
              C_B|_{E_\lambda(T)}:E_\lambda(T)\to E_{\lambda}(M_B(T))
            </me>.
          </p>
        </statement>
      </theorem>
    </subsection>

    <subsection xml:id="subsec-direct-sum">
      <title>Direct Sums</title>
      <p>
        Recall that for any subspaces <m>U,W</m> of a vector space <m>V</m>,
        the sets
        <md>
          <mrow>U+W \amp =\{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}</mrow>
          <mrow>U\cap W \amp = \{\vv \in V \,|\, \vv\in U \text{ and } \vv\in W\}</mrow>
        </md>
        are subspaces of <m>V</m>. Saying that <m>\vv\in U+W</m> means that <m>\vv</m>
        can be written as a sum of a vector in <m>U</m> and a vector in <m>W</m>.
        However, this sum may not be unique. If <m>\vv\in U\cap W</m>, <m>\uu\in U</m> and <m>\ww\in W</m>,
        then we can write <m>(\uu+\vv)+\ww = \uu + (\vv+\ww)</m>,
        giving two different representations of a vector as an element of <m>U+W</m>.
      </p>

      <theorem xml:id="thm-unique-sum">
        <statement>
          <p>
            Let <m>U</m> and <m>W</m> be subspaces of a vector space <m>V</m>.
            Let <m>\vv\in U+W</m>. Then there exist <em>unique</em> vectors
            <m>\uu\in U, \ww\in W</m> such that <m>\vv = \uu+\ww</m>
            if and only if <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>
        </statement>
      </theorem>

      <definition xml:id="def-direct-sum">
        <statement>
          <p>
            We say that a sum <m>U+W</m> is a <term>direct sum</term>,
            and write this sum as <m>U\oplus W</m>, if <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>
        </statement>
      </definition>
    </subsection>

  </section>
</chapter>
