<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-linear-trans">
  <title>Linear Transformations</title>

  <introduction>
    <p>
      At an elementary level, Linear Algebra is the study of vectors (in <m>\R^n</m>)
      and matrices. Of course, much of that study revolves around systems of equations.
      Recall that if <m>\vec{x}</m> is a vector in <m>\R^n</m> (viewed as an <m>n\times 1</m> column matrix),
      and <m>A</m> is an <m>m\times n</m> matrix, then <m>\vec{y}=A\vec{x}</m> is a vector in <m>\R^m</m>.
      Thus, multiplication by <m>A</m> produces a function from <m>\R^n</m> to <m>\R^m</m>.
    </p>

    <p>
      This example motivates the definition of a <em>linear transformation</em>,
      and as we'll see, provides the archetype for all linear transformations in the finite-dimensional setting.
      Many areas of mathematics can be viewed at some fundamental level as the study of sets with certain properties,
      and the functions between them.
      Linear algebra is no different. The sets in this context are, of course, vector spaces.
      Since we care about the linear algebraic structure of vector spaces,
      it should come as no surprise that we're most interested in functions that preserve this structure.
      That is precisely the idea behind linear transformations.
    </p>
  </introduction>
  <section xml:id="sec-lin-tran-intro">
    <title>Definition and examples</title>

    <p>
      Let <m>V</m> and <m>W</m> be vector spaces. At their most basic,
      all vector spaces are sets. Given any two sets, we can consider functions from one to the other.
      The functions of interest in linear algebra are those that respect the vector space structure of the sets.
    </p>

    <definition xml:id="def-lin-trans">
      <statement>
        <p>
          Let <m>V</m> and <m>W</m> be vector spaces.
          A function <m>T:V\to W</m> is called a <term>linear transformation</term> if:
          <ol>
            <li>
              <p>
                For all <m>\vec{v}_1,\vec{v}_2\in V</m>, <m>T(\vec{v}_1+\vec{v}_2)=T(\vec{v}_1)+T(\vec{v}_2)</m>.
              </p>
            </li>
            <li>
              <p>
                For all <m>\vec{v}\in V</m> and scalars <m>c</m>, <m>T(c\vec{v})=cT(\vec{v})</m>.
              </p>
            </li>
          </ol>
          We often use the term <term>linear operator</term>
          to refer to a linear transformation <m>T:V\to V</m> from a vector space to itself.
        </p>
      </statement>
    </definition>

    <p>
      Note on notation: it is common useage to drop the usual parentheses of function notation
      when working with linear transformations, as long as this does not cause confusion.
      That is, one might write <m>T\vec{v}</m> instead of <m>T(\vec{v})</m>,
      but one should never write <m>T\vec{v}+\vec{w}</m> in place of <m>T(\vec{v}+\vec{w})</m>,
      for the same reason that one should never write <m>2x+y</m> in place of <m>2(x+y)</m>.
      Mathematicians often think of linear transformations in terms of matrix multiplication,
      which probably explains this notation to some extent.
    </p>

    <p>
      The properties of a linear transformation tell us that a linear map <m>T</m>
      <em>preserves</em> the operations of addition and scalar multiplication.
      (When the domain and codomain are different vector spaces, we might say that <m>T</m> <em>intertwines</em>
      the operations of the two vector spaces.)
      In particular, any linear transformation <m>T</m> must preserve the zero vector,
      and respect linear combinations.
    </p>

    <theorem xml:id="thm-lt-props">
      <statement>
        <p>
          Let <m>T:V\to W</m> be a linear transformation. Then
          <ol>
            <li>
              <p>
                <m>T(\vec{0}_V) = \vec{0}_W</m>, and
              </p>
            </li>
            <li>
              <p>
                For any scalars <m>c_1,\ldots, c_n</m> and vectors <m>\vec{v}_1,\ldots, \vec{v}_n\in V</m>,
                <me>
                  T(c_1\vec{v}_1+c_2\vec{v}_2+\cdots + c_n\vec{v}_n) = c_1T(\vec{v}_1)+c_2T(\vec{v}_2)+\cdots + c_nT(\vec{v}_n)
                </me>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          <ol>
            <li>
              <p>
                Since <m>\vec{0}_V+\vec{0}_V = \vec{0}_V</m>, we have
                <me>
                  T(\vec{0}_V) = T(\vec{0}_V+\vec{0}_V) = T(\vec{0}_V)+T(\vec{0}_V)
                </me>.
                Adding <m>-T(\vec{0}_V)</m> to both sides of the above gives us <m>\vec{0}_W = T(\vec{0}_V)</m>.
              </p>
            </li>
            <li>
              <p>
                The addition property of a linear transformation can be extended to sums of three or more vectors using associativity.
                Therefore, we have
                <md>
                  <mrow>T(c_1\vec{v}_1+\cdots + c_n\vec{v}_n) \amp = T(c_1\vec{v}_1)+ \cdots T(c_n\vec{v}_n)</mrow>
                  <mrow> \amp = c_1T(\vec{v}_1)+\cdots +c_nT(\vec{v}_n)</mrow>
                </md>,
                where the second line follows from the scalar multiplication property.
              </p>
            </li>
          </ol>
        </p>
      </proof>

    </theorem>


    <example xml:id="ex-matrix-trans">
      <statement>
        <p>
          Let <m>V=\R^n</m> and let <m>W=\R^m</m>.
          For any <m>m\times n</m> matrix <m>A</m>, the map <m>T_A:\R^n\to \R^m</m> defined by
          <me>
            T_A(\vec{x}) = A\vec{x}
          </me>
          is a linear transformation. (This follows immediately from properties of matrix multiplication.)
        </p>

        <p>
          Let <m>B = \{\vec{e}_1,\ldots, \vec{e}_n\}</m> denote the standard basis of <m>\R^n</m>.
          Recall that <m>A\vec{e}_i</m> is equal to the <m>i</m>th column of <m>A</m>.
          Thus, if we know the value of a linear transformation <m>T:\R^n\to \R^m</m> on each basis vector,
          we can immediately determine the matrix <m>A</m> such that <m>T=T_A</m>:
          <me>
            A = \bbm T(\vec{e}_1) \amp T(\vec{e}_2) \amp \cdots \amp T(\vec{e}_n)\ebm
          </me>.
          This is true because <m>T</m> and <m>T_A</m> agree on the standard basis: for each <m>i=1,2,\ldots, n</m>,
          <me>
            T_A(\vec{e_i}) = A\vec{e_i} = T(\vec{e}_i)
          </me>.
          Moreover, if two linear transformations agree on a basis, they must be equal.
          Given any <m>\vec{x}\in \R^n</m>, we can write <m>\vec{x}</m> uniquely as a linear combination
          <me>
            \vec{x}=c_1\vec{e}_1+c_2\vec{e}_2+\cdots + c_n\vec{e}_n.
          </me>
          If <m>T(\vec{e}_i)=T_A(\vec{e}_i)</m> for each <m>i</m>, then by <xref ref="thm-lt-props"/> we have
          <md>
            <mrow>T(\vec{x}) \amp = T(c_1\vec{e}_1+c_2\vec{e}_2+\cdots + c_n\vec{e}_n) </mrow>
            <mrow> \amp = c_1T(\vec{e}_1)+c_2T(\vec{e}_2)+\cdots + c_nT(\vec{e}_n)</mrow>
            <mrow> \amp = c_1T_A(\vec{e}_1)+c_2T_A(\vec{e}_2)+\cdots + c_nT_A(\vec{e}_n)</mrow>
            <mrow> \amp = T_A(c_1\vec{e}_1+c_2\vec{e}_2+\cdots + c_n\vec{e}_n) </mrow>
            <mrow> \amp = T_A(\vec{x})</mrow>
          </md>.
        </p>
      </statement>
    </example>

    <p>
      Let's look at some other examples of linear transformations.
      <ul>
        <li>
          <p>
            For any vector spaces <m>V,W</m> we can define the <term>zero transformation</term> <m>0:V\to W</m>
            by <m>0(\vec{v})=\vec{0}</m> for all <m>\vec{v}\in V</m>.
          </p>
        </li>
        <li>
          <p>
            On any vector space <m>V</m> we have the <term>identity transformation</term>
            <m>1_V:V\to V</m> defined by <m>1_V(\vec{v})=\vec{v}</m> for all <m>\vec{v}\in V</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>V = F[a,b]</m> be the space of all functions <m>f:[a,b]\to \R</m>.
            For any <m>c\in [a,b]</m> we have the <term>evaluation map</term>
            <m>E_a: V\to \R</m> defined by <m>E_a(f) = f(a)</m>.
          </p>

          <p>
            To see that this is linear, note that <m>E_a(0)=\mathbf{0}(a)=0</m>,
            where <m>\mathbf{0}</m> denotes the zero function;
            for any <m>f,g\in V</m>,
            <me>
              E_a(f+g)=(f+g)(a)=f(a)+g(a)=E_a(f)+E_a(g)
            </me>,
             and for any scalar <m>c\in \R</m>,
             <me>
               E_a(cf) = (cf)(a) = c(f(a))=cE_a(f)
             </me>.
          </p>

          <p>
            Note that the evaluation map can similarly be defined as a linear transformation on any vector space of polynomials.
          </p>
        </li>

        <li>
          <p>
            On the vector space <m>C[a,b]</m> of all <em>continuous</em> functions on <m>[a,b]</m>,
            we have the integration map <m>I:C[a,b]\to \R</m> defined by
            <m>I(f)=\int_a^b f(x)\,dx</m>. The fact that this is a linear map follows from properties of integrals proved in a calculus class.
          </p>
        </li>

        <li>
          <p>
            On the vector space <m>C^1(a,b)</m> of continuously differentiable functions on <m>(a,b)</m>,
            we have the differentiation map <m>D: C^1(a,b)\to C(a,b)</m> defined by
            <m>D(f) = f'</m>. Again, linearity follows from properties of the derivative.
          </p>
        </li>

        <li>
          <p>
            Let <m>\R^\infty</m> denote the set of sequences <m>(a_1,a_2,a_3,\ldots)</m> of real numbers,
            with term-by-term addition and scalar multiplication.
            The shift operators
            <md>
              <mrow>S_L(a_1,a_2,a_3,\ldots)  \amp = (a_2,a_3,a_4,\ldots) </mrow>
              <mrow>S_R(a_1,a_2,a_3,\ldots) \amp = (0,a_1,a_2,\ldots)</mrow>
            </md>
            are both linear.
          </p>
        </li>

        <li>
          <p>
            On the space <m>M_{mn}(\R)</m> of <m>m\times n</m> matrices,
            the trace defines a linear map <m>\operatorname{tr}:M_{mn}(\R)\to \R</m>,
            and the transpose defines a linear map <m>T:M_{mn}(\R)\to M_{nm}(\R)</m>.
            The determinant and inverse operations on <m>M_{nn}</m> are <em>not</em> linear.
          </p>
        </li>
      </ul>
    </p>

    <p>
      For finite-dimensional vector spaces, it is often convenient to work in terms of a basis.
      The properties of a linear transformation tell us that we can completely define any linear transformation by giving its values on a basis.
      In fact, it's enough to know the value of a transformation on a spanning set.
      The argument given in <xref ref="ex-matrix-trans"/> can be applied to any linear transformation, to obtain the following result.
    </p>

    <theorem xml:id="thm-agree-span">
      <statement>
        <p>
          Let <m>T:V\to W</m> and <m>S:V\to W</m> be two linear transformations.
          If <m>V = \spn\{\vec{v}_1,\ldots, \vec{v}_n\}</m> and <m>T(\vec{v}_i)=S(\vec{v}_i)</m>
          for each <m>i=1,2,\ldots, n</m>, then <m>T=S</m>.
        </p>
      </statement>
    </theorem>

    <p>
      If the above spanning set is not also independent,
      then one might be concerned about the fact that there will be more than one way to express a vector as linear combination of vectors in that set.
      If we define <m>T</m> by giving its values on a spanning set, will it be well-defined?
      Suppose that we have scalars <m>a_1,\ldots, a_n, b_1,\ldots, b_n</m> such that
      <md>
        <mrow>\vec{v} \amp a_1\vec{v_1}+\cdots + a_n\vec{v}_n</mrow>
        <mrow> \amp b_1\vec{v}_1+\cdots + b_n\vec{v}_n</mrow>
      </md>
      We then have
      <md>
        <mrow>a_1T(\vec{v}_1)+\cdots + a_nT(\vec{v}_n) \amp =T(a_1\vec{v}_1+\cdots + a_n\vec{v}_n) </mrow>
        <mrow> \amp =T(b_1\vec{v}_1+\cdots +b_n\vec{v}_n)</mrow>
        <mrow>  \amp =b_1T(\vec{v}_1)+\cdots +b_nT(\vec{v}_n)</mrow>
      </md>.
      The next theorem seems like an obvious consequence of the above,
      and indeed, one might wonder where the assumption of a basis is needed.
      The distinction here is that the vectors <m>\vec{w}_1,\ldots, \vec{w}_n\in W</m>
      are chosen in advance, and then we set <m>T(vec{b}_i)=\vec{w}_i</m>,
      rather than simply defining each <m>\vec{w}_i</m> as <m>T(\vec{b}_i)</m>.
    </p>

    <theorem xml:id="thm-define-using-basis">
      <statement>
        <p>
          Let <m>V,W</m> be vector spaces. Let <m>B=\{\vec{b}_1,\ldots, \vec{b}_n\}</m>
          be a basis of <m>V</m>, and let <m>\vec{w}_1,\ldots, \vec{w}_n</m> be any vectors in <m>W</m>.
          (These vectors need not be distinct.)
          Then there exists a unique linear transformation <m>T:V\to W</m> such that
          <m>T(\vec{b}_i)=\vec{w}_i</m> for each <m>i=1,2,\ldots, n</m>; indeed,
          we can define <m>T</m> as follows:
          given <m>\vec{v}\in V</m>, write <m>\vec{v}=c_1\vec{v}_1+\cdots +c_n\vec{v}_n</m>. Then
          <me>
            T(\vec{v})=T(c_1\vec{v}_1+\cdots + c_n\vec{v}_n) = c_1\vec{w}_1+\cdots +c_n\vec{v}_n
          </me>.
        </p>
      </statement>
    </theorem>

    <p>
      With the basic theory out of the way, let's look at a few basic examples.
    </p>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:\R^2\to \R^2</m> is a linear transformation.
          If <m>T\bbm 1\\0\ebm = \bbm 3\\-4\ebm</m> and <m>T\bbm 0\\1\ebm =\bbm 5\\2\ebm</m>,
          find <m>T=\bbm -2\\4\ebm</m>.
        </p>
      </statement>
      <solution>
        <p>
          Since we know the value of <m>T</m> on the standard basis,
          we can use properties of linear transformations to immediately obtain the answer:
          <md>
            <mrow>T\bbm -2\\4\ebm \amp= T\left(-2\bbm 1\\0\ebm +4\bbm 0\\1\ebm\right)</mrow>
            <mrow> \amp = -2T\bbm1\\0\ebm+4T\bbm 0\\1\ebm</mrow>
            <mrow> \amp = -2\bbm 3\\-4\ebm +4\bbm 5\\2\ebm</mrow>
            <mrow> \amp = \bbm 14\\16\ebm</mrow>
          </md>.
        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:\R^2\to \R^2</m> is a linear tranasformation.
          Given that <m>T\bbm 3\\1\ebm = \bbm 1\\4\ebm</m>
          and <m>T\bbm 2\\-5\ebm = \bbm 2\\-1\ebm</m>,
          find <m>T\bbm 4\\3\ebm</m>.
        </p>
      </statement>
      <solution>
        <p>
          At first, this example looks the same as the one above,
          and to some extent, it is. The difference is that this time,
          we're given the values of <m>T</m> on a basis that is not the standard one.
          This means we first have to do some work to determine how to write the given vector in terms of the given basis.
        </p>

        <p>
          Suppose we have <m>a\bbm 3\\1\ebm+b\bbm 2\\-5\ebm = \bbm 4\\3\ebm</m>
          for scalars <m>a,b</m>. This is equivalent to the matrix equation
          <me>
            \bbm 3\amp 2\\1\amp -5\ebm\bbm a\\b\ebm = \bbm 4\\3\ebm.
          </me>
          Solving (perhaps using the code cell below), we get <m>a=\frac{26}{17}, b = -\frac{5}{17}</m>.
          Therefore,
          <me>
            T\bbm 3\\4\ebm = \frac{26}{17}\bbm 1\\4\ebm -\frac{5}{17}\bbm 2\\-1\ebm = \bbm 16/17\\109/17\ebm
          </me>.
        </p>
      </solution>
    </exercise>

    <sage>
      <input>
        from sympy import *
        init_printing()
        A = Matrix(2,2,[3,2,1,-5])
        B = Matrix(2,1,[4,3])
        (A**-1)*B
      </input>
    </sage>

    <exercise>
      <statement>
        <p>
          Suppose <m>T:P_2(\R)\to \R</m> is defined by
          <me>
            T(x+2)=1, T(1)=5, T(x^2+x)=0.
          </me>
          Find <m>T(2-x+3x^2)</m>.
        </p>
      </statement>
      <solution>
        <p>
          We need to find scalars <m>a,b,c</m> such that
          <me>
            2-x+3x^2 = a(x+2)+b(1)+c(x^2+x)
          </me>.
          We could set up a system and solve, but this time it's easy enough to just work our way through.
          We must have <m>c=3</m>, to get the correct coefficient for <m>x^2</m>. This gives
          <me>
            2-x+3x^2=a(x+2)+b(1)+3x^2+3x
          </me>.
          Now, we have to have <m>3x+ax=-x</m>, so <m>a=-4</m>.
          Putting this in, we get
          <me>
            2-x+3x^2=-4x-8+b+3x^2+3x
          </me>.
          Simiplifying this leaves us with <m>b=10</m>. Finally, we find:
          <md>
            <mrow>T(2-x+3x^2) \amp = T(-4(x+2)+10(1)+3(x^2+x)) </mrow>
            <mrow> \amp = -4T(x+2)+10T(1)+3T(x^2+x)</mrow>
            <mrow> \amp = -4(1)+10(5)+3(0) = 46</mrow>
          </md>.
        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Find a linear transformation <m>T:\R^2\to \R^3</m> such that
          <me>
            T(1,2)=(1,1,0) \quad \text{ and } \quad T(-1,1) = (0,2,-1)
          </me>.
          Then, determine the value of <m>T(3,2)</m>.
        </p>
      </statement>
      <solution>
        <p>
          Since <m>\{(1,2),(-1,1)\}</m> forms a basis of <m>\R^2</m>
          (the vectors are not parallel and there are two of them),
          it suffices to determine how to write a general vector in terms of this basis.
          suppose
          <me>
            x(1,2)+y(-1,1)=(a,b)
          </me>
          for a general element <m>(a,b)\in \R^2</m>.
          This is equivalent to the matrix equation <m>\bbm 1\amp -1\\2\amp 1\ebm\bbm x\\y\ebm = \bbm a\\b\ebm</m>.
          We find:
          <me>
            (a,b) = \frac13(a+b)(1,2)+\frac13(-2a+b)(-1,1).
          </me>
          Thus,
          <md>
            <mrow>T(a,b) \amp = \frac13(a+b)T(1,2)+\frac13(-2a+b)T(-1,1) </mrow>
            <mrow> \amp = \frac13(a+b)(1,1,0)+\frac13(-2a+b)(0,2,-1)</mrow>
            <mrow> \amp = \left(\frac{a+b}{3}, -a+b, \frac{2a-b}{3}\right)</mrow>
          </md>.
          Therefore,
          <me>
            T(3,2) = \left(\frac53, -1, \frac43\right)
          </me>.
        </p>
      </solution>
    </exercise>

    <sage>
      <input>
        a, b = symbols('a b', real = True, constant = True)
        A = Matrix(2,2,[1,-1,2,1])
        B = Matrix(2,1,[a,b])
        (A**-1)*B
      </input>
    </sage>

    <exercise xml:id="ex_lintrans-indep">
      <statement>
        <p>
          Let <m>T:V\to W</m> be a linear transformation.
          Prove that for any vectors <m>\vec{v}_1,\ldots, \vec{v}_n\in V</m>,
          if <m>\{T(\vec{v}_1),\ldots, T(\vec_{v}_n)\}</m> is linearly independent in <m>W</m>,
          then <m>\{\vec{v}_1,\ldots, \vec{v}_n\}</m> is linearly independent in <m>V</m>.
        </p>
      </statement>
      <solution>
        <p>
          Let us suppose that <m>\{T(\vec{v}_1),\ldots, T(\vec_{v}_n)\}</m> is linearly independent.
          We want to show that the set <m>\{\vec{v}_1,\ldots, \vec{v}_n\}</m>
          is linearly independent. To that end, suppose that we have
          <me>
            c_1\vec{v}_1+\cdots + c_n\vec{v}_n=\vec{0}
          </me>
          for some scalars <m>c_1,\ldots, c_n</m> (that we want to show must all equal zero).
        </p>

        <p>
          We want to make use of our hypothesis, so we need to bring the linear map <m>T</m> into the picture.
          We apply <m>T</m> to both sides of the equation above, giving us:
          <me>
            T(c_1\vec{v}_1+\cdots + c_n\vec{v}_n)=T(\vec{0})
          </me>.
          Now we make use of both parts of <xref ref="thm-lt-props">Theorem</xref> to get
          <me>
            c_1T(\vec{v}_1)+\cdots +c_nT(\vec{v}_n) = \vec{0}
          </me>.
          By our hypothesis that the <m>T(\vec{v}_i)</m> are independent,
          we must conclude that all the scalars are zero, and we're done.
        </p>
      </solution>
    </exercise>

  </section>


  <section xml:id="sec-kernel-image">
    <title>Kernel and Image</title>

    <p>
      Given any linear transformation <m>T:V\to W</m> we can associate two important subspaces:
      the <term>kernel</term> of <m>T</m> (also known as the <term>nullspace</term>),
      and the <term>image</term> of <m>T</m> (also known as the <term>range</term>).
    </p>

    <definition xml:id="def-kernel-image">
      <statement>
        <p>
          Let <m>T:V\to W</m> be a linear transformation. The <term>kernel</term> of <m>T</m>,
          denoted <m>\ker T</m>, is defined by
          <me>
            \ker T = \{\vec{v}\in V \,|\, T(\vec{v})=\vec{0}\}
          </me>.
          The <term>image</term> of <m>T</m>, denoted <m>\Img T</m>, is defined by
          <me>
            \Img T = \{T(\vec{v}) \,|\, \vec{v}\in V\}
          </me>.
        </p>
      </statement>
    </definition>

    <p>
      Note that the kernel of <m>T</m> is just the set of all vectors <m>T</m> sends to zero.
      The image of <m>T</m> is the range of <m>T</m> in the usual sense of the range of a function.
    </p>

    <theorem xml:id="thm-ker-img-subspace">
      <statement>
        <p>
          For any linear transformation <m>T:V\to W</m>,
          <ol>
            <li>
              <p>
                <m>\ker T</m> is a subspace of <m>V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\Img T</m> is a subspace of <m>W</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>

      <proof>
        <p>
          <ol>
            <li>
              <p>
                To show that <m>\ker T</m> is a subspace, first note that <m>\vec{0}\in \ker T</m>,
                since <m>T(\vec{0})=\vec{0}</m> for any linear transformation <m>T</m>.
                If <m>\vec{v},\vec{w}\in \ker T</m>, then <m>T(\vec{v})=\vec{0}</m>
                and <m>T(\vec{w})=0</m>, and therefore,
                <me>
                  T(\vec{v}+\vec{w})=T(\vec{v})+T(\vec{w})=\vec{0}+\vec{0}=\vec{0}
                </me>.
                Similarly, for any scalar <m>c</m> and <m>\vec{v}\in \ker T</m>,
                <me>
                  T(c\vec{v})=cT(\vec{v})=c\vec{0}=\vec{0}
                </me>.
                By the subspace test, <m>\ker T</m> is a subspace.
              </p>
            </li>

            <li>
              <p>
                Again, since <m>T(\vec{0})=\vec{0}</m>, we see that <m>\vec{0}\in \Img T</m>,
                so <m>\Img T</m> is nonempty.
                If <m>\vec{w}_1,\vec{w}_2\in \Img T</m>, then there exist <m>\vec{v}_1,\vec{v}_2\in V</m>
                such that <m>T(\vec{v}_1)=\vec{w}_1</m> and <m>T(\vec{v}_2)=\vec{w}_2</m>.
                It follows that
                <me>
                  \vec{w}_1+\vec{w}_2 = T(\vec{v}_1)+T(\vec{v}_2) = T(\vec{v}_1+\vec{v}_2)
                </me>,
                so <m>\vec{w}_1+\vec{w}_2\in \Img T</m>.
                Similarly, if <m>c</m> is any scalar and <m>\vec{w}=T(\vec{v})\in\Img T</m>,
                then
                <me>
                  c\vec{w}=cT(\vec{v})=T(c\vec{v})
                </me>,
                so <m>c\vec{w}\in \Img T</m>.
              </p>
            </li>
          </ol>
        </p>
      </proof>

    </theorem>

    <p>
      A familiar setting that you may already have encontered in a previous linear algebra course is that of a matrix transformation.
      Let <m>A</m> be an <m>m\times n</m> matrix. Then we can define <m>T:\R^n\to \R^m</m>
      by <m>T(\vec{x})=A\vec{x}</m>, where elements of <m>\R^n,\R^m</m> are considered as column vectors.
      We then have
      <me>
        \ker T = \nll(A) = \{\vec{x}\in \R^n \,|\, A\vec{x}=\vec{0}\}
      </me>
      and
      <me>
        \Img T = \csp(A) = \{A\vec{x}\,|\, \vec{x}\in \R^n\}
      </me>,
      where <m>\csp(A)</m> denotes the <term>column space</term> of <m>A</m>.
      Recall further that if we write <m>A</m> in terms of its columns as
      <me>
        A = \bbm C_1 \amp C_2 \amp \cdots \amp C_n\ebm
      </me>
      and a vector <m>\vec{x}\in \R^n</m> as <m>\vec{x}=\bbm x_1\\x_2\\\vdots \\x_n\ebm</m>,
      then
      <me>
        A\vec{x} = x_1C_1+x_2C_2+\cdots +x_nC_n
      </me>.
      Thus, any element of <m>\csp(A)</m> is a linear combination of its columns,
      explaining the name <em>column space</em>.
    </p>

    <p>
      Determining <m>\nll(A)</m> and <m>\csp(A)</m> for a given matrix <m>A</m> is,
      unsurprisingly, a matter of reducing <m>A</m> to row-echelon form.
      Finding <m>\nll(A)</m> is simply a matter of describing the set of all solutions to the homogeneous system <m>A\vec{x}=\vec{0}</m>.
      Finding <m>\csp(A)</m> relies on the following theorem.
    </p>

    <theorem xml:id="thm-colspace">
      <statement>
        <p>
          Let <m>A</m> be an <m>m\times n</m> matrix with columns <m>C_1,C_2,\ldots, C_n</m>.
          If the reduced row-echelon form of <m>A</m> has leading ones in columns <m>j_1,j_2,\ldots, j_k</m>,
          then <m>\{C_{j_1},C_{j_2},\ldots, C_{j_k}\}</m> is a basis for <m>\csp(A)</m>.
        </p>
      </statement>
    </theorem>

    <p>
      The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson.
      To see why it works, we need to remember a few basic facts from elementary linear algebra.
      First, recall that performing an elementary row operation on a matrix <m>A</m>
      is equivalent to multiplying on the left by an elementary matrix <m>E</m>
      defined using the same row operation.
    </p>

    <p>
      Since every elementary matrix is invertible, and any product of invertible matrices is invertible,
      and we can transform <m>A</m> into a row-echelon matrix <m>R</m> using elementary row operations,
      it follows that <m>R = UA</m> for an invertible matrix <m>U</m>; indeed,
      we have <m>U = E_kE_{k-1}\cdots E_2E_1</m>, where <m>E_1,\ldots, E_k</m>
      are the elemetnary matrices corresponding to the row operations used to carry <m>A</m> to <m>R</m>.
    </p>

    <p>
      A basis for <m>\csp(R)</m> is given by the columns of <m>R</m> containing the leading ones.
      The reason for this is as follows. First, recall that each nonzero row begins with a leading one.
      So if the leading ones of <m>R</m> are in columns <m>i_1,\ldots, i_k</m>,
      then there are <m>k</m> nonzero rows.
      Since all rows of zeros go at the bottom, each column in <m>R</m> has its last <m>m-k</m> entries identically zero.
      Thus,
      <me>
        \csp(R)\subseteq \left\{\bbm a_1\\\vdots \\a_k\\0\\\vdots 0\ebm\in \R^m \,|\, a_1,\ldots, a_k\in\R\right\}
      </me>,
      so <m>\dim \csp(R)\leq k</m>. But the columns containing leading ones are easily shown to be independent,
      so they form a basis of <m>\csp(R)</m>, which therefore has dimension <m>k=\operatorname{rank}(A)</m>.
    </p>

    <p>
      Next, since <m>R=UA</m>, where <m>U</m> is invertible, if <m>R=\bbm Y_1\amp Y_2\amp \cdots \amp Y_n\ebm</m>
      and <m>A = \bbm C_1\amp C_2\amp \cdots \amp C_n\ebm</m>, then <m>C_i = U^{-1}Y_i</m> for each <m>i</m>.
      It follows from the fact that <m>U</m> is invertible and that the columns containing leading ones in <m>R</m>
      form a basis for <m>\csp(R)</m> that the corresponding columns in <m>A</m> form a basis for <m>\csp(A)</m>.
      (For details, see Section 5.4 in Nicholson.)
    </p>

    <p>
      For example, consider the linear transformation <m>T:\R^4\to \R^3</m> defined by the matrix
      <me>
        A = \bbm 1 \amp 3 \amp 0 \amp -2\\
                -2 \amp -1 \amp 2 \amp 0\\
                 1 \amp 8 \amp 2 \amp -6\ebm
      </me>.
      Let's determine the RREF of <m>A</m>:
    </p>

    <sage>
      <input>
        from sympy import *
        init_printing()
        A=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])
        A.rref()
      </input>
      <output>
        Matrix(3,4,[1,0,-6/5,2/5,0,1,2/5,-4/5,0,0,0,0])
      </output>
    </sage>

    <p>
      We see that there are leading ones in the first and second column.
      Therefore, <m>\csp(A) = \Img(T) = \spn\left\{\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right\}</m>.
      Indeed, note that
      <me>
        \bbm 0\\2\\2\ebm = -\frac65\bbm 1\\-2\\1\ebm + \frac25\bbm 3\\-1\\8\ebm
      </me>
      and
      <me>
        \bbm -2\\0\\-6\ebm = \frac25\bbm 1\\-2\\1\ebm -\frac45\bbm 3\\-1\\8\ebm
      </me>,
      so that indeed, the third and fourth columns are in the span of the first and second.
    </p>

    <p>
      Furthermore, we can determine the nullspace: if <m>A\vec{x}=\vec{0}</m> where
      <m>\vec{x}=\bbm x_1\\x_2\\x_3\\x_4\ebm</m>, then we must have
      <md>
        <mrow>x_1 \amp =\frac65 x_3-\frac25 x_4</mrow>
        <mrow>x_2 \amp =-\frac25 x_3+\frac 45 x_4</mrow>
      </md>,
      so
      <me>
        \vec{x} = \bbm \frac65x_3-\frac25x_4\\ -\frac25x_3+\frac45x_4\\x_3\\x_4\ebm = \frac{x_3}{5}\bbm 6\\-2\\5\\0\ebm + \frac{x_4}{5}\bbm -2\\4\\0\\5\ebm
      </me>.
      It follows that a basis for <m>\nll(A)=\ker T</m> is <m>\left\{\bbm 6\\-2\\5\\0\ebm, \bbm -2\\4\\0\\5\ebm\right\}</m>.
    </p>

    <p>
      Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space.
      But it's probably worth your while to know how to determine these from the RREF of a matrix,
      without additional help from the computer.
      That said, let's see how the computer's output compares to what we found:
    </p>

    <sage>
      <input>
        A.nullspace()
      </input>
    </sage>

    <sage>
      <input>
        A.columnspace()
      </input>
    </sage>

    <p>
      Note that the output from the computer simply states the basis for each space.
      Of course, for computational purposes, this is typically good enough.
    </p>

    <p>
      An important result that comes out while trying to show that the <q>pivot columns</q>
      of a matrix (the ones that end up with leading ones in the RREF) are a basis for the column space
      is that the column rank (defined as the dimesion of <m>\csp(A)</m>) and the row rank
      (the dimension of the space spanned by the columns of <m>A</m>) are equal.
      One can therefore speak unabmiguously about the <term>rank</term> of a matrix <m>A</m>,
      and it is just as it's defined in a first course in linear algebra: the number of leading ones in the RREF of <m>A</m>.
    </p>

    <p>
      For a general linear transformation, we can't necessarily speak in terms of rows and columns,
      but if <m>T:V\to W</m> is linear, and either <m>V</m> or <m>W</m> is finite-dimensional,
      then we can define the rank of <m>T</m> as follows.
    </p>

    <definition xml:id="def-rank-transformation">
      <statement>
        <p>
          Let <m>T:V\to W</m> be a linear transformation.
          Then the <term>rank</term> of <m>T</m> is defined by
          <me>
            \operatorname{rank} T = \dim \Img T
          </me>,
          and the <term>nullity</term> of <m>T</m> is defined by
          <me>
            \operatorname{nullity} T = \dim \ker T
          </me>.

        </p>
      </statement>
    </definition>

    <p>
      Note that if <m>W</m> is finite-dimensional, then so is <m>\Img T</m>,
      since it's a subspace of <m>W</m>.
      On the other hand, if <m>V</m> is finite-dimensional,
      then we can find a basis <m>\{\vec{v}_1,\ldots, \vec{v}_n\}</m> of <m>V</m>,
      and the set <m>\{T(\vec{v}_1),\ldots, T(\vec{v}_n)\}</m> will span <m>\Img T</m>,
      so again the image is finite-dimensional, so the rank of <m>T</m> is finite.
      It is possible for either the rank or the nullity of a transformation to be infinite.
    </p>

    <p>
      Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces.
      From the textbook, we have the following nice example.
    </p>

    <example>
      <statement>
        <p>
          Let <m>T:M_{nn}\to M_{nn}</m> be defined by <m>T(A)=A-A^T</m>. Then
          <ol>
            <li>
              <p>
                <m>T</m> is a linear map.
              </p>
            </li>
            <li>
              <p>
                <m>\ker T</m> is equal to the set of all symmetric matrices.
              </p>
            </li>

            <li>
              <p>
                <m>\Img T</m> is equal to the set of all skew-symmetric matrices.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <solution>
        <p>
          <ol>
            <li>
              <p>
                We have <m>T(0)=0</m> since <m>0^T=0</m>.
                Using proerties of the transpose and matrix algebra, we have
                <me>
                  T(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)
                </me>
                and
                <me>
                  T(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)
                </me>.
              </p>
            </li>

            <li>
              <p>
                It's clear that if <m>A^T=A</m>, then <m>T(A)=0</m>.
                On the other hand, if <m>T(A)=0</m>, then <m>A-A^T=0</m>, so <m>A=A^T</m>.
                Thus, the kernel consists of all symmetric matrices.
              </p>
            </li>

            <li>
              <p>
                If <m>B=T(A)=A-A^T</m>, then
                <me>
                  B^T = (A-A^T)^T = A^T-A = -B
                </me>,
                so certainly every matrix in <m>\Img A</m> is skew-symmetric.
                On the other hand, if <m>B</m> is skew-symmetric, then <m>B=T(\frac12 B)</m>,
                since
                <me>
                  T\Bigl(\frac12 B\Bigr) = \frac12 T(B) = \frac12(B-B^T) = \frac12(B-(-B))= B
                </me>.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </example>

    <p>
      You'll recall from a course like Math 2000 that in the study of functions,
      the properties of being injective (one-to-one) and surjective (onto)
      are important. They're important for linear transformations as well,
      and defined in exactly the same way.
    </p>

    <p>
      It's clear that being surjective is closely tied to image.
      Indeed, by definition, <m>T:V\to W</m> is onto if <m>\Img T = W</m>.
      What might not be immediately obvious is that the kernel tells us if a linear map is injective.
    </p>

    <theorem xml:id="thm-injective-kernel">
      <statement>
        <p>
          Let <m>T:V\to W</m> be a linear transformation.
          Then <m>T</m> is injective if and only if <m>\ker T = \{\vec{0}\}</m>.
        </p>
      </statement>

      <proof>
        <p>
          Suppose <m>T</m> is injective, and let <m>\vec{v}\in \ker T</m>.
          Then <m>T(\vec{v})=\vec{0}</m>. On the other hand, we know that <m>T(\vec{0})=\vec{0}</m>,
          and since <m>T</m> is injective, we must have <m>\vec{v}=\vec{0}</m>.

          Conversely, suppose that <m>\ker T = \{0\}</m> and that <m>T(\vec{v}_1)=T(\vec{v}_2)</m>
          for some <m>\vec{v}_1,\vec{v}_2\in V</m>. Then
          <me>
            \vec{0} = T(\vec{v}_1)-T(\vec{v}_2) = T(\vec{v}_1-\vec{v}_2)
          </me>,
          so <m>\vec{v}_1-\vec{v}_2\in \ker T</m>.
          Therefore, we must have <m>\vec{v}_1-\vec{v}_2=\vec{0}</m>,
          so <m>\vec{v}_1=\vec{v}_2</m>, and it follows that <m>T</m> is injective.
        </p>
      </proof>

    </theorem>

    <p>
      Let us return to the case of a matrix transformation <m>T_A:\R^n\to \R^m</m>.
      Notice that <m>\ker T_A</m> is simply the set of all solutions to <m>A\vec{x}=\vec{0}</m>,
      while <m>\Img T_A</m> is the set of all <m>\vec{y}\in\R^m</m> for which <m>A\vec{x}=\vec{y}</m>
      <em>has</em> a solution.
    </p>

    <p>
      Recall from the discussion above that <m>\rank A = \dim \csp(A) = \dim \Img T_A</m>.
      It follows that <m>T_A</m> is surjective if and only if <m>\rank A = m</m>.
      On the other hand, <m>T_A</m> is injective if and only if <m>\rank A = n</m>,
      because we know that the system <m>A\vec{x}=\vec{0}</m> has a unique solution if and only if each column of <m>A</m> contains a leading one.
    </p>

    <p>
      This has some interesting consequences. If <m>m=n</m> (that is, if <m>A</m> is square),
      then each increase in <m>\dim \nll(A)</m> produces a corresponding decrease in <m>\dim \csp(A)</m>,
      since both correspond to the <q>loss</q> of a leading one. Moreover, if <m>\rank A = n</m>,
      then <m>T_A</m> is both injective and surjective.
      Recall that a function is invertible if and only if it is both injective and surjective.
      It should come as no surprise that invertibility of <m>T_A</m> (as a function)
      is equivalent to invertibility of <m>A</m> (as a matrix).
    </p>

    <p>
      Also, note that if <m>m \lt n</m>, then <m>\rank A\leq m \lt n</m>,
      so <m>T_A</m> could be surjective, but can't possibly be injective.
      On the other hand, if <m>m\gt n</m>, then <m>\rank A\leq n \lt m</m>,
      so <m>T_A</m> could be injective, but can't possibly be surjective.
      These results generalize to linear transformations between any finite-dimensional vector spaces.
      The first step towards this is the following theorem,
      which is sometimes known as the Fundamental Theorem of Linear Transformations.
    </p>

    <theorem xml:id="thm-dimension-lintrans">
      <title>Dimension Theorem</title>
      <statement>
        <p>
          Let <m>T:V\to W</m> be any linear transformation such that
          <m>\ker T</m> and <m>\Img T</m> are finite-dimensional.
          Then <m>V</m> is finite-dimensional, and
          <me>
            \dim V = \dim \ker T + \dim \Img T
          </me>.
        </p>
      </statement>
      <proof>
        <p>
          The trick with this proof is that we aren't assuming <m>V</m> is finite-dimensional,
          so we can't start with a basis of <m>V</m>. But we do know that <m>\Img T</m>
          is finite-dimensional, so we start with a basis <m>\{\vec{w}_1,\ldots, \vec{w}_m\}</m>
          of <m>\Img T</m>.
          Of course, every vector in <m>\Img T</m> is the image of some vector in <m>V</m>,
          so we can write <m>\vec{w}_i =T(\vec{v}_i)</m>, where <m>\vec{v}_i\in V</m>,
          for <m>i=1,2,\ldots, m</m>.
        </p>

        <p>
          Since <m>\{T(\vec{v}_1),\ldots, T(\vec{v}_m)\}</m> is a basis,
          it is linearly independent. The results of <xref ref="ex_lintrans-indep">Exercise</xref>
          tell us that the set <m>\{\vec{v}_1,\ldots, \vec{v}_m\}</m> must therefore be independent.
        </p>

        <p>
          We now introduce a basis <m>\{\vec{u}_1,\ldots, \vec{u}_n\}</m>
          of <m>\ker T</m>, which we also know to be finite-dimensional.
          If we can show that the set <m>\{\vec{u}_1,\ldots, \vec{u}_n,\vec{v}_1,\ldots, \vec{v}_m\}</m>
          is a basis for <m>V</m>, we'd be done, since the number of vectors in this basis
          is <m>\dim\ker T + \dim \Img T</m>. We must therefore show that this set is independent,
          and that it spans <m>V</m>.
        </p>

        <p>
          To see that it's independent, suppose that
          <me>
            a_1\vec{u}_1+\cdots + a_n\vec{u}_n+b_1\vec{v}_1+\cdots +\b_m\vec{v}_m=\vec{0}
          </me>.
          Applying <m>T</m> to this equation, and noting that <m>T(\vec{u}_i)=\vec{0}</m>
          for each <m>i</m>, by definition of the <m>\vec{u}_i</m>, we get
          <me>
            b_1T(\vec{v}_1)+\cdots +b_mT(\vec{v}_m)=\vec{0}
          </me>.
          We assumed that the vectors <m>T(\vec{v}_i)</m> were independent,
          so all the <m>b_i</m> must be zero. But then we get
          <me>
            a_1\vec{u}_1+\cdots +a_n\vec{u}_n=\vec{0}
          </me>,
          and since the <m>\vec{u}_i</m> are independent, all the <m>a_i</m> must be zero.
        </p>

        <p>
          To see that these vectors span, choose any <m>\vec{x}\in V</m>.
          Since <m>T(\vec{x})\in \Img T</m>, there exist scalars <m>c_1,\ldots, c_m</m>
          such that
          <men xml:id="eqn-almost-span">
            T(\vec{x})=c_1T(\vec{v_1})+\cdots +c_mT(\vec{v}_m)
          </men>.
          We'd like to be able to conclude from this that <m>\vec{x}=c_1\vec{v}_1+\cdots +c_m\vec{v}_m</m>,
          but this would be false, unless <m>T</m> was known to be injective (which it isn't).
          Failure to be injective involves the kernel -- how do we bring that into the picture?
        </p>

        <p>
          The trick is to realize that the reason we might have <m>\vec{x}\neq c_1\vec{v}_1+\cdots +c_m\vec{v}_m</m>
          is that we're off by something in the kernel.
          Indeed, <xref ref="eqn-almost-span"/> can be re-written as
          <me>
            T(\vec{x}-c_1\vec{v}_1-\cdots -c_m\vec{v}_m) = \vec{0}
          </me>,
          so <m>\vec{x}-c_1\vec{v}_1-\cdots -c_m\vec{v}_m\in\ker T</m>.
          But we have a basis for <m>\ker T</m>, so we can write
          <me>
            \vec{x}-c_1\vec{v}_1-\cdots -c_m\vec{v}_m=t_1\vec{u}_1+\cdots +t_n\vec{u}_n
          </me>
          for some scalars <m>t_1,\ldots, t_n</m>, and this can be rearanged to give
          <me>
            \vec{x}=t_1\vec{u}_1+\cdots +t_n\vec{u}_n+c_1\vec{v}_1+\cdots + c_m\vec{v}_m
          </me>,
          which completes the proof.
        </p>
      </proof>

    </theorem>

    <p>
      This is sometimes known as the <em>Rank-Nullity Theorem</em>,
      since it can be stated in the form
      <me>
        \dim V = \rank T + \operatorname{nullity} T
      </me>.
      We will see that this result is frequently useful for providing simple arguments that establish otherwise difficult results.
      A basic situation where the theorem is useful is as follows:
      we are given <m>T:V\to W</m>, where the dimensions of <m>V</m> and <m>W</m> are known.
      Since <m>\Img T</m> is a subspace of <m>W</m>, we know from <xref ref="thm-subspace-dim">Theorem</xref>
      that <m>T</m> is onto if and only if <m>\dim \Img T = \dim W</m>.
      In many cases it is easier to compute <m>\ker T</m> than it is <m>\Img T</m>,
      and the Dimension Theorem lets us determine <m>\dim\Img T</m> if we know <m>\dim V</m> and <m>\dim \ker T</m>.
    </p>

    <p>
      A useful consequence of this result is that if we know <m>V</m> is finite-dimensional,
      we can order any basis such that the first vectors in the list are a basis of <m>\ker T</m>,
      and the images of the remaining vectors produce a basis of <m>\Img T</m>.
    </p>

    <p>
      Note that one consequence of the dimension theorem is that we must have
      <me>
        \dim \ker T\leq \dim V \quad \text{ and } \quad \dim \Img T\leq \dim V
      </me>.
      Of course, we must also have <m>\dim\Img T\leq \dim W</m>,
      since <m>\Img T</m> is a subspace of <m>W</m>.
      In the case of a matrix transformation <m>T_A</m>,
      this means that the rank of <m>T_A</m> is at most the minimum of <m>\dim V</m> and <m>\dim W</m>.
      This once again has consequences for the existence and uniqueness of solutions for linear systems with the coefficient matrix <m>A</m>.
    </p>

    <exercise xml:id="ex-dimension-injection-surjection">
      <statement>
        <p>
          Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces. Prove the following:
          <ol>
            <li>
              <p>
                <m>\dim V\leq \dim W</m> if and only if there exists an injection <m>T:V\to W</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\dim V\geq \dim W</m> if and only if there exists a surjection <m>T:V\to W</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <solution>
        <p>
          <ol>
            <li>
              <p>
                Suppose <m>T:V\to W</m> is injective. Then <m>\ker T = \{0\}</m>, so
                <me>
                  \dim V = 0 + \dim \Img T \leq \dim W
                </me>,
                since <m>\Img T</m> is a subspace of <m>W</m>.
              </p>

              <p>
                Conversely, suppose <m>\dim V\leq \dim W</m>.
                Choose a basis <m>\{\vec{v}_1,\ldots, \vec{v}_m\}</m> of <m>V</m>,
                and a basis <m>\{\vec{w}_1\,ldots, \vec{w}_n\}</m> of <m>W</m>, where <m>m\leq n</m>.
                By <xref ref="thm-define-using-basis">Theorem</xref>, there exists a linear transformation
                <m>T:V\to W</m> with <m>T(\vec{v}_i)=\vec{w}_i</m> for <m>i=1,\ldots, m</m>.
                (The main point here is that we run out of basis vectors for <m>V</m> before we run out of basis vectors for <m>W</m>.)
                This map is injective: if <m>T(\vec{v})=\vec{0}</m>,
                write <m>\vec{v}=c_1\vec{v}_1+\cdots + c_m\vec{v}_m</m>.
                Then
                <md>
                  <mrow>\vec{0} \amp = T(\vec{v})</mrow>
                  <mrow> \amp = T(c_1\vec{v}_1+\cdots + c_m\vec{v}_m)</mrow>
                  <mrow>  \amp = c_1T(\vec{v}_1)+\cdots + c_mT(\vec{v}_m)</mrow>
                  <mrow>  \amp = c_1\vec{w}_1+\cdots +c_m\vec{w}_m</mrow>
                </md>.
                Since <m>\{\vec{w}_1,\ldots, \vec{w}_m\}</m> is a subset of a basis, it's indendependent.
                Therefore, the scalars <m>c_i</m> must all be zero, and therefore <m>\vec{v}=\vec{0}</m>.
              </p>
            </li>

            <li>
              <p>
                Suppose <m>T:V\to W</m> is surjective. Then <m>\dim \Img T = \dim W</m>, so
                <me>
                  \dim W = \dim V - \dim \ker T\geq \dim V
                </me>.
                Conversely, suppose <m>\dim V\geq \dim W</m>. Again,
                choose a basis <m>\{\vec{v}_1,\ldots, \vec{v}_m\}</m> of <m>V</m>,
                and a basis <m>\{\vec{w}_1\,ldots, \vec{w}_n\}</m> of <m>W</m>,
                where this time, <m>m\geq n</m>.
                We can define a linear transformation as follows:
                <me>
                  T(\vec{v}_1)=\vec{w}_1,\ldots, T(\vec{v}_n)=\vec{w}_n, \text{ and } T(\vec{v}_j) = \vec{0} \text{ for } j>n.
                </me>
                It's easy to check that this map is a surjection:
                given <m>\vec{w}\in W</m>, we can write it in terms of our basis as <m>\vec{w}=c_1\vec{w}_1+\cdots + c_n\vec{w}_n</m>.
                Using these same scalars, we can define <m>\vec{v}=c_1\vec{v}_1+\cdots + c_n\vec{v}_n\in V</m> such that <m>T(\vec{v})=\vec{w}</m>.
              </p>

              <p>
                Note that it's not important how we define <m>T(\vec{v}_j)</m> when <m>j>n</m>.
                The point is that this time, we run out of basis vectors for <m>W</m> before we run out of basis vectors for <m>V</m>.
                Once each vector in the basis of <m>W</m> is in the image of <m>T</m>, we're guaranteed that <m>T</m> is surjective,
                and we can define the value of <m>T</m> on any remaining basis vectors however we want.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </exercise>
  </section>

  <section xml:id="sec-isomorphism">
    <title>Isomorphisms (a.k.a. invertible linear maps)</title>
    <p>
      We ended the last section with an important result.
      <xref ref="ex-dimension-injection-surjection">Exercise</xref>
      showed that existence of an injective linear map <m>T:V\to W</m>
      is equivalent to <m>\dim V\leq \dim W</m>,
      and that existence of a surjective linear map is equivalent to <m>\dim V\geq \dim W</m>.
      It's probably not surprising than that existence of a <em>bijective</em> linear map <m>T:V\to W</m>
      is equivalent to <m>\dim V = \dim W</m>.
    </p>

    <definition xml:id="def-isomorphism">
      <statement>
        <p>
          A bijective linear transformation <m>T:V\to W</m> is called an <term>isomorphism</term>.
          If such a map exists, we say that <m>V</m> and <m>W</m> are <term>isomorphic</term>, and write <m>V\cong W</m>.
        </p>
      </statement>
    </definition>

    <theorem xml:id="thm-iso-dimension">
      <statement>
        <p>
          For any finite-dimensional vector spaces <m>V</m> and <m>W</m>,
          <m>V\cong W</m> if any only if <m>\dim V = \dim W</m>.
        </p>
      </statement>
      <proof>
        <p>
          If <m>T:V\to W</m> is a bijection, then it is both injective and surjective.
          Since <m>T</m> is injective, <m>\dim V\leq \dim W</m>,
          by <xref ref="ex-dimension-injection-surjection">Exercise</xref>.
          By this same exercise,  since <m>T</m> is surjective, we must have <m>\dim V\geq \dim W</m>.
          It follows that <m>\dim V=\dim W</m>.
        </p>

        <p>
          Suppose now that <m>\dim V =\dim W</m>.
          Then we can choose bases <m>\{\vec{v}_1,\ldots, \vec{v}_n\}</m> of <m>V</m>,
          and <m>\{\vec{w}_1,\ldots, \vec{w}_n\}</m> of <m>W</m>.
          <xref ref="thm-define-using-basis">Theorem</xref> then guarantees the existence of a linear map <m>T:V\to W</m>
          such that <m>T(\vec{v}_i)=\vec{w}_i</m> for each <m>i=1,2,\ldots, n</m>.
          Repeating the arguments of <xref ref="ex-dimension-injection-surjection">Exercise</xref> shows that <m>T</m> is a bijection.
        </p>
      </proof>

    </theorem>

    <p>
      Note that buried in the theorem above is the following useful fact:
      <em>an isomorphism <m>T:V\to W</m> takes any basis of <m>V</m> to a basis of <m>W</m></em>.
      Another remarkable result of the above theorem is that <em>any two vector spaces of the same dimension are isomorphic</em>!
      In particular, we have the following theorem.
    </p>

    <theorem xml:id="thm-iso-rn">
      <statement>
        <p>
          If <m>\dim V=n</m>, then <m>V\cong \R^n</m>.
        </p>
      </statement>
    </theorem>
    <p>
      This theorem is a direct consequence of <xref ref="thm-iso-dimension"/>.
      But it's useful to understand how it works in practice.
    </p>

    <definition xml:id="def-coefficient-iso">
      <statement>
        <p>
          Let <m>V</m> be a finite-dimensional vector space,
          and let <m>B=\{\vec{e}_1,\ldots, \vec{e}_n\}</m> be a basis for <m>V</m>.
          The <term>coefficient isomorphism</term> associated to <m>B</m>
          is the map <m>C_B:V\to \R^n</m> defined by
          <me>
            C_B(c_1\vec{e}_1+c_2\vec{e}_2+\cdots +c_n\vec{e}_n)=\bbm c_1\\c_2\\\vdots \\c_n\ebm
          </me>.

        </p>
      </statement>
    </definition>
    <p>
      Note that this is a well-defined map since every vector in <m>V</m> can be written uniquely in terms of the basis <m>B</m>.
    </p>

    <p>
      The coefficient isomorphism is especially useful when we want to analyze a linear map computationally.
      Suppose we're given <m>T:V\to W</m> where <m>V, W</m> are finite-dimensional.
      Let us choose bases <m>B=\{\vec{v}_1,\ldots, \vec{v}_n\}</m> of <m>V</m>
      and <m>B' = \{\vec{w}_1,\ldots, \vec{w}_m\}</m> of <m>W</m>.
      The choice of these two bases determines scalars <m>a_{ij}, 1\leq i\leq n, 1\leq j\leq m</m>, such that
      <me>
        T(\vec{v}_j) = a_{1j}\vec{w}_1+a_{2j}\vec{w}_2+\cdots + a_{mj}\vec{w}_j,
      </me>
      for each <m>i=1,2,\ldots, n</m>. The resulting matrix <m>A=[a_{ij}]</m>
      defines a matrix transformation <m>T_A:\R^n\to \R^m</m> such that
      <me>
        T_A\circ C_B = C_{B'}\circ T
      </me>.
      The relationship among the four maps used here is best captured by the following <q>commutative diagram</q>:
    </p>

    <figure xml:id="fig_transformation_matrix">
      <caption>Defining the matrix of a linear map with respect to choices of basis.</caption>
      <image xml:id="img_trans_matrix" width="35%">
        <latex-image>
          \begin{tikzpicture}
          \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
          {
          V \amp W \\
          \R^n \amp \R^m \\};
          \path[-stealth]
          (m-1-1) edge node [left] {$C_B$} (m-2-1)
                  edge node [above] {$T$} (m-1-2)
          (m-2-1) edge node [above] {$T_A$} (m-2-2)
          (m-1-2) edge node [right] {$C_{B'}$} (m-2-2);
          \end{tikzpicture}
        </latex-image>
      </image>
    </figure>

    <p>
      With this connection between linear maps (in general) and matrices,
      it can be worthwhile to pause and consider invertibility in the context of matrices.
      Recall that an <m>n\times n</m> matrix <m>A</m> is <em>invertible</em> if there exists a matrix <m>A^{-1}</m>
      such that <m>AA^{-1}=I_n</m> and <m>A^{-1}A=I_n</m>.
    </p>

    <p>
      The same definition can be made for linear maps.
      We've defined what it means for a map <m>T:V\to W</m> to be invertible as a <em>function</em>.
      In particular, we relied on the fact that any bijection has an inverse.
    </p>

    <p>
      First, a note on notation. Given linear maps <m>U\xrightarrow{T} V\xrightarrow{S} W</m>,
      we typically write the composition <m>S\circ T:U\to W</m> as a <q>product</q> <m>ST</m>.
      The reason for this is again to mimic the case of matrices. Let <m>A</m> be an <m>m\times n</m>
      matrix, and let <m>B</m> be an <m>n\times k</m> matrix. Then we have linear maps
      <me>
        \R^k \xrightarrow{T_B} \R^n\xrightarrow{T_A} \R^m
      </me>,
      and the composition <m>T_A\circ T_B:\R^k\to \R^m</m> satisfies
      <me>
        T_A\circ T_B(\vec{x}) = T_A(T_B(\vec{x})) = T_A(B\vec{x})=A(B\vec{x})=(AB)\vec{x}=T_{AB}(\vec{x})
      </me>.
      Note that the rules given in elementary linear algebra, for the relative sizes of matrices that can be multiplied,
      are simply a manifestation of the fact that to compose functions,
      the range of the first must be contained in the domain of the second.
    </p>

    <exercise>
      <statement>
        <p>
          Show that the composition of two linear maps is again a linear map.
        </p>
      </statement>
      <solution>
        <p>
          Suppose we have linear maps <m>U\xrightarrow{T} V\xrightarrow{S} W</m>,
          and let <m>\vec{u}_1,\vec{u}_2\in U</m>. Then
          <md>
            <mrow>ST(\vec{u}_1+\vec{u}_2) \amp = S(T(\vec{u}_1+\vec{u}_2)) </mrow>
            <mrow> \amp = S(T(\vec{u}_1)+T(\vec{u}_2))</mrow>
            <mrow>  \amp = S(T(\vec{u}_1))+S(T(\vec{u}_2)) </mrow>
            <mrow>  \amp = ST(\vec{u}_1)+ST(\vec{u}_2)</mrow>
          </md>,
          and for any scalar <m>c</m>,
          <me>
            ST(c\vec{u}_1) = S(T(c\vec{u}_1))=S(cT(\vec{u}_1)) = cS(T(\vec{u}_1))=c(ST(\vec{u}_1))
          </me>.

        </p>
      </solution>
    </exercise>
    <p>
      Moreover, <xref ref="thm-iso-dimension"/> tells us why we can only consider invertibility for square matrices:
      we know that invertible linear maps are only defined between spaces of equal dimension.
      In analogy with matrices, some texts will define a linear map <m>T:V\to W</m>
      to be invertible if there exists a linear map <m>S:W\to V</m> such that
      <me>
        ST = 1_V \quad \text{ and } \quad TS = 1_W
      </me>.
      From this definition, one can show that <m>S</m> and <m>T</m> must be bijections,
      and of course, if <m>T</m> is a bijection (as in our definition) then we know it has an inverse.
      What remains to be seen is that this inverse is also a linear map.
    </p>

    <exercise>
      <statement>
        <p>
          Let <m>T:V\to W</m> be a bijective linear transformation.
          Show that <m>T^{-1}:W\to V</m> is a linear transformation.
        </p>
      </statement>
      <solution>
        <p>
          Let <m>\vec{w}_1,\vec{w}_2\in W</m>.
          Then there exist <m>\vec{v}_1,\vec{v}_2\in V</m>  with <m>\vec{w}_1=T(\vec{v}_1), \vec{w}_2=T(\vec{v}_2)</m>.
          We then have
          <md>
            <mrow>T^{-1}(\vec{w}_1+\vec{w}_2) \amp = T^{-1}(T(\vec{v}_1)+T(\vec{v}_2)) </mrow>
            <mrow> \amp = T^{-1}(T(\vec{v}_1+\vec{v}_2))</mrow>
            <mrow>  \amp = \vec{v}_1+\vec{v}_2</mrow>
            <mrow>  \amp = T^{-1}(\vec{w}_1)+T^{-1}(\vec{w}_2)</mrow>
          </md>.
          For any scalar <m>c</m>, we similarly have
          <me>
            T^{-1}(c\vec{w}_1) = T^{-1}(cT(\vec{v}_1))=T^{-1}(T(c\vec{v}_1)) = c\vec{v}_1 = cT^{-1}(\vec{w}_1)
          </me>.
        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Show that if <m>ST=1_V</m>, then <m>S</m> is surjective and <m>T</m> is injective.
          Conclude that if <m>ST=1_V</m> and <m>TS=1_w</m>, then <m>S</m> and <m>T</m> are both bijections.
        </p>
      </statement>
      <hint>
        <p>
          This is really a Math 2000 problem.
        </p>
      </hint>
    </exercise>
  </section>
</chapter>
