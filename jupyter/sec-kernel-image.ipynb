{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">2.2<\/span> <span class=\"title\">Kernel and Image<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-257\">Given any linear transformation $T:V\\to W$ we can associate two important subspaces: the <dfn class=\"terminology\">kernel<\/dfn> of $T$ (also known as the <dfn class=\"terminology\">nullspace<\/dfn>), and the <dfn class=\"terminology\">image<\/dfn> of $T$ (also known as the <dfn class=\"terminology\">range<\/dfn>).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-kernel-image\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-258\">Let $T:V\\to W$ be a linear transformation. The <dfn class=\"terminology\">kernel<\/dfn> of $T\\text{,}$ denoted $\\ker T\\text{,}$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ker T = \\{\\vv\\in V \\,|\\, T(\\vv)=\\mathbf{0}\\}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">The <dfn class=\"terminology\">image<\/dfn> of $T\\text{,}$ denoted $\\im T\\text{,}$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\im T = \\{T(\\vv) \\,|\\, \\vv\\in V\\}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-259\">Note that the kernel of $T$ is just the set of all vectors $T$ sends to zero. The image of $T$ is the range of $T$ in the usual sense of the range of a function.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-ker-img-subspace\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-260\">For any linear transformation $T:V\\to W\\text{,}$<\/p><ol class=\"decimal\"><li id=\"li-70\"><p id=\"p-261\">$\\ker T$ is a subspace of $V\\text{.}$<\/p><\/li><li id=\"li-71\"><p id=\"p-262\">$\\im T$ is a subspace of $W\\text{.}$<\/p><\/li><\/ol><\/article><article class=\"proof\" id=\"proof-11\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><ol id=\"p-263\" class=\"decimal\"><li id=\"li-72\"><p id=\"p-264\">To show that $\\ker T$ is a subspace, first note that $\\mathbf{0}\\in \\ker T\\text{,}$ since $T(\\mathbf{0})=\\mathbf{0}$ for any linear transformation $T\\text{.}$ If $\\vv,\\ww\\in \\ker T\\text{,}$ then $T(\\vv)=\\mathbf{0}$ and $T(\\ww)=0\\text{,}$ and therefore,<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(\\vv+\\ww)=T(\\vv)+T(\\ww)=\\mathbf{0}+\\mathbf{0}=\\mathbf{0}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Similarly, for any scalar $c$ and $\\vv\\in \\ker T\\text{,}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(c\\vv)=cT(\\vv)=c\\mathbf{0}=\\mathbf{0}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">By the subspace test, $\\ker T$ is a subspace.<\/p><\/li><li id=\"li-73\"><p id=\"p-265\">Again, since $T(\\mathbf{0})=\\mathbf{0}\\text{,}$ we see that $\\mathbf{0}\\in \\im T\\text{,}$ so $\\im T$ is nonempty. If $\\ww_1,\\ww_2\\in \\im T\\text{,}$ then there exist $\\vv_1,\\vv_2\\in V$ such that $T(\\vv_1)=\\ww_1$ and $T(\\vv_2)=\\ww_2\\text{.}$ It follows that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ww_1+\\ww_2 = T(\\vv_1)+T(\\vv_2) = T(\\vv_1+\\vv_2)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $\\ww_1+\\ww_2\\in \\im T\\text{.}$ Similarly, if $c$ is any scalar and $\\ww=T(\\vv)\\in\\im T\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nc\\ww=cT(\\vv)=T(c\\vv)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $c\\ww\\in \\im T\\text{.}$<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-266\">A familiar setting that you may already have encountered in a previous linear algebra course is that of a matrix transformation. Let $A$ be an $m\\times n$ matrix. Then we can define $T:\\R^n\\to \\R^m$ by $T(\\xx)=A\\xx\\text{,}$ where elements of $\\R^n,\\R^m$ are considered as column vectors. We then have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\ker T = \\nll(A) = \\{\\xx\\in \\R^n \\,|\\, A\\xx=\\mathbf{0}\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\im T = \\csp(A) = \\{A\\xx\\,|\\, \\xx\\in \\R^n\\}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $\\csp(A)$ denotes the <dfn class=\"terminology\">column space<\/dfn> of $A\\text{.}$ Recall further that if we write $A$ in terms of its columns as<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA = \\bbm C_1 \\amp C_2 \\amp \\cdots \\amp C_n\\ebm\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and a vector $\\xx\\in \\R^n$ as $\\xx=\\bbm x_1\\\\x_2\\\\\\vdots \\\\x_n\\ebm\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA\\xx = x_1C_1+x_2C_2+\\cdots +x_nC_n\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Thus, any element of $\\csp(A)$ is a linear combination of its columns, explaining the name <em class=\"emphasis\">column space<\/em>.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-267\">Determining $\\nll(A)$ and $\\csp(A)$ for a given matrix $A$ is, unsurprisingly, a matter of reducing $A$ to row-echelon form. Finding $\\nll(A)$ is simply a matter of describing the set of all solutions to the homogeneous system $A\\xx=\\mathbf{0}\\text{.}$ Finding $\\csp(A)$ relies on the following theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-colspace\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-268\">Let $A$ be an $m\\times n$ matrix with columns $C_1,C_2,\\ldots, C_n\\text{.}$ If the reduced row-echelon form of $A$ has leading ones in columns $j_1,j_2,\\ldots, j_k\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\{C_{j_1},C_{j_2},\\ldots, C_{j_k}\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">is a basis for $\\csp(A)\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-269\">The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson. To see why it works, we need to remember a few basic facts from elementary linear algebra. First, recall that performing an elementary row operation on a matrix $A$ is equivalent to multiplying on the left by an elementary matrix $E$ defined using the same row operation.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-270\">Since every elementary matrix is invertible, and any product of invertible matrices is invertible, and we can transform $A$ into a row-echelon matrix $R$ using elementary row operations, it follows that $R = UA$ for an invertible matrix $U\\text{;}$ indeed, we have $U = E_kE_{k-1}\\cdots E_2E_1\\text{,}$ where $E_1,\\ldots, E_k$ are the elementary matrices corresponding to the row operations used to carry $A$ to $R\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-271\">A basis for $\\csp(R)$ is given by the columns of $R$ containing the leading ones. The reason for this is as follows. First, recall that each nonzero row begins with a leading one. So if the leading ones of $R$ are in columns $i_1,\\ldots, i_k\\text{,}$ then there are $k$ nonzero rows. Since all rows of zeros go at the bottom, each column in $R$ has its last $m-k$ entries identically zero. Thus,<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\csp(R)\\subseteq \\left\\{\\bbm a_1\\\\\\vdots \\\\a_k\\\\0\\\\\\vdots 0\\ebm\\in \\R^m \\,|\\, a_1,\\ldots, a_k\\in\\R\\right\\}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $\\dim \\csp(R)\\leq k\\text{.}$ But the columns containing leading ones are easily shown to be independent, so they form a basis of $\\csp(R)\\text{,}$ which therefore has dimension $k=\\operatorname{rank}(A)\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-272\">Next, since $R=UA\\text{,}$ where $U$ is invertible, if $R=\\bbm Y_1\\amp Y_2\\amp \\cdots \\amp Y_n\\ebm$ and $A = \\bbm C_1\\amp C_2\\amp \\cdots \\amp C_n\\ebm\\text{,}$ then $C_i = U^{-1}Y_i$ for each $i\\text{.}$ It follows from the fact that $U$ is invertible and that the columns containing leading ones in $R$ form a basis for $\\csp(R)$ that the corresponding columns in $A$ form a basis for $\\csp(A)\\text{.}$ (For details, see Section 5.4 in Nicholson.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-273\">For example, consider the linear transformation $T:\\R^4\\to \\R^3$ defined by the matrix<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nA = \\bbm 1 \\amp 3 \\amp 0 \\amp -2\\\\\n-2 \\amp -1 \\amp 2 \\amp 0\\\\\n1 \\amp 8 \\amp 2 \\amp -6\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Let's determine the RREF of $A\\text{:}$<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])\nA.rref()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-274\">We see that there are leading ones in the first and second column. Therefore, $\\csp(A) = \\im(T) = \\spn\\left\\{\\bbm 1\\\\-2\\\\1\\ebm, \\bbm 3\\\\-1\\\\8\\ebm\\right\\}\\text{.}$ Indeed, note that<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\bbm 0\\\\2\\\\2\\ebm = -\\frac65\\bbm 1\\\\-2\\\\1\\ebm + \\frac25\\bbm 3\\\\-1\\\\8\\ebm\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\bbm -2\\\\0\\\\-6\\ebm = \\frac25\\bbm 1\\\\-2\\\\1\\ebm -\\frac45\\bbm 3\\\\-1\\\\8\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so that indeed, the third and fourth columns are in the span of the first and second.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-275\">Furthermore, we can determine the nullspace: if $A\\xx=\\mathbf{0}$ where $\\xx=\\bbm x_1\\\\x_2\\\\x_3\\\\x_4\\ebm\\text{,}$ then we must have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{align*}\nx_1 \\amp =\\frac65 x_3-\\frac25 x_4\\\\\nx_2 \\amp =-\\frac25 x_3+\\frac 45 x_4\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">so<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx = \\bbm \\frac65x_3-\\frac25x_4\\\\ -\\frac25x_3+\\frac45x_4\\\\x_3\\\\x_4\\ebm = \\frac{x_3}{5}\\bbm 6\\\\-2\\\\5\\\\0\\ebm + \\frac{x_4}{5}\\bbm -2\\\\4\\\\0\\\\5\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">It follows that a basis for $\\nll(A)=\\ker T$ is $\\left\\{\\bbm 6\\\\-2\\\\5\\\\0\\ebm, \\bbm -2\\\\4\\\\0\\\\5\\ebm\\right\\}\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-276\">Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space. But it's probably worth your while to know how to determine these from the <abbr class=\"initialism\">RREF<\/abbr> of a matrix, without additional help from the computer. That said, let's see how the computer's output compares to what we found:<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.nullspace()"], "outputs":[]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.columnspace()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-277\">Note that the output from the computer simply states the basis for each space. Of course, for computational purposes, this is typically good enough.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-278\">An important result that comes out while trying to show that the “pivot columns” of a matrix (the ones that end up with leading ones in the RREF) are a basis for the column space is that the column rank (defined as the dimension of $\\csp(A)$) and the row rank (the dimension of the space spanned by the columns of $A$) are equal. One can therefore speak unambiguously about the <dfn class=\"terminology\">rank<\/dfn> of a matrix $A\\text{,}$ and it is just as it's defined in a first course in linear algebra: the number of leading ones in the RREF of $A\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-279\">For a general linear transformation, we can't necessarily speak in terms of rows and columns, but if $T:V\\to W$ is linear, and either $V$ or $W$ is finite-dimensional, then we can define the rank of $T$ as follows.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-rank-transformation\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-280\">Let $T:V\\to W$ be a linear transformation. Then the <dfn class=\"terminology\">rank<\/dfn> of $T$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\operatorname{rank} T = \\dim \\im T\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and the <dfn class=\"terminology\">nullity<\/dfn> of $T$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\operatorname{nullity} T = \\dim \\ker T\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-281\">Note that if $W$ is finite-dimensional, then so is $\\im T\\text{,}$ since it's a subspace of $W\\text{.}$ On the other hand, if $V$ is finite-dimensional, then we can find a basis $\\{\\vv_1,\\ldots, \\vv_n\\}$ of $V\\text{,}$ and the set $\\{T(\\vv_1),\\ldots, T(\\vv_n)\\}$ will span $\\im T\\text{,}$ so again the image is finite-dimensional, so the rank of $T$ is finite. It is possible for either the rank or the nullity of a transformation to be infinite.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-282\">Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces. From the textbook, we have the following nice example.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"example example-like\" id=\"example-4\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Example<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-283\">Let $T:M_{nn}\\to M_{nn}$ be defined by $T(A)=A-A^T\\text{.}$ Then<\/p><ol class=\"decimal\"><li id=\"li-74\"><p id=\"p-284\">$T$ is a linear map.<\/p><\/li><li id=\"li-75\"><p id=\"p-285\">$\\ker T$ is equal to the set of all symmetric matrices.<\/p><\/li><li id=\"li-76\"><p id=\"p-286\">$\\im T$ is equal to the set of all skew-symmetric matrices.<\/p><\/li><\/ol><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-17\" id=\"solution-17\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-17\"><div class=\"solution solution-like\"><ol id=\"p-287\" class=\"decimal\"><li id=\"li-77\"><p id=\"p-288\">We have $T(0)=0$ since $0^T=0\\text{.}$ Using proerties of the transpose and matrix algebra, we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)\\text{.}\n\\end{equation*}\n<\/div><\/li><li id=\"li-78\"><p id=\"p-289\">It's clear that if $A^T=A\\text{,}$ then $T(A)=0\\text{.}$ On the other hand, if $T(A)=0\\text{,}$ then $A-A^T=0\\text{,}$ so $A=A^T\\text{.}$ Thus, the kernel consists of all symmetric matrices.<\/p><\/li><li id=\"li-79\"><p id=\"p-290\">If $B=T(A)=A-A^T\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nB^T = (A-A^T)^T = A^T-A = -B\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so certainly every matrix in $\\im A$ is skew-symmetric. On the other hand, if $B$ is skew-symmetric, then $B=T(\\frac12 B)\\text{,}$ since<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT\\Bigl(\\frac12 B\\Bigr) = \\frac12 T(B) = \\frac12(B-B^T) = \\frac12(B-(-B))= B\\text{.}\n\\end{equation*}\n<\/div><\/li><\/ol><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-291\">You'll recall from a course like Math 2000 that in the study of functions, the properties of being injective (one-to-one) and surjective (onto) are important. They're important for linear transformations as well, and defined in exactly the same way.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-292\">It's clear that being surjective is closely tied to image. Indeed, by definition, $T:V\\to W$ is onto if $\\im T = W\\text{.}$ What might not be immediately obvious is that the kernel tells us if a linear map is injective.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-injective-kernel\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.6<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-293\">Let $T:V\\to W$ be a linear transformation. Then $T$ is injective if and only if $\\ker T = \\{\\mathbf{0}\\}\\text{.}$<\/p><\/article><article class=\"proof\" id=\"proof-12\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-294\">Suppose $T$ is injective, and let $\\vv\\in \\ker T\\text{.}$ Then $T(\\vv)=\\mathbf{0}\\text{.}$ On the other hand, we know that $T(\\mathbf{0})=\\mathbf{0}\\text{,}$ and since $T$ is injective, we must have $\\vv=\\mathbf{0}\\text{.}$ Conversely, suppose that $\\ker T = \\{0\\}$ and that $T(\\vv_1)=T(\\vv_2)$ for some $\\vv_1,\\vv_2\\in V\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\mathbf{0} = T(\\vv_1)-T(\\vv_2) = T(\\vv_1-\\vv_2)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $\\vv_1-\\vv_2\\in \\ker T\\text{.}$ Therefore, we must have $\\vv_1-\\vv_2=\\mathbf{0}\\text{,}$ so $\\vv_1=\\vv_2\\text{,}$ and it follows that $T$ is injective.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-295\">Let us return to the case of a matrix transformation $T_A:\\R^n\\to \\R^m\\text{.}$ Notice that $\\ker T_A$ is simply the set of all solutions to $A\\xx=\\mathbf{0}\\text{,}$ while $\\im T_A$ is the set of all $\\yy\\in\\R^m$ for which $A\\xx=\\yy$ <em class=\"emphasis\">has<\/em> a solution.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-296\">Recall from the discussion above that $\\rank A = \\dim \\csp(A) = \\dim \\im T_A\\text{.}$ It follows that $T_A$ is surjective if and only if $\\rank A = m\\text{.}$ On the other hand, $T_A$ is injective if and only if $\\rank A = n\\text{,}$ because we know that the system $A\\xx=\\mathbf{0}$ has a unique solution if and only if each column of $A$ contains a leading one.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-297\">This has some interesting consequences. If $m=n$ (that is, if $A$ is square), then each increase in $\\dim \\nll(A)$ produces a corresponding decrease in $\\dim \\csp(A)\\text{,}$ since both correspond to the “loss” of a leading one. Moreover, if $\\rank A = n\\text{,}$ then $T_A$ is both injective and surjective. Recall that a function is invertible if and only if it is both injective and surjective. It should come as no surprise that invertibility of $T_A$ (as a function) is equivalent to invertibility of $A$ (as a matrix).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-298\">Also, note that if $m \\lt n\\text{,}$ then $\\rank A\\leq m \\lt n\\text{,}$ so $T_A$ could be surjective, but can't possibly be injective. On the other hand, if $m\\gt n\\text{,}$ then $\\rank A\\leq n \\lt m\\text{,}$ so $T_A$ could be injective, but can't possibly be surjective. These results generalize to linear transformations between any finite-dimensional vector spaces. The first step towards this is the following theorem, which is sometimes known as the Fundamental Theorem of Linear Transformations.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-dimension-lintrans\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.7<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Dimension Theorem.<\/span><\/h6><p id=\"p-299\">Let $T:V\\to W$ be any linear transformation such that $\\ker T$ and $\\im T$ are finite-dimensional. Then $V$ is finite-dimensional, and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim V = \\dim \\ker T + \\dim \\im T\\text{.}\n\\end{equation*}\n<\/div><\/article><article class=\"proof\" id=\"proof-13\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-300\">The trick with this proof is that we aren't assuming $V$ is finite-dimensional, so we can't start with a basis of $V\\text{.}$ But we do know that $\\im T$ is finite-dimensional, so we start with a basis $\\{\\ww_1,\\ldots, \\ww_m\\}$ of $\\im T\\text{.}$ Of course, every vector in $\\im T$ is the image of some vector in $V\\text{,}$ so we can write $\\ww_i =T(\\vv_i)\\text{,}$ where $\\vv_i\\in V\\text{,}$ for $i=1,2,\\ldots, m\\text{.}$<\/p><p id=\"p-301\">Since $\\{T(\\vv_1),\\ldots, T(\\vv_m)\\}$ is a basis, it is linearly independent. The results of <a href=\"sec-lin-tran-intro.ipynb#ex%5Flintrans-indep\" class=\"internal\" title=\"Exercise 2.1.10\">Exercise 2.1.10<\/a> tell us that the set $\\{\\vv_1,\\ldots, \\vv_m\\}$ must therefore be independent.<\/p><p id=\"p-302\">We now introduce a basis $\\{\\uu_1,\\ldots, \\uu_n\\}$ of $\\ker T\\text{,}$ which we also know to be finite-dimensional. If we can show that the set $\\{\\uu_1,\\ldots, \\uu_n,\\vv_1,\\ldots, \\vv_m\\}$ is a basis for $V\\text{,}$ we'd be done, since the number of vectors in this basis is $\\dim\\ker T + \\dim \\im T\\text{.}$ We must therefore show that this set is independent, and that it spans $V\\text{.}$<\/p><p id=\"p-303\">To see that it's independent, suppose that<\/p><div class=\"displaymath\">\n\\begin{equation*}\na_1\\uu_1+\\cdots + a_n\\uu_n+b_1\\vv_1+\\cdots +b_m\\vv_m=\\mathbf{0}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Applying $T$ to this equation, and noting that $T(\\uu_i)=\\mathbf{0}$ for each $i\\text{,}$ by definition of the $\\uu_i\\text{,}$ we get<\/p><div class=\"displaymath\">\n\\begin{equation*}\nb_1T(\\vv_1)+\\cdots +b_mT(\\vv_m)=\\mathbf{0}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">We assumed that the vectors $T(\\vv_i)$ were independent, so all the $b_i$ must be zero. But then we get<\/p><div class=\"displaymath\">\n\\begin{equation*}\na_1\\uu_1+\\cdots +a_n\\uu_n=\\mathbf{0}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and since the $\\uu_i$ are independent, all the $a_i$ must be zero.<\/p><p id=\"p-304\">To see that these vectors span, choose any $\\xx\\in V\\text{.}$ Since $T(\\xx)\\in \\im T\\text{,}$ there exist scalars $c_1,\\ldots, c_m$ such that<\/p><div class=\"displaymath\">\n\\begin{equation}\nT(\\xx)=c_1T(\\vv_1)+\\cdots +c_mT(\\vv_m)\\text{.}\\label{eqn-almost-span}\\tag{2.2.1}\n\\end{equation}\n<\/div><p data-braille=\"continuation\">We'd like to be able to conclude from this that $\\xx=c_1\\vv_1+\\cdots +c_m\\vv_m\\text{,}$ but this would be false, unless $T$ was known to be injective (which it isn't). Failure to be injective involves the kernel -- how do we bring that into the picture?<\/p><p id=\"p-305\">The trick is to realize that the reason we might have $\\xx\\neq c_1\\vv_1+\\cdots +c_m\\vv_m$ is that we're off by something in the kernel. Indeed, <a href=\"sec-kernel-image.ipynb#mjx-eqn-eqn-almost-span\" class=\"internal\" title=\"Equation 2.2.1\">(2.2.1)<\/a> can be re-written as<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(\\xx-c_1\\vv_1-\\cdots -c_m\\vv_m) = \\mathbf{0}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $\\xx-c_1\\vv_1-\\cdots -c_m\\vv_m\\in\\ker T\\text{.}$ But we have a basis for $\\ker T\\text{,}$ so we can write<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx-c_1\\vv_1-\\cdots -c_m\\vv_m=t_1\\uu_1+\\cdots +t_n\\uu_n\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">for some scalars $t_1,\\ldots, t_n\\text{,}$ and this can be rearanged to give<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx=t_1\\uu_1+\\cdots +t_n\\uu_n+c_1\\vv_1+\\cdots + c_m\\vv_m\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which completes the proof.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-306\">This is sometimes known as the <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">Rank-Nullity Theorem<\/em>, since it can be stated in the form<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim V = \\rank T + \\operatorname{nullity} T\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">We will see that this result is frequently useful for providing simple arguments that establish otherwise difficult results. A basic situation where the theorem is useful is as follows: we are given $T:V\\to W\\text{,}$ where the dimensions of $V$ and $W$ are known. Since $\\im T$ is a subspace of $W\\text{,}$ we know from <a href=\"sec-dimension.ipynb#thm-subspace-dim\" class=\"internal\" title=\"Theorem 1.6.16\">Theorem 1.6.16<\/a> that $T$ is onto if and only if $\\dim \\im T = \\dim W\\text{.}$ In many cases it is easier to compute $\\ker T$ than it is $\\im T\\text{,}$ and the Dimension Theorem lets us determine $\\dim\\im T$ if we know $\\dim V$ and $\\dim \\ker T\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-307\">A useful consequence of this result is that if we know $V$ is finite-dimensional, we can order any basis such that the first vectors in the list are a basis of $\\ker T\\text{,}$ and the images of the remaining vectors produce a basis of $\\im T\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-308\">Note that one consequence of the dimension theorem is that we must have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\dim \\ker T\\leq \\dim V \\quad \\text{ and } \\quad \\dim \\im T\\leq \\dim V\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Of course, we must also have $\\dim\\im T\\leq \\dim W\\text{,}$ since $\\im T$ is a subspace of $W\\text{.}$ In the case of a matrix transformation $T_A\\text{,}$ this means that the rank of $T_A$ is at most the minimum of $\\dim V$ and $\\dim W\\text{.}$ This once again has consequences for the existence and uniqueness of solutions for linear systems with the coefficient matrix $A\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"ex-dimension-injection-surjection\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2.2.8<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-309\">Let $V$ and $W$ be finite-dimensional vector spaces. Prove the following:<\/p><ol class=\"decimal\"><li id=\"li-80\"><p id=\"p-310\">$\\dim V\\leq \\dim W$ if and only if there exists an injection $T:V\\to W\\text{.}$<\/p><\/li><li id=\"li-81\"><p id=\"p-311\">$\\dim V\\geq \\dim W$ if and only if there exists a surjection $T:V\\to W\\text{.}$<\/p><\/li><\/ol><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-18\" id=\"solution-18\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-18\"><div class=\"solution solution-like\"><ol id=\"p-312\" class=\"decimal\"><li id=\"li-82\"><p id=\"p-313\">Suppose $T:V\\to W$ is injective. Then $\\ker T = \\{0\\}\\text{,}$ so<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim V = 0 + \\dim \\im T \\leq \\dim W\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">since $\\im T$ is a subspace of $W\\text{.}$<\/p><p id=\"p-314\">Conversely, suppose $\\dim V\\leq \\dim W\\text{.}$ Choose a basis $\\{\\vv_1,\\ldots, \\vv_m\\}$ of $V\\text{,}$ and a basis $\\{\\ww_1\\,ldots, \\ww_n\\}$ of $W\\text{,}$ where $m\\leq n\\text{.}$ By <a href=\"sec-lin-tran-intro.ipynb#thm-define-using-basis\" class=\"internal\" title=\"Theorem 2.1.5\">Theorem 2.1.5<\/a>, there exists a linear transformation $T:V\\to W$ with $T(\\vv_i)=\\ww_i$ for $i=1,\\ldots, m\\text{.}$ (The main point here is that we run out of basis vectors for $V$ before we run out of basis vectors for $W\\text{.}$) This map is injective: if $T(\\vv)=\\mathbf{0}\\text{,}$ write $\\vv=c_1\\vv_1+\\cdots + c_m\\vv_m\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\mathbf{0} \\amp = T(\\vv)\\\\\n\\amp = T(c_1\\vv_1+\\cdots + c_m\\vv_m)\\\\\n\\amp = c_1T(\\vv_1)+\\cdots + c_mT(\\vv_m)\\\\\n\\amp = c_1\\ww_1+\\cdots +c_m\\ww_m\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">Since $\\{\\ww_1,\\ldots, \\ww_m\\}$ is a subset of a basis, it's independent. Therefore, the scalars $c_i$ must all be zero, and therefore $\\vv=\\mathbf{0}\\text{.}$<\/p><\/li><li id=\"li-83\"><p id=\"p-315\">Suppose $T:V\\to W$ is surjective. Then $\\dim \\im T = \\dim W\\text{,}$ so<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim W = \\dim V - \\dim \\ker T\\geq \\dim V\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Conversely, suppose $\\dim V\\geq \\dim W\\text{.}$ Again, choose a basis $\\{\\vv_1,\\ldots, \\vv_m\\}$ of $V\\text{,}$ and a basis $\\{\\ww_1\\,ldots, \\ww_n\\}$ of $W\\text{,}$ where this time, $m\\geq n\\text{.}$ We can define a linear transformation as follows:<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(\\vv_1)=\\ww_1,\\ldots, T(\\vv_n)=\\ww_n, \\text{ and } T(\\vv_j) = \\mathbf{0} \\text{ for } j>n.\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">It's easy to check that this map is a surjection: given $\\ww\\in W\\text{,}$ we can write it in terms of our basis as $\\ww=c_1\\ww_1+\\cdots + c_n\\ww_n\\text{.}$ Using these same scalars, we can define $\\vv=c_1\\vv_1+\\cdots + c_n\\vv_n\\in V$ such that $T(\\vv)=\\ww\\text{.}$<\/p><p id=\"p-316\">Note that it's not important how we define $T(\\vv_j)$ when $j>n\\text{.}$ The point is that this time, we run out of basis vectors for $W$ before we run out of basis vectors for $V\\text{.}$ Once each vector in the basis of $W$ is in the image of $T\\text{,}$ we're guaranteed that $T$ is surjective, and we can define the value of $T$ on any remaining basis vectors however we want.<\/p><\/li><\/ol><\/div><\/div><\/div><\/article><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-kernel-image.ipynb"}
}