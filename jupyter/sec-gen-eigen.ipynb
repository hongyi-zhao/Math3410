{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">5.4<\/span> <span class=\"title\">Generalized eigenspaces<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-681\">Example <a href=\"sec-direct-sum.ipynb#eg-invariant-block\" class=\"internal\" title=\"Example 5.3.9\">Example 5.3.9<\/a> showed us that if $V=U\\oplus W\\text{,}$ where $U$ and $W$ are $T$-invariant, then the matrix $M_B(T)$ has block diagonal form $\\bbm A \\amp 0\\\\0\\amp B\\ebm\\text{,}$ as long as the basis $B$ is the union of bases of $U$ and $W\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-682\">We want to take this idea further. If $V = U_1\\oplus U_2\\oplus \\cdots \\oplus U_k\\text{,}$ where each subspace $U_j$ is $T$-invariant, then with respect to a basis $B$ consisting of basis vectors for each subspace, we will have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nM_B(T)=\\bbm A_1 \\amp 0 \\amp \\cdots \\amp 0\\\\\n0 \\amp A_2 \\amp \\cdots \\amp 0\\\\\n\\vdots \\amp \\vdots \\amp \\ddots \\amp \\vdots\\\\\n0 \\amp 0 \\amp \\cdots \\amp A_k\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where each $A_j$ is the matrix of $T|_{U_j}$ with respect to some basis of $U_j\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-683\">Our goal moving forward is twofold: one, to make the blocks as small as possible, so that $M_B(T)$ is as close to diagonal as possible, and two, to make the blocks as simple as possible. Of course, if $T$ is diagonalizable, then we can get all blocks down to size $1\\times 1\\text{,}$ but this is not always possible.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-684\">Recall from <a href=\"subsec-eigen-basics.ipynb\" class=\"internal\" title=\"Section 4.1: Eigenvalues and Eigenvectors\">Section 4.1<\/a> that if the characteristic polynomial of $T$ (or equivalently, any matrix representation $A$ of $T$) is<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nc_T(x) = (x-\\lambda_1)^{m_1}(x-\\lambda_2)^{m_2}\\cdots (x-\\lambda_k)^{m_k}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">then $\\dim E_{\\lambda_j}(T)\\leq m_j$ for each $j=1,\\ldots, k\\text{,}$ and $T$ is diagonalizable if and only if we have equality for each $j\\text{.}$ (This guarantees that we have sufficiently many independent eigenvectors to form a basis of $V\\text{.}$)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-685\">Since eigenspaces are $T$-invariant, we see that being able to diagonalize $T$ is equivalent to having the direct sum decomposition<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nV = E_{\\lambda_1}(T)\\oplus E_{\\lambda_2}(T)\\oplus \\cdots \\oplus E_{\\lambda_k}(T)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">If $T$ cannot be diagonalized, it's because we came up short on the number of eigenvectors, and the direct sum of all eigenspaces only produces some subspace of $V$ of lower dimension. We now consider how one might enlarge a set of independent eigenvectors in some standard, and ideally optimal, way.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-686\">First, we note that for any operator $T\\text{,}$ the restriction of $T$ to $\\ker T$ is the zero operator, since by definition, $T(\\vv)=\\zer$ for all $\\vv\\in\\ker T\\text{.}$ Since we define $E_{\\lambda}(T)=\\ker (T-\\lambda I)\\text{,}$ it follows that $T-\\lambda I$ restricts to the zero operator on the eigenspace $E_\\lambda(T)\\text{.}$ The idea is to relax the condition “identically zero” to something that will allow us to potentially enlarge some of our eigenspaces, so that we end up with enough vectors to span $V\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-687\">It turns out that the correct replacement for “identically zero” is “nilpotent”. What we would like to find is some subspace $G_\\lambda(T)$ such that the restriction of $T-\\lambda I$ to $G_\\lambda(T)$ will be nilpotent. (Recall that this means $(T-\\lambda I)^k = 0$ for some integer $k$ when restricted to $G_\\lambda(T)\\text{.}$) The only problem is that we don't (yet) know what this subspace should be. To figure it out, we rely on some ideas you may have explored in your last assignment.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-nullspace-power\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">5.4.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-688\">Let $T:V\\to V$ be a linear operator. Then:<\/p><ol class=\"decimal\"><li id=\"li-130\">$\\displaystyle \\{\\zer\\}\\subseteq \\ker T \\subseteq \\ker T^2 \\subseteq \\cdots \\subseteq \\ker T^k\\subseteq \\cdots$<\/li><li id=\"li-131\"><p id=\"p-689\">If $\\ker T^{k+1}=\\ker T^k$ for some $k\\text{,}$ then $\\ker T^{k+m}=\\ker T^k$ for all $m\\geq 0\\text{.}$<\/p><\/li><li id=\"li-132\"><p id=\"p-690\">If $n=\\dim V\\text{,}$ then $\\ker T^{n+1} = \\ker T^n\\text{.}$<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-691\">In other words, for any operator $T\\text{,}$ the kernels of successive powers of $T$ can get bigger, but the moment the kernel doesn't change for the next highest power, it stops changing for all further powers of $T\\text{.}$ That is, we have a sequence of kernels of strictly greater dimension until we reach a maximum, at which point the kernels stop growing. And of course, the maximum dimension cannot be more than the dimension of $V\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-generalized-eigenspace\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">5.4.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-692\">Let $T:V\\to V$ be a linear operator, and let $\\lambda$ be an eigenvalue of $T\\text{.}$ The <dfn class=\"terminology\">generalized eigenspace<\/dfn> of $T$ associated to the eigenvalue $\\lambda$ is denoted $G_\\lambda(T)\\text{,}$ and defined as<\/p><div class=\"displaymath\">\n\\begin{equation*}\nG_\\lambda(T) = \\ker (T-\\lambda I)^n\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $n=\\dim V\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-693\">Some remarks are in order. First, we can actually define $G_\\lambda(T)$ for any scalar $\\lambda\\text{.}$ But this space will be trivial if $\\lambda$ is not an eigenvalue. Second, it is possible to show (although we will not do so here) that if $\\lambda$ is an eigenvalue with multiplicity $m\\text{,}$ then $G_\\lambda(T)=\\ker (T-\\lambda I)^m\\text{.}$ (The kernel will usually have stopped growing well before we hit $n=\\dim V\\text{,}$ but we know they're all eventually equal, so using $n$ guarantees we have everything).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-694\">We will not prove it here (see Nicholson, or Axler), but the advantage of using generalized eigenspaces is that they're just big enough to cover all of $V\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-gen-eigen-decomp\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">5.4.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-695\">Let $V$ be a complex vector space, and let $T:V\\to V$ be a linear operator. (We can take $V$ to be real if we assume that $T$ has all real eigenvalues.) Let $\\lambda_1,\\ldots, \\lambda_k$ be the distinct eigenvalues of $T\\text{.}$ Then each generalized eigenspace $G_{\\lambda_j}(T)$ is $T$-invariant, and we have the direct sum decomposition<\/p><div class=\"displaymath\">\n\\begin{equation*}\nV = G_{\\lambda_1}(T)\\oplus G_{\\lambda_2}(T)\\oplus \\cdots \\oplus G_{\\lambda_k}(T)\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-696\">For each eigenvalue $\\lambda_j$ of $T\\text{,}$ let $l_j$ denote the <em class=\"emphasis\">smallest<\/em> integer power such that $G_{\\lambda_j}(T) = (T-\\lambda_j I)^{l_j}\\text{.}$ Then certainly we have $l_j\\leq m_j$ for each $j\\text{.}$ (Note also that if $l_j=1\\text{,}$ then $G_{\\lambda_j}(T)=E_{\\lambda_j}(T)\\text{.}$)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-697\">The polynomial $m_T(x) = (x-\\lambda_1)^{l_1}(x-\\lambda_2)^{l_2}\\cdots (x-\\lambda_k)^{l_k}$ is the polynomial of <em class=\"emphasis\">smallest degree<\/em> such that $m_T(T)=0\\text{.}$ The polynomial $m_T(x)$ is called the <dfn class=\"terminology\">minimal polynomial<\/dfn> of $T\\text{.}$ Note that $T$ is diagonalizable if and only if the minimal polynomial of $T$ has no repeated roots.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-698\">In <a href=\"sec-jordan-form.ipynb\" class=\"internal\" title=\"Section 5.5: Jordan Canonical Form\">Section 5.5<\/a>, we'll explore a systematic method for determining the generalized eigenspaces of a matrix, and in particular, for computing a basis for each generalized eigenspace, with respect to which the corresponding block in the block-diagonal form is especially simple.<\/p><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-gen-eigen.ipynb"}
}