<?xml version="1.0" encoding="UTF-8" ?>



<pretext>
  <docinfo>
    <rename element="inlineexercise">Exercise</rename>
    <macros>
      \newcommand{\spn}{\operatorname{span}}
      \newcommand{\bbm}{\begin{bmatrix}}
      \newcommand{\ebm}{\end{bmatrix}}
      \newcommand{\R}{\mathbb{R}}
    </macros>
  </docinfo>
  <article xml:id="linalg_jupyter">
    <title>Computational Exercises for Math 3410</title>

    <frontmatter>

      <titlepage>
          <author>
              <personname>Sean Fitzpatrick</personname>
              <institution>University of Lethbridge</institution>
          </author>
          <date><today /></date>
      </titlepage>

      <abstract>
          <p>
            An introduction to computational tools for Math 3410 Linear Algebra,
            focused on the Python language.
          </p>
      </abstract>

    </frontmatter>

    <introduction>
      <p>
        Linear algebra is a mature, rich subject, full of both fascinating theory and useful applications.
        One of the things you might have taken away from a first course in the subject is that there's a lot of tedious calculation involved.
        This is true, if you're a human. But the algorithms you learn in a course like Math 1410 are easily implemented on a computer.
        If we want to be able to discuss any of the interesting applications of linear algebra,
        we're going to need to learn how to do linear algebra on a computer.
      </p>
      <p>
        There are many good mathematical software products that can deal with linear algebra,
        like Maple, Mathematica, and MatLab. But all of these are proprietary, and expensive.
        Sage is a popular open source system for mathematics,
        and students considering further studies in mathematics would do well to learn Sage.
        Since most people in Math 3410 are probably not considering a career as a mathematician,
        we'll try to do everything in Python.
      </p>
      <p>
        Python is a very poplular programming language, partly because of its ease of use.
        Those of you enrolled in Education may find yourself teaching Python to your students one day.
        Also, if you do want to use Sage, you're in luck: Sage is an amalamation of many different software tools,
        including Python. So any Python code you encouter in this course can also be run on Sage.
      </p>
    </introduction>

    <section xml:id="section-jupyter">
      <title>Jupyter</title>
      <p>
        The first thing you need to know about doing linear algebra in Python is how to access a Python environment.
        Fortunately, you do not need to install any software for this.
        The University of Lethbridge has access to the <term>Syzygy Jupyter Hub</term> service,
        provided by <init>PIMS</init> (the Pacific Institute for Mathematical Sciences), Cybera, and Compute Canada.
        To access Syzygy, go to <url href="https://uleth.syzygy.ca">uleth.syzygy.ca</url>
        and log in with your ULeth credentials.
      </p>

      <p>
        Note: if you click the login button and nothing happens, click the back button and try again.
        Sometimes there's a problem with our single sign-on service.
      </p>

      <p>
        The primary type of document you'll encounter on Syzygy is the <term>Jupyter notebook</term>.
        Content in a Juypter notebook is organized into <term>cells</term>.
        Some cells contain text, which can be in either <init>HTML</init> or <term>Markdown</term>.
        Markdown is a simple markup language. It's not as versatile as HTML, but it's easier to use.
        On Jupyter, markdown supports the LaTeX language for mathematical expressions.
        Use single dollar signs for inline math: <c>$\frac{d}{dx}\sin(x)=\cos(x)$</c>
        produces <m>\frac{d}{dx}\sin(x)=\cos(x)</m>, for example.
      </p>

      <p>
        If you want <q>display math</q>, use double dollar signs.
        Unfortunately, entering matrices is a bit tedious.
        For example, <c>$$A = \begin{bmatrix}1 &amp; 2 &amp; 3\\4 &amp; 5 &amp; 6 &amp;\end{bmatrix}$$</c>
        produces
        <me>
          A = \begin{bmatrix}1\amp 2\amp 3\\4\amp 5\amp 6\end{bmatrix}
        </me>.
        Later we'll see how to enter things like matrices in Python.
      </p>

      <p>
        It's also possible to use markdown to add <em>emphasis</em>, images, URLs, <etc />.
        For details, see the following <url href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet">Markdown cheatsheet</url>,
        or this <url href="https://callysto.ca/wp-content/uploads/2018/12/Callysto-Cheatsheet_12.19.18_web.pdf">quick reference</url>
        from <url href="https://callysto.ca/">callysto.ca</url>.
      </p>

      <p>
        What's cool about a Jupyter notebook is that in addition to markdown cells,
        which can present content and provide explanation,
        we can also include <em>code cells</em>. Jupyter supports many different programming languages,
        but we will stick mainly to Python.
      </p>
    </section>

    <section xml:id="sec-python-basics">
      <title>Python basics</title>
      <p>
        OK, so you've logged into Syzygy and you're ready to write some code.
        What does basic code look like in Python?
        The good news is that you don't need to be a programmer to do linear algebra in Python.
        Python includes many different <em>libraries</em> that keep most of the code under the hood,
        so all you have to remember is what command you need to use to accomplish a task.
        That said, it won't hurt to learn a little bit of basic coding.
      </p>

      <p>
        Basic arithmetic operations are understood, and you can simply type them in.
        Hit <c>shift+enter</c> in a code cell to execute the code and see the result.
      </p>

      <sage>
        <input>
          3+4
        </input>
      </sage>

      <sage>
        <input>
          3*4
        </input>
      </sage>

      <sage>
        <input>
          3**4
        </input>
      </sage>

        <p>
          OK, great. But sometimes we want to do calculations with more than one step.
          For that, we can assign variables.
        </p>

        <sage>
          <input>
            a = 14
            b = -9
            c = a+b
            print(a, b, c)
          </input>
        </sage>

        <p>
          Sometimes you might need input that's a string, rather than a number.
          We can do that, too.
        </p>

        <sage>
          <input>
            string_var = "Hey, look at my string!"
            print(string_var)
          </input>
        </sage>

        <p>
          Another basic construction is a list.
          Getting the hang of lists is useful, because in a sense,
          matrices are just really fancy lists.
        </p>

        <sage>
          <input>
            empty_list = list()
            this_too = []
            list_of_zeros = [0]*7
            print(list_of_zeros)
          </input>
        </sage>

        <p>
          Once you have an empty list, you might want to add something to it.
          This can be done with the <c>append</c> command.
        </p>

        <sage>
          <input>
            empty_list.append(3)
            print(empty_list)
            print(len(empty_list))
          </input>
        </sage>

        <p>
          Go back and re-run the above code cell two or three more times.
          What happens? Probably you can guess what the <c>len</c> command is for.
          Now let's get really carried away and do some <q>for real</q> coding, like loops!
        </p>

        <sage>
          <input>
            for i in range(10):
                empty_list.append(i)
            print(empty_list)
          </input>
        </sage>

        <p>
          Notice the indentation in the second line.
          This is how Python handles things like for loops, with indentation rather than bracketing.
          We could say more about lists but perhaps it's time to talk about matrices.
          For further reading, you can <url href="https://developers.google.com/edu/python/lists">start here</url>.
        </p>
    </section>

    <section xml:id="sec-sympy">
      <title>SymPy for linear algebra</title>
      <introduction>
        <p>
          <term>SymPy</term> is a Python library for symbolic algebra.
          On its own, it's not as powerful as programs like Maple,
          but it handles a lot of basic manipulations in a fairly simple fashion,
          and when we need more power, it can interface with other Python libraries.
        </p>

        <p>
          Another advantage of SymPy is sophisticated <q>pretty-printing</q>.
          In fact, we can enable MathJax within SymPy,
          so that output is rendered in the same way as when LaTeX is entered in a markdown cell.
        </p>
      </introduction>

      <subsection xml:id="subsec-sympy-basics">
        <title>SymPy basics</title>
        <p>
          Running the following Sage cell will load the SymPy library and turn on MathJax.
        </p>

        <sage>
          <input>
            from sympy import *
            init_printing()
          </input>
        </sage>

        <p>
          <alert>Note:</alert> if you are going to be working with multiple libraries,
          and more than one of them defines a certain command,
          instead of <c>from sympy import all</c> you can do <c>import sympy as sy</c>.
          If you do this, each SymPy command will need to be appended with <c>sy</c>; for example,
          you might write <c>sy.Matrix</c> instead of simply <c>Matrix</c>.
          Let's use SymPy to create a <m>2\times 3</m> matrix.
        </p>

        <sage>
          <input>
            A = Matrix(2,3,[1,2,3,4,5,6])
            A
          </input>
        </sage>

        <p>
          The <c>A</c> on the second line asks Python to print the matrix using SymPy's printing support.
          If we use Python's <c>print</c> command, we get something different:
        </p>

        <sage>
          <input>
            print(A)
          </input>
        </sage>

        <p>
          We'll have more on matrices in <xref ref="subsec-sympy-matrix"/>.
          For now, let's look at some more basic constructions.
          One basic thing to be mindful of is the type of numbers we're working with.
          For example, if we enter <c>2/7</c> in a code cell,
          Python will interpret this as a floating point number (essentially, a division).
        </p>
        <p>
          (If you are using Sage cells in HTML rather than Jupyter, this will automatically be interpreted as a fraction.)
        </p>

        <sage>
          <input>
            2/7
          </input>
        </sage>

        <p>
          But we often do linear algebra over the rational numbers,
          and so SymPy will let you specify this:
        </p>

        <sage>
          <input>
            Rational(2,7)
          </input>
        </sage>

        <p>
          You might not think to add the comma above, because you're used to writing fractions like <m>2/7</m>.
          Fortunately, the SymPy authors thought of that:
        </p>

        <sage>
          <input>
            Rational(2/7)
          </input>
        </sage>

        <p>
          Hmm... You might have got the output you expected in the cell above, but maybe not.
          If you got a much worse looking fraction, read on.
        </p>

        <p>
          Another cool command is the <c>sympify</c> command, which can be called with the shortcut <c>S</c>.
          The input <c>2</c> is interpreted as an <c>int</c> by Python, but <c>S(2)</c> is a <q>SymPy <c>Integer</c></q>:
        </p>

        <sage>
          <input>
            S(2)/7
          </input>
        </sage>

        <p>
          Of course, sometimes you <em>do</em> want to use floating point, and you can specify this, too:
        </p>

        <sage>
          <input>
            2.5
          </input>
        </sage>

        <sage>
          <input>
            Float(2.5)
          </input>
        </sage>

        <sage>
          <input>
            Float(2.5e10)
          </input>
        </sage>

        <p>
          One note of caution: <c>Float</c> is part of SymPy, and not the same as the core Python <c>float</c> command.
          You can also put decimals into the Rational command and get the corresponding fraction:
        </p>

        <sage>
          <input>
            Rational(0.75)
          </input>
        </sage>

        <p>
          The only thing to beware of is that computers convert from decimal to binary and then back again,
          and sometimes weird things can happen:
        </p>

        <sage>
          <input>
            Rational(0.2)
          </input>
        </sage>

        <p>
          Of course, there are workarounds. One way is to enter <m>0.2</m> as a string:
        </p>

        <sage>
          <input>
            Rational('0.2')
          </input>
        </sage>

        <p>
          Another is to limit the size of the denominator:
        </p>

        <sage>
          <input>
            Rational(0.2).limit_denominator(10**12)
          </input>
        </sage>

        <p>
          Try some other examples above. Some inputs to try are <c>1.23</c> and <c>23e-10</c>
        </p>

        <p>
          We can also deal with repeating decimals. These are entered as strings, with square brackets around the repeating part.
          Then we can <q>sympify</q>:
        </p>

        <sage>
          <input>
            S('0.[1]')
          </input>
        </sage>

        <p>
          Finally, SymPy knows about mathematical constants like <m>e, \pi, i</m>,
          which you'll need from time to time in linear algebra.
          If you ever need <m>\infty</m>, this is entered as <c>oo</c>.
        </p>

        <sage>
          <input>
            I*I
          </input>
        </sage>
        <sage>
          <input>
            I-sqrt(-1)
          </input>
        </sage>
        <sage>
          <input>
            pi.is_irrational
          </input>
        </sage>
      </subsection>
      <subsection xml:id="subsec-sympy-matrix">
        <title>Matrices in SymPy</title>
        <p>
          (more to come)
        </p>
      </subsection>
    </section>

    <section xml:id="sec-span">
      <title>Some calculations involving span</title>
      <p>
        Recall that a <term>linear combination</term> of a set of vectors
        <m>\vec{v}_1,\ldots, \vec{v}_k</m> is a vector expression of the form
        <me>
          \vec{w}=c_1\vec{v}_1+c_2\vec{v}_2+\cdots +c_k\vec{v}_k,
        </me>
        where <m>c_1,\ldots, c_k</m> are scalars.
      </p>

      <p>
        The <term>span</term> of those same vectors is the set of all possible linear combinations:
        <me>
          \spn\{\vec{v}_1,\ldots, \vec{v}_k\} = \{c_1\vec{v}_1+ \cdots + c_k\vec{v}_k \,|\, c_1,\ldots, c_k \in \mathbb{F}\}.
        </me>
        Therefore, the questions <q>Is the vector <m>\vec{w}</m> in <m>\spn\{\vec{v}_1,\ldots, \vec{v}_k\}</m>?</q>
        is really asking, <q>Can <m>\vec{w}</m> be written as a linear combination of <m>\vec{v}_1,\ldots, \vec{v}_k</m>?</q>
      </p>

      <p>
        With the appropriate setup, all such questions become questions about solving systems of equations.
        Here, we will look at a few such examples.
      </p>

      <exercise>
        <statement>
          <p>
            Determine whether the vector <m>\bbm 2\\3\ebm</m> is in the span of the vectors <m>\bbm 1\\1\ebm,\bbm -1\\2\ebm</m>.
          </p>
        </statement>
      </exercise>

      <p>
        This is really asking: are there scalars <m>s,t</m> such that
        <me>
          s\bbm 1\\1\ebm + t\bbm -1\\2\ebm = \bbm 2\\3\ebm
        </me>?
        And this, in turn, is equivalent to the system
        <md>
          <mrow>s -t \amp=2 </mrow>
          <mrow>s+2t \amp=3 </mrow>
        </md>,
        which is the same as the matrix equation
        <me>
          \bbm 1\amp -1\\1\amp 2\ebm\bbm s\\t\ebm = \bbm 2\\3\ebm.
        </me>
        Solving the system confirms that there is indeed a solution, so the answer to our original question is yes.
      </p>

      <p>
        To confirm the above example (and see what the solution is), we can use the computer.
      </p>
      <sage>
        <input>
          from sympy import *
          init_printing()
        </input>
      </sage>

      <sage>
        <input>
          A = Matrix(2,3,[1,-1,2,1,2,3])
          A.rref()
        </input>
      </sage>

      <p>
        The above code produces the reduced row-echelon form of the augmented matrix for our system.
        Do you remember how to get the answer from here? Here's another approach.
      </p>

      <sage>
        <input>
          B = Matrix(2,2,[1,-1,1,2])
          B
        </input>
      </sage>

      <sage>
        <input>
          C = Matrix(2,1,[2,3])
          X = (B**-1)*C
          X
        </input>
      </sage>

      <p>
        Our next example involves polynomials.
        At first this looks like a different problem,
        but it's essentially the same once we set it up.
      </p>

      <exercise>
        <statement>
          <p>
            Determine whether <m>p(x)=1+x+4x^2</m> belongs to
            <m>\spn\{1+2x-x^2,3+5x+2x^2\}</m>.
          </p>
        </statement>
      </exercise>

      <p>
        We seek scalars <m>s,t</m> such that
        <me>
          s(1+2x-2x^2)+t(3+5x+2x^2)=1+x+4x^2
        </me>.
        On the left-hand side, we expand and gather terms:
        <me>
          (s+3t)+(2s+5t)x+(-2s+2t)x^2 = 1+x+4x^2
        </me>.
        These two polynomials are equal if and only if we can solve the system
        <md>
          <mrow>s+3t \amp = 1 </mrow>
          <mrow>2s+5t \amp =1</mrow>
          <mrow> -2s+2t \amp =4</mrow>
        </md>.
      </p>

      <p>
        We can solve this computationally using matrices again.
      </p>
      <sage>
        <input>
          M = Matrix(3,3,[1,3,1,2,5,1,-2,2,4])
          M.rref()
        </input>
      </sage>
      <p>
        So, what's the answer? Is <m>p(x)</m> in the span?
        Can we determine what polynomials are in the span?
        Let's consider a general polynomial <m>q(x)=a+bx+cx^2</m>.
        A bit of thought tells us that the coefficients <m>a,b,c</m>
        should replace the constants <m>1,1,4</m> above.
      </p>

      <sage>
        <input>
          a, b, c = symbols('a b c', real = True, constant = True)
          N = Matrix(3,3,[1,3,a,2,5,b,-2,2,c])
          N
        </input>
      </sage>

      <p>
        Asking the computer to reduce this matrix won't produce the desired result.
        (Maybe someone can figure out how to define the constants in a way that works?)
        But we can always specify row operations.
      </p>

      <sage>
        <input>
          N1 = N.elementary_row_operation(op='n->n+km',row1=1,row2=0,k=-2)
          N1
        </input>
      </sage>

      <p>
        Now we repeat. Here are two more empty cells to work with:
      </p>

      <sage>

      </sage>
      <sage>

      </sage>

      <p>
        We end this section with a few non-computational, but useful results,
        which will be left as exercises to be done in class, or by the reader.
      </p>

      <exercise>
        <statement>
          <p>
            Let <m>V</m> be a vector space, and let <m>X,Y\subseteq V</m>.
            Show that if <m>X\subseteq Y</m>, then <m>\spn X \subseteq \spn Y</m>.
          </p>
        </statement>
      </exercise>

      <exercise>
        <statement>
          <p>
            Can <m>\{(1,2,0), (1,1,1)</m> span <m>\{(a,b,0)\,|\, a,b \in\R\}</m>?
          </p>
        </statement>
      </exercise>

      <theorem xml:id="theorem-surplus-span">
        <statement>
          <p>
            Let <m>V</m> be a vector space, and let <m>\vec{v}_1,\ldots, \vec{v}_k\in V</m>.
            If <m>\vec{u}\in \spn\{\vec{v}_1,\ldots, \vec{v}_k\}</m>, then
            <me>
              \spn\{\vec{u},\vec{v}_1,\ldots, \vec{v}_k\} = \spn\{\vec{v}_1,\ldots, \vec{v}_k\}
            </me>.
          </p>
        </statement>
      </theorem>

      <p>
        The moral of <xref ref="theorem-surplus-span">Theorem</xref>
        is that one vector in a set is a linear combination of the others,
        we can remove it from the set without affecting the span.
        This suggests that we might want to look for the most <q>efficient</q>
        spanning sets <ndash /> those in which no vector in the set can be written in terms of the others.
        Such sets are called <term>linearly independent</term>,
        and they are the subject of <xref ref="sec-independence">Section</xref>.
      </p>
    </section>

    <section xml:id="sec-independence">
      <title>Linear Independence</title>

      <p>
        In any vector space <m>V</m>, we say that a set of vectors
        <me>
          \{\vec{v}_1,\ldots,\vec{v}_2\}
        </me>
        is <term>linearly independent</term> if for any scalars <m>c_1,\ldots, c_k</m>
        <me>
          c_1\vec{v}_1+\cdots + c_k\vec{v}_k = \vec{0} \quad\Rightarrow\quad c_1=\cdots = c_k=0
        </me>.
      </p>

      <p>
        This means that no vector in the set can be written as a linear combination of the other vectors in that set.
        We will soon see that when looking for vectors that span a subspace,
        it is especially useful to find a spanning set that is also linearly independent.
        The following lemma establishes some basic properties of independent sets.
      </p>

      <lemma>
        <statement>
          <p>
            In any vector space <m>V</m>:
            <ol>
              <li>
                <p>
                  If <m>\vec{v}\neq\vec{0}</m>, then <m>\{\vec{v}\}</m> is indenpendent.
                </p>
              </li>
              <li>
                <p>
                  If <m>S\subseteq V</m> contains the zero vector, then <m>S</m> is dependent.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </lemma>

      <p>
        The definition of linear independence tells us that if <m>\{\vec{v}_1,\ldots, \vec{v}_k\}</m>
        is an independent set of vectors, then there is only one way to write <m>\vec{0}</m>
        as a linear combination of these vectors; namely,
        <me>
          \vec{0} = 0\vec{v}_1+0\vec{v}_2+\cdots +0\vec{v_k}
        </me>.
        In fact, more is true: <em>any</em> vector in the span of a linearly independent set
        can be written in only one way as a linear combination of those vectors.
      </p>

      <p>
        Computationally, questions about linear independence are just questions
        about homogeneous systems of linear equations.
        For example, suppose we want to know if the vectors
        <me>
          \vec{u}=\bbm 1\\-1\\4\ebm, \vec{v}=\bbm 0\\2\\-3\ebm, \vec{w}=\bbm 4\\0\\-3\ebm
        </me>
        are linearly independent in <m>\mathbb{R}^3</m>.
        This question leads to the vector equation
        <me>
          x\vec{u}+y\vec{v}+z\vec{w}=\vec{0}
        </me>,
        which becomes the matrix equation
        <me>
          \bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm\bbm x\\y\\z\ebm = \bbm 0\\0\\0\ebm
        </me>.
      </p>

      <p>
        We now apply some basic theory from linear algebra.
        A unique (and therefore, trivial) solution to this system is guaranteed if the matrix
        <m>A = \bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm</m> is invertible,
        since in that case we have <m>\bbm x\\y\\z\ebm = A^{-1}\vec{0} = \vec{0}</m>.
      </p>

      <p>
        This approach is problematic, however, since it won't work if we have 2 vectors, or 4.
        Instead, we look at the reduced row-echelon form.
        A unique solution corresponds to having a leading 1 in each column of <m>A</m>.
        Let's check this condition.
      </p>

      <sage>
        <input>
          from sympy import *
          init_printing()
        </input>
      </sage>

      <sage>
        <input>
          A = Matrix(3,3,[1,0,4,-1,2,0,4,-3,-3])
          A.rref()
        </input>
      </sage>

      <p>
        One observation is useful here, and will lead to a better understanding of independence.
        First, it would be impossible to have 4 or more linearly independent vectors in <m>\mathbb{R}^3</m>.
        Why? (How many leading ones can you have in a <m>3\times 4</m> matrix?)
        Second, having two or fewer vectors makes it more likely that the set is independent.
      </p>

      <p>
        The largest set of linearly independent vectors possible in <m>\mathbb{R}^3</m> contains three vectors.
        You might have also observed that the smallest number of vectors needed to span <m>\mathbb{R}^3</m> is 3.
        Hmm. Seems like there's something interesting going on here. But first, some more computation.
      </p>

      <exercise>
        <statement>
          <p>
            Determine whether the set <m>\left\{\bbm 1\\2\\0\ebm, \bbm -1\\0\\3\ebm,\bbm -1\\4\\9\ebm\right\}</m>
            is linearly independent in <m>\R^3</m>.
          </p>
        </statement>
      </exercise>

      <p>
        Again, we set up a matrix and reduce:
      </p>

      <sage>
        <input>
          A = Matrix(3,3,[1,-1,-1,2,0,4,0,3,9])
          A.rref()
        </input>
      </sage>

      <p>
        Notice that this time we don't get a unique solution, so we can conclude that these vectors are <em>not</em> independent.
        Furthermore, you can probably deduce from the above that we have <m>2\vec{v}_1+3\vec{v}_2-\vec{v}_3=\vec{0}</m>.
        Now suppose that <m>\vec{w}\in\spn\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}</m>.
        In how many ways can we write <m>\vec{w}</m> as a linear combination of these vectors?
      </p>

      <exercise>
        <statement>
          <p>
            Which of the following subsets of <m>P_2(\mathbb{R})</m> are independent?
            <md>
              <mrow>\text{(a) } S_1 = \{x^2+1, x+1, x\}</mrow>
              <mrow>\text{(b) } S_2 = \{x^2-x+3, 2x^2+x+5, x^2+5x+1\}</mrow>
            </md>
          </p>
        </statement>
      </exercise>

      <p>
        In each case, we set up the defining equation for independence, collect terms,
        and then analyze the resulting system of equations.
        (If you work with polynomials often enough,
        you can probably jump straight to the matrix.
        For now, let's work out the details.)
      </p>

      <p>Suppose
        <me>
          r(x^2+1)+s(x+1)+tx = 0
        </me>.
        Then <m>rx^2+(s+t)x+(r+s)=0=0x^2+0x+0</m>, so
        <md>
          <mrow>r \amp =0</mrow>
          <mrow>s+t \amp =0</mrow>
          <mrow>r+s\amp =0</mrow>
        </md>.
        And in this case, we don't even need to ask the computer.
        The first equation gives <m>r=0</m> right away,
        and putting that into the third equation gives <m>s=0</m>,
        and the second equation then gives <m>t=0</m>.
      </p>

      <p>
        Since <m>r=s=t=0</m> is the only solution, the set is independent.
      </p>

      <p>
        Repeating for <m>S_2</m> leads to the equation
        <me>
          (r+2s+t)x^2+(-r+s+5t)x+(3r+5s+t)1=0.
        </me>
        This gives us:
      </p>

      <sage>
        <input>
          A = Matrix(3,3,[1,2,1,-1,1,5,3,5,1])
          A.rref()
        </input>
      </sage>

      <exercise>
        <statement>
          <p>
            Determine whether or not the set
            <me>
              \left\{\bbm -1\amp 0\\0\amp -1\ebm, \bbm 1\amp -1\\ -1\amp 1\ebm,
                     \bbm 1\amp 1\\1\amp 1\ebm, \bbm 0\amp -1\\-1\amp 0\ebm\right\}
            </me>
            is linearly independent in <m>M_2(\mathbb{R})</m>.
          </p>
        </statement>
      </exercise>

      <p>
        Again, we set a linear combination equal to the zero vector, and combine:
        <md>
          <mrow>a\bbm -1\amp 0\\0\amp -1\ebm +b\bbm 1\amp -1\\ -1\amp 1\ebm
            +c\bbm 1\amp 1\\1\amp 1\ebm +d \bbm 0\amp -1\\-1\amp 0\ebm = \bbm 0\amp 0\\ 0\amp 0\ebm</mrow>
          <mrow>\bbm -a+b+c\amp -b+c-d\\-b+c-d\amp -a+b+c\ebm = \bbm 0\amp 0\\0\amp 0\ebm</mrow>
        </md>.
      </p>

      <p>
        We could proceed, but we might instead notice right away that equations 1 and 4 are identical,
        and so are equations 2 and 3.
        With only two distinct equations and 4 unknowns, we're certain to find nontrivial solutions.
      </p>
    </section>

    <section xml:id="sec-dimension">
      <title>Basis and dimension</title>

      <p>
        Next, we begin with an important result, sometimes known as the <q>Fundamental Theorem</q>:
      </p>

      <theorem xml:id="theorem-steinitz">
        <title>Fundamental Theorem (Steinitz Exchange Lemma)</title>
        <statement>
          <p>
            Suppose <m>V = \spn\{\vec{v}_1,\ldots, \vec{v}_n\}</m>.
            If <m>\{\vec{w}_1,\ldots, \vec{w}_m\}</m> is a linearly independent set of vectors in <m>V</m>,
            then <m>m\leq n</m>.
          </p>
        </statement>
      </theorem>

      <p>
        If a set of vectors spans a vector space <m>V</m>, and it is not independent,
        we observed that it is possible to remove a vector from the set and still span <m>V</m>.
        This suggests that spanning sets that are also linearly independent are of particular importance,
        and indeed, they are important enough to have a name.
      </p>

      <definition xml:id="def-basis">
        <statement>
          <p>
            Let <m>V</m> be a vector space. A set <m>\mathcal{B}=\{\vec{e}_1,\ldots, \vec{e}_n\}</m>
            is called a <term>basis</term> of <m>V</m> if <m>\mathcal{B}</m> is linearly independent,
            and <m>\operatorname{span}\mathcal{B} = V</m>.
          </p>
        </statement>
      </definition>

      <p>
        The importance of a basis is that vector vector <m>\vec{v}\in V</m> can be written in terms of the basis,
        and this expression as a linear combination of basis vectors is <em>unique</em>.
        Another important fact is that every basis has the same number of elements.
      </p>

      <theorem xml:id="thm-invariance">
        <title>Invariance Theorem</title>

        <statement>
          <p>
            If <m>\{\vec{e}_1,\ldots, \vec{e}_n\}</m> and <m>\{\vec{f}_1,\ldots, \vec{f}_m\}</m>
            are both bases of a vector space <m>V</m>, then <m>m=n</m>.
          </p>
        </statement>
      </theorem>

      <p>
        Suppose <m>V=\spn\{\vec{v}_1,\ldots,\vec{v}_n\}</m>.
        If this set is not linearly independent, <xref ref="theorem-surplus-span">Theorem</xref>
        tells us that we can remove a vector from the set, and still span <m>V</m>.
        We can repeat this procedure until we have a linearly independent set of vectors, which will then be a basis.
        These results let us make a definition.
      </p>

      <definition xml:id="def-dimension">
        <statement>
          <p>
            Let <m>V</m> be a vector space.
            If <m>V</m> can be spanned by a finite number of vectors,
            then we call <m>V</m> a <term>finite-dimensional</term> vector space.
            If <m>V</m> is finite-dimensional (and non-trivial), and <m>\{\vec{e}_1,\ldots, \vec{e}_n\}</m>
            is a basis of <m>V</m>, we say that <m>V</m> has <term>dimension</term> <m>n</m>,
            and write
            <me>
              \dim V = n
            </me>.
            If <m>V</m> cannot be spanned by finitely many vectors, we say that <m>V</m>
            is <term>infinite-dimensional</term>.
          </p>
        </statement>
      </definition>

      <exercise>
        <statement>
          <p>
            Find a basis for <m>U=\{X\in M_{22} \,|\, XA = AX\}</m>, if <m>A = \bbm 1\amp 1\\0\amp 0\ebm</m>
          </p>
        </statement>
      </exercise>

      <exercise>
        <statement>
          <p>
            Show that the following are bases of <m>\R^3</m>:
            <ul>
              <li><m>\{(1,1,0),(1,0,1),(0,1,1)\}</m></li>
              <li><m>\{(-1,1,1),(1,-1,1),(1,1,,-1)</m></li>
            </ul>
          </p>
        </statement>
      </exercise>

      <sage>
        <input>
          from sympy import *
          init_printing()
        </input>
      </sage>

      <sage>

      </sage>

      <exercise>
        <statement>
          <p>
            Show that the following is a basis of <m>M_{22}</m>:
            <me>
              \left\{\bbm 1\amp 0\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 1\\0\amp 1\ebm, \bbm 1\amp 0\\0\amp 0\ebm\right\}
            </me>.
          </p>
        </statement>
      </exercise>

      <sage>

      </sage>

      <exercise>
        <statement>
          <p>
            Show that <m>\{1+x,x+x^2,x^2+x^3,x^3\}</m> is a basis for <m>P_3</m>.
          </p>
        </statement>
      </exercise>

      <sage>

      </sage>

      <exercise>
        <statement>
          <p>
            Find a basis and dimension for the following subpaces of <m>P_2</m>:
            <ol label='a'>
              <li>
                <m>U_1 = \{a(1+x)+b(x+x^2)\,|\, a,b\in\R\}</m>
              </li>
              <li>
                <m>U_2=\{p(x)\in P_2 \,|\, p(1)=0\}</m>
              </li>
              <li>
                <m>U_3 = \{p(x)\in P_2 \,|\, p(x)=p(-x)\}</m>
              </li>
            </ol>
          </p>
        </statement>
        <solution>
          <p>
            <ol label='a'>
              <li>
                <p>
                  By definition, <m>U_1 = \spn \{1+x,x+x^2\}</m>,
                  and these vectors are independent, since neither is a scalar multiple of the other.
                  Since there are two vectors in this basis, <m>\dim U_1 = 2</m>.
                </p>
              </li>
              <li>
                <p>
                  If <m>p(1)=0</m>, then <m>p(x)=(x-1)q(x)</m> for some polynomial <m>q</m>.
                  Since <m>U_2</m> is a subspace of <m>P_2</m>, the degree of <m>q</m> is at most 2.
                  Therefore, <m>q(x)=ax+b</m> for some <m>a,b\in\R</m>, and
                  <me>
                    p(x) = (x-1)(ax+b) = a(x^2-x)+b(x-1)
                  </me>.
                  Since <m>p</m> was arbitrary, this shows that <m>U_2 = \spn\{x^2-x,x-1\}</m>.
                </p>

                <p>
                  The set <m>\{x^2-x,x-1\}</m> is also independent,
                  since neither vector is a scalar multiple of the other.
                  Therefore, this set is a basis, and <m>\dim U_2=2</m>.
                </p>
              </li>
              <li>
                <p>
                  If <m>p(x)=p(-x)</m>, then <m>p(x)</m> is an even polynomial,
                  and therefore <m>p(x)=a+bx^2</m> for <m>a,b\in\R</m>.
                  (If you didn't know this it's easily verified: if
                  <me>
                    a+bx+cx^2 = a+b(-x)+c(-x)^2
                  </me>,
                  we can immediately cancel <m>a</m> from each side,
                  and since <m>(-x)^2=x^2</m>, we can cancel <m>cx^2</m> as well.
                  This leaves <m>bx=-bx</m>, or <m>2bx=0</m>, which implies that <m>b=0</m>.)
                </p>

                <p>
                  It follows that the set <m>\{1,x^2\}</m> spans <m>U_3</m>,
                  and since this is a subset of the standard basis <m>\{1,x,x^2\}</m> of <m>P_2</m>,
                  it must be independent, and is therefore a basis of <m>U_3</m>,
                  letting us conclude that <m>\dim U_3=2</m>.
                </p>
              </li>
            </ol>
          </p>
        </solution>
      </exercise>

      <p>
        We've noted a few times now that if <m>\vec{w}\in\spn\{\vec{v}_1,\ldots, \vec{v}_n\}</m>,
        then
        <me>
          \spn\{\vec{w},\vec{v}_1,\ldots, \vec{v}_n\}=\spn\{\vec{v}_1,\ldots, \vec{v}_n\}
        </me>
        If <m>\vec{w}</m> is not in the span, we can make another useful observation:
      </p>

      <lemma xml:id="lemma-independent">
        <title>Independent Lemma</title>
        <statement>
          <p>
            Suppose <m>\{\vec{v}_1,\ldots, \vec{v}_n\}</m> is a linearly independent set of vectors in a vector space <m>V</m>.
            If <m>\vec{u}\in V</m> but <m>\vec{u}\notin \spn\{\vec{v}_1,\ldots, \vec{v}_n\}</m>,
            then <m>\{\vec{u},\vec{v}_1,\ldots, \vec{v}_n\}</m> is independent.
          </p>
        </statement>
        <proof>
          <p>
            Suppose <m>S=\{\vec{v}_1,\ldots, \vec{v}_n\}</m> is independent,
            and that <m>\vec{u}\notin\spn S</m>. Suppose we have
            <me>
              a\vec{u}+c_1\vec{v}_1+c_2\vec{v}_2+\cdots +c_n\vec{b}_n=\vec{0}
            </me>
            for scalars <m>a,c_1,\ldots, c_n</m>. We must have <m>a=0</m>;
            otherwise, we can multiply by <m>\frac1a</m> and rearrange to obtain
            <me>
              \vec{u} = -\frac{c_1}{a}\vec{v}_1-\cdots -\frac{c_n}{a}\vec{v}_n
            </me>,
            but this would mean that <m>\vec{u}\in \spn S</m>, contradicting our assumption.
          </p>

          <p>
            With <m>a=0</m> we're left with
            <me>
              c_1\vec{v}_1+c_2\vec{v}_2+\cdots +c_n\vec{b}_n=\vec{0}
            </me>,
            and since we assumed that the set <m>S</m> is independent,
            we must have <m>c_1=c_2=\cdots=c_n=0</m>. Since we already showed <m>a=0</m>,
            this shows that <m>\{\vec{u},\vec{v}_1,\ldots, \vec{v}_n\}</m> is independent.
          </p>
        </proof>

      </lemma>
      <p>
        This is, in fact, an <q>if and only if</q> result.
        If <m>\vec{u}\in\spn\{\vec{v}_1,\ldots, \vec{v}_n\}</m>, then <m>\{\vec{u},\vec{v}_1,\ldots, \vec{v}_n\}</m> is not independent.
        Above, we argued that if <m>V</m> is finite dimensional,
        then any spanning set for <m>V</m> can be reduced to a basis.
        It probably won't surprise you that the following is also true.
      </p>

      <lemma xml:id="lem-enlarge-independent">
        <statement>
          <p>
            Let <m>V</m> be a finite-dimensional vector space,
            and let <m>U</m> be any subspace of <m>V</m>.
            Then any independent set of vectors <m>\{\vec{u}_1,\ldots, \vec{u}_k\}</m> in <m>U</m>
            can be enlarged to a basis of <m>U</m>.
          </p>
        </statement>
        <proof>
          <p>
            This follows from <xref ref="lemma-independent"/>.
            If our independent set of vectors spans <m>U</m>, then it's a basis and we're done.
            If not, we can find some vector not in the span,
            and add it to our set to obtain a larger set that is still independent.
            We can continue adding vectors in this fashion until we obtain a spanning set.
          </p>

          <p>
            Note that this process <em>must</em> terminate: <m>V</m> is finite-dimensional,
            so there is a finite spanning set for <m>V</m>, and therefore for <m>U</m>.
            By the Steinitz Exchange lemma, our independent set cannot get larger than this spanning set.
          </p>
        </proof>

      </lemma>

      <theorem xml:id="thm-basis-exist">
        <statement>
          <p>
            Any finite-dimensional vector space <m>V</m> has a basis.
            Moreover:
            <ol>
              <li>
                <p>
                  If <m>V</m> can be spanned by <m>m</m> vectors,
                  then <m>\dim V\leq m</m>.
                </p>
              </li>
              <li>
                <p>
                  Given an independent set <m>I</m> in <m>V</m>,
                  and a basis <m>\mathcal{B}</m> of <m>V</m>,
                  we can enlarge <m>I</m> to a basis of <m>V</m> by adding elements of <m>\mathcal{B}</m>.
                </p>
              </li>
            </ol>
          </p>

          <p>
            If <m>U</m> is a subspace of <m>V</m>, then:
            <ol>
              <li>
                <p>
                  <m>U</m> is finite-dimensional, and <m>\dim U\leq \dim V</m>.
                </p>
              </li>
              <li>
                <p>
                  If <m>\dim U = \dim V</m>, then <m>U=V</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>

      <exercise>
        <statement>
          <p>
            Find a basis of <m>M_{22}(\R)</m> that contains the vectors
            <me>
              \vec{v}=\bbm 1\amp 1\\0\amp 0\ebm, \vec{w}=\bbm 0\amp 1\\0\amp 1\ebm
            </me>.
          </p>
        </statement>
        <solution>
          <p>
            By the previous theorem, we can form a basis by adding vectors from the standard basis
            <me>
              \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}
            </me>.
            It's easy to check that <m>\bbm 1\amp 0\\0\amp 0\ebm</m> is not in the span of <m>\{\vec{v},\vec{w}\}</m>.
            To get a basis, we need one more vector.
            Observe that all three of our vectors so far have a zero in the <m>(2,1)</m>-entry.
            Thus, <m>\bbm 0\amp 0\\1\amp 0\ebm</m> cannot be in the span of the first three vectors,
            and adding it gives us our basis.
          </p>
        </solution>
      </exercise>

      <exercise>
        <statement>
          <p>
            Extend the set <m>\{1+x,x+x^2,x-x^3\}</m> to a basis of <m>P_3(\R)</m>.
          </p>
        </statement>
        <solution>
          <p>
            Again, we only need to add one vector from the standard basis
            <m>\{1,x,x^2,x^3\}</m>, and it's not too hard to check that any of them will do.
          </p>
        </solution>
      </exercise>

      <exercise>
        <statement>
          <p>
            Give two examples of infinite-dimensional vector spaces.
            Support your answer.
          </p>
        </statement>
      </exercise>

      <p>
        Let's recap our results so far:
        <ul>
          <li>
            <p>
              A basis for a vector space <m>V</m> is an independent set of vectors that spans <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              The number of vectors in any basis of <m>V</m> is a constant, called the dimension of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              The number of vectors in any independent set is always less than or equal to the number of vectors in a spanning set.
            </p>
          </li>
          <li>
            <p>
              In a finite-dimensional vector space, any independent set can be enlarged to a basis,
              and any spanning set can be cut down to a basis by deleting vectors that are in the span of the remaining vectors.
            </p>
          </li>
        </ul>
        Another important aspect of dimension is that it reduces many problems,
        such as determining equality of subspaces, to counting problems.
      </p>

      <theorem xml:id="thm-subspace-dim">
        <statement>
          <p>
            Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
            <ol>
              <li>
                <p>
                  If <m>U\subseteq W</m>, then <m>\dim U\leq \dim W</m>.
                </p>
              </li>
              <li>
                <p>
                  If <m>U\subseteq W</m> and <m>\dim U=\dim W</m>, then <m>U=W</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
        <proof>
          <p>
            <ol>
              <li>
                <p>
                  Suppose <m>U\subseteq W</m>, and let <m>B=\{\vec{u}_1,\ldots, \vec{u}_k\}</m>
                  be a basis for <m>U</m>. Since <m>B</m> is a basis, it's independent.
                  And since <m>B\subseteq U</m> and <m>U\subseteq W</m>, <m>B\subseteq W</m>.
                  Thus, <m>B</m> is an independent subset of <m>W</m>,
                  and since any basis of <m>W</m> spans <m>W</m>,
                  we know that <m>\dim U = k \leq \dim W</m>,
                  by <xref ref="theorem-steinitz">Theorem</xref>.
                </p>
              </li>
              <li>
                <p>
                  Suppose <m>U\subseteq W</m> and <m>\dim U = \dim W</m>.
                  Let <m>B</m> be a basis for <m>U</m>.
                  As above, <m>B</m> is an independent subset of <m>W</m>.
                  If <m>W\neq U</m>, then there is some <m>\vec{w}\in W</m> with <m>\vec{w}\notin U</m>.
                  But <m>U=\spn B</m>, so that would mean that <m>B\cup \{\vec{w}\}</m>
                  is a linearly independent set containing <m>\dim U+1</m> vectors.
                  This is impossible, since <m>\dim W=\dim U</m>,
                  so no independent set can contain more than <m>\dim U</m> vectors.
                </p>
              </li>
            </ol>
          </p>
        </proof>

      </theorem>

      <p>
        An even more useful counting result is the following:
      </p>

      <theorem xml:id="thm-half-the-work">
        <statement>
          <p>
            Let <m>V</m> be an <m>n</m>-dimensional vector space.
            If the set <m>S</m> contains <m>n</m> vectors,
            then <m>S</m> is independent if and only if <m>\spn S=V</m>.
          </p>
        </statement>
        <proof>
          <p>
            If <m>S</m> is independent, then it can be extended to a basis <m>B</m> with <m>S\subseteq B</m>.
            But <m>S</m> and <m>B</m> both contain <m>n</m> vectors (since <m>\dim V=n</m>),
            so we must have <m>S=B</m>.
          </p>

          <p>
            If <m>S</m> spans <m>V</m>, then <m>S</m> must contain a basis <m>B</m>,
            and as above, since <m>S</m> and <m>B</m> contain the same number of vectors,
            they must be equal.
          </p>
        </proof>

      </theorem>

      <paragraphs xml:id="pars-subspace-combine">
        <title>New subspaces from old</title>
        <p>
          On your first assignment, you showed that if <m>U</m> and <m>W</m> are subspaces of a vector space <m>V</m>,
          then the intersection <m>U\cap W</m> is also a subspace of <m>V</m>.
          You also showed that the union <m>U\cup W</m> is generally not a subspace,
          unless one subspace is contained in the other
          (in which case the union is just the larger of the two subspaces we already have).
        </p>

        <p>
          In class, we discussed the fact that the right way to define a subspace containing both <m>U</m> and <m>W</m>
          is using their <term>sum</term>: we define the sum <m>U+W</m> of two subspaces by
          <me>
            U+W = \{\vec{u}+\vec{w} \,|\, \vec{u}\in U \text{ and } \vec{w}\in W\}
          </me>.
          We proved that <m>U+W</m> is again a subspace of <m>V</m>.
        </p>

        <p>
          If <m>U\cap W = \{\vec{0}\}</m>, we say that the sum is a <term>direct sum</term>,
          and write it as <m>U\oplus W</m>.
          The following theorem might help us understand why direct sums are singled out for special attention:
        </p>
        <theorem xml:id="thm-sum-dimension">
          <statement>
            <p>
              Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
              Then <m>U+W</m> is finite-dimensional, and
              <me>
                \dim(U+W)=\dim U + \dim W - \dim(U\cap W)
              </me>.
            </p>
          </statement>

        </theorem>

        <p>
          If the sum is direct, then we have simply <m>\dim(U\oplus W) = \dim U + \dim W</m>.
          The other reason why direct sums are preferable, is that any <m>\vec{v}\in U\oplus W</m>
          can be written <em>uniquely</em> as <m>\vec{v}=\vec{u}+\vec{w}</m>
          where <m>\vec{U}\in U</m> and <m>\vec{w}\in W</m>.
        </p>

        <theorem xml:id="thm-direct-sum">
          <statement>
            <p>
              For any subspaces <m>U,W</m> of a vector space <m>V</m>,
              <m>U\cap W = \{\vec{0}\}</m> if and only if for every <m>\vec{v}\in U+W</m>
              there exist unique <m>\vec{u}\in U, \vec{w}\in W</m> such that <m>\vec{v}=\vec{u}+\vec{w}</m>.
            </p>
          </statement>
          <proof>
            <p>
              Suppose that <m>U\cap W = \{\vec{0}\}</m>, and suppose that we have
              <m>\vec{v} = \vec{u}_1+\vec{w}_1 = \vec{u}_2+\vec{w}_2</m>,
              for <m>\vec{u}_1,\vec{u}_2\in U,\vec{w}_1,\vec{w}_2\in W</m>.
              Then <m>\vec{0}=(\vec{u}_1-\vec{u}_2)+(\vec{w}_1-\vec{w}_2)</m>,
              which implies that
              <me>
                \vec{w}_1-\vec{w}_2 = -(\vec{u}_1-\vec{u}_2)
              </me>.
              Now, <m>\vec{u}=\vec{u}_1-\vec{u}_2\in U</m>,
              since <m>U</m> is a subspace, and similarly,
              <m>\vec{w}=\vec{w}_1-\vec{w}_2\in W</m>.
              But we also have <m>\vec{w}=-\vec{u}</m>, which implies that <m>\vec{w}\in U</m>.
              Therefore, <m>\vec{w}\in U\cap W</m>, which implies that <m>\vec{w}=\vec{0}</m>,
              so <m>\vec{w}_1=\vec{w}_2</m>.
              But we must also then have <m>\vec{u}=\vec{0}</m>, so <m>\vec{u}_1=\vec{u}_2</m>.
            </p>

            <p>
              Conversely, suppose that every <m>\vec{v}\in U+W</m> can be written uniquely as <m>\vec{v}=vec{u}+\vec{w}</m>,
              with <m>\vec{u}\in U</m> and <m>\vec{w}\in W</m>. Suppose that <m>\vec{a}\in U\cap W</m>.
              Then <m>\vec{a}\in U</m> and <m>\vec{a}\in W</m>, so we also have <m>-\vec{a}\in W</m>,
              since <m>W</m> is a subspace.
              But then <m>\vec{0}=\vec{a}+(-\vec{a})</m>, where <m>\vec{a}\in U</m> and <m>-\vec{a}\in W</m>.
              On the other hand, <m>\vec{0}=\vec{0}+\vec{0}</m>,
              and <m>\vec{0}</m> belongs to both <m>U</m> and <m>W</m>. It follows that <m>\vec{a}=\vec{0}</m>.
              Since <m>\vec{a}</m> was arbitrary, <m>U\cap W = \{\vec{0}\}</m>.
            </p>
          </proof>

        </theorem>

        <p>
          We end with one last application of the theory we've developed on the existence of a basis for a finite-dimensional vector space.
          As we continue on to later topics, we'll find that it is often useful to be able to decompose a vector space into a direct sum of subspaces.
          Using bases, we can show that this is always possible.
        </p>

        <theorem xml:id="thm-construct-complement">
          <statement>
            <p>
              Let <m>V</m> be a finite-dimensonal vector space, and let <m>U</m> be any subspace of <m>V</m>.
              Then there exists a subspace <m>W\subseteq V</m> such that <m>U\oplus W = V</m>.
            </p>
          </statement>
          <proof>
            <p>
              Let <m>\{\vec{u}_1,\ldots, \vec{u}_m\}</m> be a basis of <m>U</m>.
              Since <m>U\subseteq W</m>, the set <m>\{\vec{u}_1,\ldots, \vec{u}_m\}</m>
              is a linearly independent subset of <m>V</m>.
              Since any linearly independent set can be extended to a basis of <m>V</m>,
              there exist vectors <m>\vec{w}_1,\ldots,\vec{w}_n</m> such that
              <me>
                \{\vec{u}_1,\ldots, \vec{u}_m,\vec{w}_1,\ldots, \vec{w}_n\}
              </me>
              is a basis of <m>V</m>.
            </p>

            <p>
              Now, let <m>W = \spn\{\vec{w}_1,\ldots, \vec{w}_n\}</m>.
              Then <m>W</m> is a subspace, and <m>\{\vec{w}_1,\ldots, \vec{w}_n\}</m>
              is a basis for <m>W</m>. (It spans, and must be independent since it's a subset of an independent set.)
            </p>

            <p>
              Clearly, <m>U+W=V</m>, since <m>U+W</m> contains the basis for <m>V</m> we've constructed.
              To show the sum is direct, it suffices to show that <m>U\cap W = \{\vec{0}\}</m>.
              To that end, suppose that <m>\vec{v}\in U\cap W</m>.
              Since <m>\vec{v}\in U</m>, we have
              <me>
                \vec{v}=a_1\vec{u}_1+\cdots +a_m\vec{u}_m
              </me>
              for scalars <m>a_1,\ldots, a_m</m>. Since <m>\vec{v}\in W</m>, we can write
              <me>
                \vec{v}=b_1\vec{w}_1+\cdots + b_n\vec{w}_n
              </me>
              for scalars <m>b_1,\ldots, b_n</m>. But then
              <me>
                \vec{0}=\vec{v}-\vec{v}=a_1\vec{u}_1+\cdots a_m\vec{u}_m-b_1\vec{w}_1-\cdots -b_n\vec{w}_n.
              </me>
              Since <m>\{\vec{u}_1,\ldots, \vec{u}_m,\vec{w}_1,\ldots, \vec{w}_n\}</m> is a basis for <m>V</m>,
              it's independent, and therefore, all of the <m>a_i,b_j</m> must be zero, and therefore, <m>\vec{v}=\vec{0}</m>.

            </p>
          </proof>

          <p>
            The subspace <m>W</m> constructed in the theorem above is called a <term>complement</term> of <m>U</m>.
            It is not unique; indeed, it depends on the choice of basis vectors.
            For example, if <m>U</m> is a one-dimensional subspace of <m>\R^2</m>; that is, a line,
            then any other non-parallel line through the origin provides a complement of <m>U</m>.
            Later we will see that an especially useful choice of complement is the <term>orthogonal complement</term>.
          </p>
        </theorem>

      </paragraphs>


    </section>
  </article>
</pretext>
