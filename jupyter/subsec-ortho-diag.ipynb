{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">4.2<\/span> <span class=\"title\">Diagonalization of symmetric matrices<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-469\">Recall that an $n\\times n$ matrix $A$ is <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">symmertric<\/em> if $A^T=A\\text{.}$ Symmetry of $A$ is equivalent to the following: for any vectors $\\xx,\\yy\\in\\R^n\\text{,}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx\\dotp (A\\yy) = (A\\xx)\\dotp \\yy\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">To see that this is implied by the symmetry of $A\\text{,}$ note that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx\\dotp (A\\yy) = \\xx^T(A\\yy)=(\\xx^TA^T)\\yy = (A\\xx)^T\\yy=(A\\xx)\\dotp\\yy\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">For inner product spaces, the above is taken as the <em class=\"emphasis\">definition<\/em> of what it means for an operator to be symmetric.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-39\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-470\">Prove that if $\\xx\\dotp(A\\yy)=(A\\xx)\\dotp \\yy$ for any $\\xx,\\yy\\in\\R^n\\text{,}$ then $A$ is symmetric.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-31\" id=\"solution-31\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-31\"><div class=\"solution solution-like\"><p id=\"p-471\">Take $\\xx=\\mathbf{e}_i$ and $\\yy=\\mathbf{e}_j\\text{,}$ where $\\{\\mathbf{e}_1,\\ldots, \\mathbf{e}_n\\}$ is the standard basis for $\\R^n\\text{.}$ Then with $A = [a_{ij}]$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\na_{ij} =\\mathbf{e}_i\\dotp(A\\mathbf{e}_j) = (A\\mathbf{e}_i)\\dotp \\mathbf{e}_j = a_{ji}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which shows that $A^T=A\\text{.}$<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-472\">A useful property of symmetric matrices, mentioned earlier, is that eigenvectors corresponding to distinct eigenvalues are orthogonal.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-ortho-eigen-symm\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-473\">If $A$ is a symmetric matrix, then eigenvectors corresponding to <em class=\"emphasis\">distinct<\/em> eigenvalues are orthogonal.<\/p><\/article><article class=\"proof\" id=\"proof-20\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-474\">To see this, suppose $A$ is symmetric, and that we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA\\xx_1=\\lambda_1\\xx_1\\quad \\text{ and } A\\xx_2=\\lambda_2\\xx_2\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">with $\\xx_1\\neq\\mathbf{0},\\xx_2\\neq \\mathbf{0}\\text{,}$ and $\\lambda_1\\neq \\lambda_2\\text{.}$ We then have, since $A$ is symmetric, and using the result above,<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\lambda_1(\\xx_1\\dotp \\xx_2) = (\\lambda_1\\xx_1)\\dotp \\xx_2 = (A\\xx_1)\\dotp \\xx_2 = \\xx_1\\dotp(A\\xx_2) = \\xx_1(\\lambda_2\\xx_2) = \\lambda_2(\\xx_1\\dotp\\xx_2)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">It follows that $(\\lambda_1-\\lambda_2)(\\xx_1\\dotp \\xx_2)=0\\text{,}$ and since $\\lambda_1\\neq \\lambda_2\\text{,}$ we must have $\\xx_1\\dotp \\xx_2=0\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-475\">The procedure for diagonalizing a matrix is as follows: assuming that $\\dim E_\\lambda(A)$ is equal to the multiplicity of $\\lambda$ for each distinct eigenvalue $\\lambda\\text{,}$ we find a basis for $E_\\lambda(A)\\text{.}$ The union of the bases for each eigenspace is then a basis of eigenvectors for $\\R^n\\text{,}$ and the matrix $P$ whose columns are those eigenvectors will satisfy $P^{-1}AP = D\\text{,}$ where $D$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-476\">If $A$ is symmetric, we know that eigenvectors from <em class=\"emphasis\">different<\/em> eigenspaces will be orthogonal to each other. If we further choose an orthogonal basis of eigenvectors for each eigenspace (which is possible via the Gram-Schmidt procedure), then we can construct an orthogonal basis of eigenvectors for $\\R^n\\text{.}$ Furthermore, if we normalize each vector, then we'll have an orthonormal basis. The matrix $P$ whose columns consist of these orthonormal basis vectors has a name.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-orthogonal-matrix\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-477\">A matrix $P$ is called <dfn class=\"terminology\">orthogonal<\/dfn> if $P^T = P^{-1}\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-ortho-matrix\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-478\">A matrix $P$ is orthogonal if and only if the columns of $P$ form an orthonormal basis for $\\R^n\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-479\">A fun fact is that if the columns of $P$ are orthonormal, then so are the rows. But this is not true if we ask for the columns to be merely orthogonal. For example, the columns of $A = \\bbm 1\\amp 0\\amp 5\\\\-2\\amp 1\\amp 2\\\\1\\amp 2\\amp -1\\ebm $ are orthogonal, but the rows certainly are not. But if we normalize the columns, we get<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nP = \\bbm 1\/\\sqrt{6}\\amp 0 \\amp 1\/\\sqrt{30}\\\\-2\/\\sqrt{6}\\amp 1\/\\sqrt{5}\\amp 2\/\\sqrt{30}\\\\1\/\\sqrt{6}\\amp 2\/\\sqrt{5}\\amp -1\/\\sqrt{30}\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which, as you can confirm, is an orthogonal matrix.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-ortho-diag\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-480\">An $n\\times n$ matrix $A$ is said to be <em class=\"emphasis\">orthogonally diagonalizable<\/em> if there exists an orthogonal matrix $P$ such that $P^TAP$ is diagonal.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-481\">The above definition leads to the following result, also known as the Principal Axes Theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-real-spectral\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.6<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Real Spectral Theorem.<\/span><\/h6><p id=\"p-482\">The following are equivalent for a real $n\\times n$ matrix $A\\text{:}$<\/p><ol class=\"decimal\"><li id=\"li-107\"><p id=\"p-483\">$A$ is symmetric.<\/p><\/li><li id=\"li-108\"><p id=\"p-484\">There is an orthonormal basis for $\\R^n$ consisting of eigenvectors of $A\\text{.}$<\/p><\/li><li id=\"li-109\"><p id=\"p-485\">$A$ is orthogonally diagonalizable.<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-40\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.2.7<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-486\">Determine the eigenvalues of $A=\\bbm 5\\amp -2\\amp -4\\\\-2\\amp 8\\amp -2\\\\-4\\amp -2\\amp 5\\ebm\\text{,}$ and find an orthogonal matrix $P$ such that $P^TAP$ is diagonal.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-32\" id=\"solution-32\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-32\"><div class=\"solution solution-like\"><p id=\"p-487\">We'll solve this problem with the help of the computer.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA = Matrix(3,3,[5,-2,-4,-2,8,-2,-4,-2,5])\np=A.charpoly().as_expr()\nfactor(p)"], "outputs":[]}<p id=\"p-488\">We get $c_A(x)=x(x-9)^2\\text{,}$ so our eigenvalues are $0$ and $9\\text{.}$ For $0$ we have $E_0(A) = \\nll(A)\\text{:}$<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.nullspace()"], "outputs":[]}<p id=\"p-489\">For $9$ we have $E_9(A) = \\nll(A-9I)\\text{.}$<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["B=A-9*eye(3)\nB.nullspace()"], "outputs":[]}<p id=\"p-490\">The approach above is useful as we're trying to remind ourselves how eigenvalues and eigenvectors are defined and computed. Eventually we might want to be more efficient. Fortunately, there's a command for that.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.eigenvects()"], "outputs":[]}<p id=\"p-491\">Note that the output above lists each eigenvalue, followed by its multiplicity, and then the associated eigenvectors.<\/p><p id=\"p-492\">This gives us a basis for $\\R^3$ consisting of eigenvalues of $A\\text{,}$ but we want an orthogonal basis. Note that the eigenvector corresponding to $\\lambda = 0$ is orthogonal to both of the eigenvectors corresponding to $\\lambda =9\\text{.}$ But these eigenvectors are not orthogonal to each other. To get an orthogonal basis for $E_9(A)\\text{,}$ we apply the Gram-Schmidt algorithm.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["L=B.nullspace()\nGramSchmidt(L)"], "outputs":[]}<p id=\"p-493\">This gives us an orthogonal basis of eigenvectors. Scaling to clear fractions, we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\left\\{\\bbm 2\\\\1\\\\2\\ebm, \\bbm -1\\\\2\\\\0\\ebm, \\bbm -4\\\\-2\\\\5\\ebm\\right\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">From here, we need to normalize each vector to get the matrix $P\\text{.}$ But we might not like that the last vector has norm $\\sqrt{45}\\text{.}$ One option to consider is to apply Gram-Schmidt with the vectors in the other order.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["L=[Matrix([-1,0,1]),Matrix([-1,2,0])]\nGramSchmidt(L)"], "outputs":[]}<p id=\"p-494\">That gives us the (slightly nicer) basis<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\left\\{\\bbm 2\\\\1\\\\2\\ebm, \\bbm -1\\\\0\\\\1\\ebm, \\bbm 1\\\\-4\\\\1\\ebm\\right\\}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">The corresponding orthonormal basis is<\/p><div class=\"displaymath\">\n\\begin{equation*}\nB = \\left\\{\\frac{1}{3}\\bbm 2\\\\1\\\\2\\ebm, \\frac{1}{\\sqrt{2}}\\bbm -1\\\\0\\\\1\\ebm, \\frac{1}{\\sqrt{18}}\\bbm 1\\\\-4\\\\1\\ebm\\right\\}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">This gives us the matrix $P=\\bbm 2\/3\\amp -1\/\\sqrt{2}\\amp 1\/\\sqrt{18}\\\\1\/3\\amp 0 \\amp -4\/\\sqrt{18}\\\\2\/3\\amp 1\/\\sqrt{2}\\amp 1\/\\sqrt{18}\\ebm\\text{.}$ Let's confirm that $P$ is orthogonal.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["P=Matrix(3,3,[2\/3, -1\/sqrt(2),1\/sqrt(18), 1\/3,0,-4\/sqrt(18),2\/3,1\/sqrt(2),1\/sqrt(18)])\nP,P*P.transpose()"], "outputs":[]}<p id=\"p-495\">Since $PP^T=I_3\\text{,}$ we can conclude that $P^T=P^{-1}\\text{,}$ so $P$ is orthogonal, as required. Finally, we diagonalize $A\\text{.}$<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["Q=P.transpose()\nQ*A*P"], "outputs":[]}<p id=\"p-496\">Incidentally, the SymPy library for Python does have a diagaonalization routine; however, it does not do orthogonal diagonalization by default. Here is what it provides for our matrix $A\\text{.}$<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.diagonalize()"], "outputs":[]}<\/div><\/div><\/div><\/article><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "subsec-ortho-diag.ipynb"}
}