{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">4.1<\/span> <span class=\"title\">Eigenvalues and Eigenvectors<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-eigenvalue\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-430\">Let $A$ be an $n\\times n$ matrix. A number $\\lambda$ is called an <dfn class=\"terminology\">eigenvalue<\/dfn> of $A$ if there exists a nonzero vector $\\xx$ such that<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA\\xx = \\lambda\\xx\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Any such vector $\\xx$ is called an <dfn class=\"terminology\">eigenvector<\/dfn> associated to the eigenvalue $\\lambda\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-431\">Note that eigenvalues and eigenvectors can just as easily be defined for a general linear operator $T:V\\to V\\text{.}$ In this context, and eigenvector $\\xx$ is sometimes referred to as a <em class=\"emphasis\">characteristic vector<\/em> (or characteristic direction) for $T\\text{,}$ since the property $T(\\xx)=\\lambda \\xx$ simply states that the transformed vector $T(\\xx)$ is parallel to the original vector $\\xx\\text{.}$ Some linear algebra textbooks that focus more on general linear transformations frame this topic in the context of <em class=\"emphasis\">invariant subspaces<\/em> for a linear operator.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-432\">A subspace $U\\subseteq V$ is <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">invariant<\/em> with respect to $T$ if $T(\\uu)\\in U$ for all $\\uu\\in U\\text{.}$ Note that if $\\xx$ is an eigenvector of $T\\text{,}$ then $\\spn\\{\\xx\\}$ is an invariant subspace. To see this, note that if $T(\\xx)=\\lambda \\xx$ and $\\yy=k\\xx\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(\\yy)=T(k\\xx)=kT(\\xx)=k(\\lambda \\xx)=\\lambda(k\\xx)=\\lambda\\yy)\\text{.}\n\\end{equation*}\n<\/div><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-433\">Note that if $\\xx$ is an eigenvector of the matrix $A\\text{,}$ then we have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation}\n(A-\\lambda I_n)\\xx=\\mathbf{0}\\text{,}\\label{eq-eigen-null}\\tag{4.1.1}\n\\end{equation}\n<\/div><p data-braille=\"continuation\">where $I_n$ denotes the $n\\times n$ identity matrix. Thus, if $\\lambda$ is an eigenvalue of $A\\text{,}$ any corresponding eigenvector is an element of $\\nll(A-\\lambda I_n)\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-eigenspace\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-434\">For any real number $\\lambda$ and matrix $A\\text{,}$ we define the <dfn class=\"terminology\">eigenspace<\/dfn> $E_\\lambda(A)$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\nE_\\lambda(A) = \\nll (A-\\lambda I_n)\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-435\">Note that $E_\\lambda(A)$ can be defined for any real number $\\lambda\\text{,}$ whether or not $\\lambda$ is an eigenvalue. However, the eigenvalues of $A$ are distinguished by the property that there is a <em class=\"emphasis\">nonzero<\/em> solution to <a href=\"subsec-eigen-basics.ipynb#mjx-eqn-eq-eigen-null\" class=\"internal\" title=\"Equation 4.1.1\">(4.1.1)<\/a>. Furthermore, we know that <a href=\"subsec-eigen-basics.ipynb#mjx-eqn-eq-eigen-null\" class=\"internal\" title=\"Equation 4.1.1\">(4.1.1)<\/a> can only have nontrivial solutions if the matrix $A-\\lambda I_n$ is not invertible. We also know that $A-\\lambda I_n$ is non-invertible if and only if $\\det (A-\\lambda I_n) = 0\\text{.}$ This gives us the following theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-eigenspace-nonzero\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-436\">The following are equivalent for any $n\\times n$ matrix $A$ and real number $\\lambda\\text{:}$<\/p><ol class=\"decimal\"><li id=\"li-101\"><p id=\"p-437\">$\\lambda$ is an eigenvalue of $A\\text{.}$<\/p><\/li><li id=\"li-102\">$\\displaystyle E_\\lambda(A)\\neq \\{\\mathbf{0}\\}$<\/li><li id=\"li-103\">$\\displaystyle \\det(A-\\lambda I_n) = 0$<\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-438\">The polynomial $p_A(x)=\\det(xI_n -A)$ is called the <dfn xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"terminology\">characteristic polynomial<\/dfn> of $A\\text{.}$ (Note that $\\det(x I_n-A) = (-1)^n\\det(A-x I_n)\\text{.}$ We choose this order so that the coefficient of $x^n$ is always 1.) The equation<\/p><div class=\"displaymath\">\n\\begin{equation}\n\\det(xI_n - A) = 0\\label{eq-characteristic}\\tag{4.1.2}\n\\end{equation}\n<\/div><p data-braille=\"continuation\">is called the <dfn class=\"terminology\">characteristic equation<\/dfn> of $A\\text{.}$ The solutions to this equation are precisely the eigenvalues of $A\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-439\">Recall that a matrix $B$ is said to be <dfn class=\"terminology\">similar<\/dfn> to a matrix $A$ if there exists an invertible matrix $P$ such that $B = P^{-1}AP\\text{.}$ Much of what follows concerns the question of whether or not a given $n\\times n$ matrix $A$ is <dfn class=\"terminology\">diagonalizable<\/dfn>.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-diagonalizable\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-440\">An $n\\times n$ matrix $A$ is said to be <dfn class=\"terminology\">diagonalizable<\/dfn> if $A$ is similar to a diagonal matrix.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-441\">The following results will frequently be useful.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-similar-properties\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-442\">The relation $A\\sim B$ if and only if $A$ is simlar to $B$ is an equivalence relation. Moreover, if $A\\sim B\\text{,}$ then:<\/p><ul class=\"disc\"><li id=\"li-104\">$\\displaystyle \\det A = \\det B$<\/li><li id=\"li-105\">$\\displaystyle \\tr A = \\tr B$<\/li><li id=\"li-106\">$\\displaystyle c_A(x) = c_B(x)$<\/li><\/ul><p data-braille=\"continuation\">In other words, $A$ and $B$ have the same determinant, trace, and characteristic polynomial (and thus, the same eigenvalues).<\/p><\/article><article class=\"proof\" id=\"proof-18\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-443\">The first two follow directly from properties of the determinant and trace. For the last, note that if $B = P^{-1}AP\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nP^{-1}(xI_n-A)P = P^{-1}(xI_n)P-P^{-1}AP = xI_n B\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $xI_n-B\\sim xI_n-A\\text{,}$ and therefore $\\det(xI_n-B)=\\det(xI_n-A)\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"example example-like\" id=\"example-7\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Example<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.6<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-444\">Determine the eigenvalues and eigenvectors of $A = \\bbm 0\\amp 1\\amp 1\\\\1\\amp 0\\amp 1\\\\1\\amp 1\\amp 0\\ebm\\text{.}$<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-30\" id=\"solution-30\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-30\"><div class=\"solution solution-like\"><p id=\"p-445\">We begin with the characteristic polynomial. We have<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\det(xI_n - A) \\amp =\\det\\bbm x \\amp -1\\amp -1\\\\-1\\amp x \\amp -1\\\\-1\\amp -1\\amp x\\ebm\\\\\n\\amp = x \\begin{vmatrix}x \\amp -1\\\\-1\\amp x\\end{vmatrix}\n+1\\begin{vmatrix}-1\\amp -1\\\\-1\\amp x\\end{vmatrix}\n-1\\begin{vmatrix}-1\\amp x\\\\-1\\amp -1\\end{vmatrix}\\\\\n\\amp = x(x^2-1)+(-x-1)-(1+x)\\\\\n\\amp x(x-1)(x+1)-2(x+1)\\\\\n\\amp (x+1)[x^2-x-2]\\\\\n\\amp (x+1)^2(x-2)\\text{.}\n\\end{align*}\n<\/div><p id=\"p-446\">The roots of the characteristic polynomial are our eigenvalues, so we have $\\lambda_1=-1$ and $\\lambda_2=2\\text{.}$ Note that the first eigenvalue comes from a repeated root. This is typically where things get interesting. If an eigenvalue does not come from a repeated root, then there will only be one (independent) eigenvector that corresponds to it. (That is, $\\dim E_\\lambda(A)=1\\text{.}$) If an eigenvalue is repeated, it could have more than one eigenvector, but this is not guaranteed.<\/p><p id=\"p-447\">We find that $A-(-1)I_n = \\bbm 1\\amp 1\\amp 1\\\\1\\amp 1\\amp 1\\\\1\\amp 1\\amp 1\\ebm\\text{,}$ which has reduced row-echelon form $\\bbm 1\\amp 1\\amp 1\\\\0\\amp 0\\amp 0\\\\0\\amp 0\\amp 0\\ebm\\text{.}$ Solving for the nullspace, we find that there are two independent eigenvectors:<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx_{1,1}=\\bbm 1\\\\-1\\\\0\\ebm, \\quad \\text{ and } \\quad \\xx_{1,2}=\\bbm 1\\\\0\\\\-1\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so<\/p><div class=\"displaymath\">\n\\begin{equation*}\nE_{-1}(A) = \\spn\\left\\{\\bbm 1\\\\-1\\\\0\\ebm, \\bbm 1\\\\0\\\\-1\\ebm\\right\\}\\text{.}\n\\end{equation*}\n<\/div><p id=\"p-448\">For the second eigenvector, we have $A-2I = \\bbm -2\\amp 1\\amp 1\\\\1\\amp -2\\amp 1\\\\1\\amp 1\\amp -2\\ebm\\text{,}$ which has reduced row-echelon form $\\bbm 1\\amp 0\\amp -1\\\\0\\amp 1\\amp -1\\\\0\\amp 0\\amp 0\\ebm\\text{.}$ An eigenvector in this case is given by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx_2 = \\bbm 1\\\\1\\\\1\\ebm\\text{.}\n\\end{equation*}\n<\/div><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-449\">In general, if the characteristic polynomial can be factored as<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\np_A(x)=(x-\\lambda)^mq(x)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $q(x)$ is not divisible by $x-\\lambda\\text{,}$ then we say that $\\lambda$ is an eigenvalue of <dfn class=\"terminology\">multiplicity<\/dfn> $m\\text{.}$ A main result is the following.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-multiplicity\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.7<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-450\">Let $\\lambda$ be an eigenvalue of $A$ of multiplicity $m\\text{.}$ Then $\\dim E_\\lambda(A)\\leq m\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-451\">Some textbooks refer to the multiplicity $m$ of an eigenvalue as the <em class=\"emphasis\">algebraic multiplicity<\/em> of $\\lambda\\text{,}$ and the number $\\dim E_\\lambda(A)$ as the <em class=\"emphasis\">geometric multiplicity<\/em> of $\\lambda\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-452\">To prove <a href=\"subsec-eigen-basics.ipynb#thm-multiplicity\" class=\"internal\" title=\"Theorem 4.1.7\">Theorem 4.1.7<\/a> we need the following lemma, from Section 5.5 of Nicholson's textbook.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"lemma theorem-like\" id=\"lem-block-eigen\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Lemma<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.1.8<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-453\">Let $\\{\\xx_1,\\ldots, \\xx_k\\}$ be a set of linearly independent eigenvectors of a matrix $A\\text{,}$ with corresponding eigenvalues $\\lambda_1,\\ldots, \\lambda_k$ (not necessarily distinct). Extend this set to a basis $\\{\\xx_1,\\ldots, \\xx_k,\\xx_{k+1},\\ldots, \\xx_n\\}\\text{,}$ and let $P=\\bbm \\xx_1\\amp \\cdots \\amp \\xx_n\\ebm$ be the matrix whose columns are the basis vectors. (Note that $P$ is necessarily invertible.) Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nP^{-1}AP = \\bbm \\diag(\\lambda_1,\\ldots, \\lambda_k) \\amp B\\\\0\\amp A_1\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $B$ has size $k\\times (n-k)\\text{,}$ and $A_1$ has size $(n-k)\\times (n-k)\\text{.}$<\/p><\/article><article class=\"proof\" id=\"proof-19\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-454\">We have<\/p><div class=\"displaymath\">\n\\begin{align*}\nP^{-1}AP \\amp = P^{-1}A\\bbm \\xx_1\\amp \\cdots \\amp \\xx_n\\ebm\\\\\n\\amp =\\bbm (P^{-1}A)\\xx_1\\amp \\cdots \\amp (P^{-1}A)\\xx_n\\ebm\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">For $1\\leq i\\leq k\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n(P^{-1}A)(\\xx_i) = P^{-1}(A\\xx_i) = P^{-1}(\\lambda_i\\xx_i)=\\lambda_i(P^{-1}\\xx_i)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">But $P^{-1}\\xx_i$ is the $i$th column of $P^{-1}P = I_n\\text{,}$ which proves the result.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-455\">We can use <a href=\"subsec-eigen-basics.ipynb#lem-block-eigen\" class=\"internal\" title=\"Lemma 4.1.8\">Lemma 4.1.8<\/a> to prove that $\\dim E_\\lambda(A)\\leq m$ as follows. Suppose $\\{\\xx_1,\\ldots, \\xx_k\\}$ is a basis for $E_\\lambda(A)\\text{.}$ Then this is a linearly independent set of eigenvectors, so our lemma guarantees the existence of a matrix $P$ such that<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nP^{-1}AP = \\bbm \\lambda I_k \\amp B\\\\0\\amp A_1\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Let $\\tilde{A}=P^{-1}AP\\text{.}$ On the one hand, since $\\tilde{A}\\sim A\\text{,}$ we have $c_A(x)=c_{\\tilde{A}}(x)\\text{.}$ On the other hand,<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\det(xI_n-\\tilde{A}) = \\det\\bbm (x-\\lambda)I_k \\amp -B\\\\0 \\amp xI_{n-k}-A_1\\ebm = (x-\\lambda)^k\\det(xI_{n-k}A_1)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">This shows that $c_A(x)$ is divisible by $(x-\\lambda)^k\\text{.}$ Since $m$ is the largest integer such that $c_A(x)$ is divisible by $(x-\\lambda)^m\\text{,}$ we must have $\\dim E_\\lambda(A)=k\\leq m\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-456\"><a href=\"subsec-eigen-basics.ipynb#thm-multiplicity\" class=\"internal\" title=\"Theorem 4.1.7\">Theorem 4.1.7<\/a> provides an initial criterion for diagonalization: if the dimension of each eigenspace $E_\\lambda(A)$ is equal to the multiplicity of $\\lambda\\text{,}$ then $A$ is diagonalizable. The truth of this statement relies on one additional fact: any set of eigenvectors corresponding to <em class=\"emphasis\">distinct<\/em> eigenvalues is linearly independent. The proof of this fact is a relatively straightforward proof by induction. It can be found in Section 5.5 of Nicholson for those who are interested. However, our focus for the remainder of the section will be on diagonalization of <em class=\"emphasis\">symmetric<\/em> matrices, and soon we will see that for such matrices, eigenvectors corresponding to different eigenvalues are, in fact, <em class=\"emphasis\">orthogonal<\/em>.<\/p><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "subsec-eigen-basics.ipynb"}
}