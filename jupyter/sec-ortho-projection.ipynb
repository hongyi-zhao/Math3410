{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">3.2<\/span> <span class=\"title\">Orthogonal Projection<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-392\">In <a href=\"sec-orthogonal-sets.ipynb#ex-test-span\" class=\"internal\" title=\"Exercise 3.1.14\">Exercise 3.1.14<\/a>, we saw that <a href=\"sec-orthogonal-sets.ipynb#thm-fourier-expansion\" class=\"internal\" title=\"Theorem 3.1.13: Fourier expansion theorem\">Fourier expansion theorem<\/a> gives us an efficient way of testing whether or not a given vector belongs to the span of an orthogonal set. When the answer is “no”, the quantity we compute while testing turns out to be very useful: it gives the <em class=\"emphasis\">orthogonal projection<\/em> of that vector onto the span of our orthogonal set. This turns out to be exactly the ingredient needed to solve certain minimum distance problems.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-393\">You may recall the following from elementary linear algebra, or vector calculus. Given an nonzero vector $\\uu$ and a vector $\\vv\\text{,}$ the <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">projection<\/em> of $\\vv$ onto $\\uu$ is given by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\proj{\\uu}{\\vv} = \\left(\\frac{\\vv\\dotp\\uu}{\\len{\\uu}^2}\\right)\\uu\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Note that this looks just like one of the terms in <a href=\"sec-orthogonal-sets.ipynb#thm-fourier-expansion\" class=\"internal\" title=\"Theorem 3.1.13: Fourier expansion theorem\">Fourier expansion theorem<\/a>. Recall also that the vector $\\vv-\\proj{\\uu}{\\vv}$ is orthogonal to $\\uu\\text{.}$ Our next result is a generalization of this observation.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-orthogonal-lemma\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.1<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Orthogonal Lemma.<\/span><\/h6><p id=\"p-394\">Let $\\{\\vv_1,\\vv_2,\\ldots, \\vv_m\\}$ be an orthogonal set of vectors in $\\R^n\\text{,}$ and let $\\xx$ be any vector in $\\R^n\\text{.}$ Define the vector $\\vv_{m+1}$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vv_{m+1} = \\xx-\\left(\\frac{\\xx\\dotp\\vv_1}{\\len{\\vv_1}^2}\\vv_1+\\cdots + \\frac{\\xx\\dotp\\vv_m}{\\len{\\vv_m}^2}\\vv_m\\right)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Then:<\/p><ol class=\"decimal\"><li id=\"li-95\"><p id=\"p-395\">$\\vv_{m+1}\\dotp \\vv_i = 0$ for each $i=1,\\ldots, m\\text{.}$<\/p><\/li><li id=\"li-96\"><p id=\"p-396\">If $\\xx\\notin\\spn\\{\\vv_1,\\ldots, \\vv_m\\}\\text{,}$ then $\\vv_{m+1}\\neq \\mathbf{0}\\text{,}$ and therefore, $\\{\\vv_1,\\ldots, \\vv_m,\\vv_{m+1}\\}$ is an orthogonal set.<\/p><\/li><\/ol><\/article><article class=\"proof\" id=\"proof-17\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><ol id=\"p-397\" class=\"decimal\"><li id=\"li-97\"><p id=\"p-398\">For any $i=1,\\ldots m\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vv_{m+1}\\dotp\\vv_i = \\xx\\dotp\\vv_i - \\frac{\\xx\\dotp\\vv_i}{\\len{\\vv_i}^2}(\\vv_i\\dotp\\vv_i)=0\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">since $\\vv_i\\dotp\\vv_j = 0$ for $i\\neq j\\text{.}$<\/p><\/li><li id=\"li-98\"><p id=\"p-399\">It follows from the <a href=\"sec-orthogonal-sets.ipynb#thm-fourier-expansion\" class=\"internal\" title=\"Theorem 3.1.13: Fourier expansion theorem\">Fourier expansion theorem<\/a> that $\\vv_{m+1}=\\mathbf{0}$ if and only if $\\xx\\in\\spn\\{\\vv_1,\\ldots, \\vv_m\\}\\text{,}$ and the fact that $\\{\\vv_1,\\ldots, \\vv_m,\\vv_{m+1}\\}$ is an orthogonal set then follows from the first part.<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-400\">It follows from the <a href=\"sec-ortho-projection.ipynb#thm-orthogonal-lemma\" class=\"internal\" title=\"Theorem 3.2.1: Orthogonal Lemma\">Orthogonal Lemma<\/a> that for any subspace $U\\subseteq \\R^n\\text{,}$ any set of orthogonal vectors in $U$ can be extended to an orthogonal basis of $U\\text{.}$ Since any set containing a single nonzero vector is orthogonal, it follows that every subspace has an orthogonal basis. (If $U=\\{\\mathbf{0}\\}\\text{,}$ we consider the empty basis to be orthogonal.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-401\">The procedure for creating an orthogonal basis is clear. Start with a single nonzero vector $\\vv_1\\in U\\text{.}$ If $U\\neq \\spn\\{\\vv_1\\}\\text{,}$ choose a vector $\\xx_1\\in U$ with $\\xx_1\\notin\\spn\\{\\vv_1\\}\\text{.}$ The <a href=\"sec-ortho-projection.ipynb#thm-orthogonal-lemma\" class=\"internal\" title=\"Theorem 3.2.1: Orthogonal Lemma\">Orthogonal Lemma<\/a> then provides us with a vector<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\vv_2 = \\xx_1-\\frac{\\xx_1\\dotp\\vv_1}{\\len{\\vv_1}^2}\\vv_1\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">such that $\\{\\vv_1,\\vv_2\\}$ is orthogonal. If $U=\\spn\\{\\vv_1,\\vv_2\\}\\text{,}$ we're done. Otherwise, we repeat the process, choosing $\\xx_2\\notin\\spn\\{\\vv_1,\\vv_2\\}\\text{,}$ and then using the <a href=\"sec-ortho-projection.ipynb#thm-orthogonal-lemma\" class=\"internal\" title=\"Theorem 3.2.1: Orthogonal Lemma\">Orthogonal Lemma<\/a> to obtain $\\vv_3\\text{,}$ and so on, until an orthogonal basis is obtained.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-402\">With one minor modification, the above procedure provides us with a major result. Suppose $U$ is a subspace of $\\R^n\\text{,}$ and start with <em class=\"emphasis\">any<\/em> basis $\\{\\xx_1,\\ldots, \\xx_m\\}$ of $U\\text{.}$ By choosing our $\\xx_i$ in the procedure above to be these basis vectors, we obtain the <em class=\"emphasis\">Gram-Schmidt algorithm<\/em> for constructing an orthogonal basis.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-gram-schmidt\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.2<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Gram-Schmidt Orthonormalization Algorithm.<\/span><\/h6><p id=\"p-403\">Let $U$ be a subspace of $\\R^n\\text{,}$ and let $\\{\\xx_1,\\ldots, \\xx_m\\}$ be a basis of $U\\text{.}$ Define vectors $\\vv_1,\\ldots, \\vv_m$ in $U$ as follows:<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\vv_1 \\amp = \\xx_1 \\\\\n\\vv_2 \\amp = \\xx_2 - \\frac{\\xx_2\\dotp\\vv_1}{\\len{\\vv_1}^2}\\vv_1\\\\\n\\vv_3 \\amp = \\xx_3 - \\frac{\\xx_3\\dotp\\vv_1}{\\len{\\vv_1}^2}\\vv_1-\\frac{\\xx_3\\dotp\\vv_2}{\\len{\\vv_2}^2}\\vv_2\\\\\n\\vdots \\amp \\\\\n\\vv_m \\amp = \\xx_m - \\frac{\\xx_m\\dotp\\vv_1}{\\len{\\vv_1}^2}\\vv_1-\\cdots - \\frac{\\xx_m\\dotp\\vv_{m-1}}{\\len{\\vv_{m-1}}^2}\\vv_{m-1}\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">Then $\\{\\vv_1,\\ldots, \\vv_m\\}$ is an orthogonal basis for $U\\text{.}$ Moreover, for each $k=1,2,\\ldots, m\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\spn\\{\\vv_1,\\ldots, \\vv_k\\} = \\spn\\{\\xx_1,\\ldots, \\xx_k\\}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-404\">Of course, once we've used Gram-Schmidt to find an orthogonal basis, we can normalize each vector to get an orthonormal basis. The Gram-Schmidt algorithm is ideal when we know how to find <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">a<\/em> basis for a subspace, but we need to know an orthogonal basis. For example, suppose we want an orthonormal basis for the nullspace of the matrix<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA = \\bbm 2 \\amp -1 \\amp 3 \\amp 0 \\amp 5\\\\0 \\amp 2 \\amp -3  \\amp 1 \\amp 4\\\\ -4 \\amp 2 \\amp -6 \\amp 0 \\amp -10\\\\ 2 \\amp 1 \\amp 0 \\amp 1 \\amp 9\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">First, we find <em class=\"emphasis\">any<\/em> basis for the nullspace.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA = Matrix([[2,-1,3,0,5],\n            [0,2,-3,1,4],\n            [-4,2,-6,0,-10],\n            [2,1,0,1,9]])\nA.nullspace()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-405\">Let's make that basis look a little nicer by using some scalar multiplication to clear fractions.<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\nB=\\left\\{\\xx_1=\\bbm 3\\\\-6\\\\-4\\\\0\\\\0\\ebm, \\xx_2=\\bbm 1\\\\2\\\\0\\\\-4\\\\0\\ebm, \\xx_3=\\bbm 7\\\\4\\\\0\\\\0\\\\-2\\ebm\\right\\}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Definitely not an orthogonal basis. So we take $\\vv_1=\\xx_1\\text{,}$ and<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\vv_2 \\amp = \\xx_2-\\left(\\frac{\\xx_2\\dotp\\vv_1}{\\len{\\vv_1}^2}\\right)\\vv_1\\\\\n\\amp = \\bbm 1\\\\2\\\\0\\\\-4\\\\0\\ebm -\\frac{-9}{61}\\bbm 3\\\\-6\\\\-4\\\\-0\\\\0\\ebm \\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">which equals something I'm not sure I want to try to simplify. Finally, we find<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vv_3 = \\xx_3-\\left(\\frac{\\xx_3\\dotp \\vv_1}{\\len{\\vv_1}^2}\\right)\\vv_1-\\left(\\frac{\\xx_3\\dotp\\vv_2}{\\len{\\vv_2}^2}\\right)\\vv_2\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">And now you probably get about five minutes into the fractions and say something that shouldn't appear in print. This sounds like a job for the computer.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["B = A.nullspace()\nGramSchmidt(B)"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-406\">Oh wait, you wanted that normalized? Turns out the <code class=\"code-inline tex2jax_ignore\">GramSchmidt<\/code> function has an optional argument of true or false. The default is false, which is to not normalize. Setting it to true gives an orthonormal basis:<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["GramSchmidt(B,true)"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-407\">OK, so that's nice, and fairly intimidating looking. Did it work? We can specify the vectors in our list by giving their positions, which are 0, 1, and 2, respectively.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["L=GramSchmidt(B)\nL[0],L[1],L[2]"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-408\">Let's compute dot products:<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["L[0].dot(L[1]),L[1].dot(L[2]),L[0].dot(L[2])"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-409\">Let's also confirm that these are indeed in the nullspace.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A*L[0],A*L[1],A*L[2]"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-410\">Boom. Let's try another example. This time we'll keep the vectors a little smaller in case you want to try it by hand.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-35\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-411\">Confirm that the set $B=\\{(1,-2,1), (3,0,-2), (-1,1,2)\\}$ is a basis for $\\R^3\\text{,}$ and use the <a href=\"sec-ortho-projection.ipynb#thm-gram-schmidt\" class=\"internal\" title=\"Theorem 3.2.2: Gram-Schmidt Orthonormalization Algorithm\">Gram-Schmidt Orthonormalization Algorithm<\/a> to find an orthonormal basis.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-26\" id=\"solution-26\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-26\"><div class=\"solution solution-like\"><p id=\"p-412\">First, note that we can actually jump right into the Gram-Schmidt procedure. If the set $B$ is not a basis, then it won't be independent, and when we attempt to construct the third vector in our orthonormal basis, its projection on the the subspace spanned by the first two will be the same as the original vector, and we'll get zero when we subtract the two.<\/p><p id=\"p-413\">We let $\\xx_1=(1,-2,1), \\xx_2=(3,0,-2), \\xx_3=(-1,1,2)\\text{,}$ and set $\\vv_1=\\xx_1\\text{.}$ Then we have<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\vv_2 \\amp = \\xx_2-\\left(\\frac{\\xx_2\\dotp \\vv_1}{\\len{\\vv_1}^2}\\right)\\vv_1 \\\\\n\\amp = (3,0,-2)-\\frac{1}{6}(1,-2,1)\\\\\n\\amp = \\frac16(17,2,-3) \\text{.}\n\\end{align*}\n<\/div><p id=\"p-414\">Next, we compute $\\vv_3\\text{.}$<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\vv_3 \\amp = \\xx_3-\\left(\\frac{\\xx_3\\dotp \\vv_1}{\\len{\\vv_1}^2}\\right)\\vv_1 - \\left(\\frac{\\xx_3\\dotp \\vv_2}{\\len{\\vv_2}^2}\\right)\\vv_2\\\\\n\\amp = (-1,1,2)-\\frac{-1}{6}(1,-2,1)-\\cdot \\frac{-21}{303}(17,2,-3)\\\\\n\\amp = (-1,1,2)+\\frac16(1,-2,1)+\\frac{7}{101}(17,2,-3)\\\\\n\\amp = \\frac{1}{606}\\bigl((-606,606,1212)+(101,-202,101)+(782,84,-126)\\bigr)\\\\\n\\amp = \\frac{1}{606}(277,488,1187)\\text{.}\n\\end{align*}\n<\/div><p id=\"p-415\">OK. Now, given the frequency with which typos occur in this text, and the fact that I tried to do the above problem in my head while typing (with an occasional calculator check), there's a good chance there's a mistake somewhere. Let's check our work.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nL=(Matrix([1,-2,1]),Matrix([3,0,-2]),Matrix([-1,1,2]))\nGramSchmidt(L)"], "outputs":[]}<\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><aside class=\"aside aside-like\" id=\"aside-1\"><p id=\"p-416\">You'll notice that I used $6\\vv_2$ rather than $\\vv_2$ in the calculation of $\\vv_3\\text{.}$ This lets me avoid fractions (momentarily), and doesn't affect the answer, since for any nonzero scalar $c\\text{,}$<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{align*}\n\\left(\\frac{c\\vv\\dotp \\xx}{\\len{c\\vv}^2}\\right)\\amp(c\\vv)\\\\\n\\amp= \\left(\\frac{c(\\vv\\dotp\\xx)}{c^2\\len{\\vv}^2}\\right)(c\\vv)\\\\\n\\amp=\\left(\\frac{\\vv\\dotp\\xx}{\\len{\\vv^2}}\\right)\\vv\\text{.}\n\\end{align*}\n<\/div><\/aside><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading\"><span class=\"title\">Projections.<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-417\">We hinted above that the calculations we've been doing have a lot to do with projection. Since any single nonzero vector forms an orthogonal basis for its span, the projection<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\proj{\\uu}{\\vv}=\\left(\\frac{\\uu\\dotp\\vv}{\\len{\\uu}^2}\\right)\\uu\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">can be viewed as the orthogonal projection of the vector $\\vv\\text{,}$ not onto the vector $\\uu\\text{,}$ but onto the subspace $\\spn\\{\\uu\\}\\text{.}$ This is, after all, how we viewed projections in elementary linear algebra: we drop the perpendicular from the tip of $\\vv$ onto the <em class=\"emphasis\">line<\/em> in the direction of $\\uu\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-418\">Now that we know how to define an orthogonal basis for a subspace, we can define orthogonal projection onto subspaces of dimension greater than one.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-ortho-projection\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-419\">Let $U$ be a subspace of $\\R^n$ with orthogonal basis $\\{\\uu_1,\\ldots, \\uu_k\\}\\text{.}$ For any vector $\\vv\\in \\R^n\\text{,}$ we define the <dfn class=\"terminology\">orthogonal projection<\/dfn> of $\\vv$ onto $U$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\proj{U}{\\vv} = \\sum_{i=1}^k\\left(\\frac{\\uu_i\\dotp\\vv}{\\len{\\uu_i}^2}\\right)\\uu_i\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-420\">Note that $\\proj{U}{\\vv}$ is indeed an element of $U\\text{,}$ since it's a linear combination of its basis vectors. In the case of the trivial subspace $U=\\{\\mathbf{0}\\}\\text{,}$ we define orthogonal projection of any vector to be $\\mathbf{0}\\text{,}$ since really, what other choice do we have? (This case isn't really of any interest, we just like being thorough.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-421\">Let's see how this might be put to use in a classic problem: finding the distance from a point to a plane.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><aside class=\"aside aside-like\" id=\"aside-2\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-422\">One limitation of this approach to projection is that we must project onto a <em class=\"emphasis\">subspace<\/em>. Given a plane like $x-2y+4z=4\\text{,}$ we would need to modify our approach. One way to do it would be to find a point on the plane, and then try to translate everything to the origin. It's interesting to think about how this might be accomplished (in particular, in what direction would the translation have to be performed?) but someone external to the questions we're interested in here.<\/p><\/aside><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"example example-like\" id=\"example-6\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Example<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-423\">Find the distance from the point $(3,1,-2)$ to the plane $P$ defined by $x-2y+4z=0\\text{.}$<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-27\" id=\"solution-27\"><span class=\"type\">Solution<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">1<\/span><span class=\"space\"> <\/span><span class=\"title\">Using projection onto a normal vector<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-27\"><div class=\"solution solution-like\"><p id=\"p-424\">In an elementary linear algebra (or calculus) course, we would solve this problem as follows. First, we would need two vectors parallel to the plane. If $\\bbm x\\\\y\\\\z\\ebm$ lies in the plane, then $x-2y+4z=0\\text{,}$ so $x=2y-4z\\text{,}$ and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\bbm x\\\\y\\\\z\\ebm = \\bbm 2y-4z\\\\y\\\\z\\ebm = y\\bbm 2\\\\1\\\\0\\ebm + z\\bbm -4\\\\0\\\\1\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so $\\uu=\\bbm 2\\\\1\\\\0\\ebm$ and $\\vv\\bbm -4\\\\0\\\\1\\ebm$ are parallel to the plane. We then compute the normal vector<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\mathbf{n}=\\uu\\times\\vv=\\bbm 1\\\\-2\\\\4\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and compute the projection of the position vector $\\mathbf{p}=\\bbm 3,1,-2\\ebm$ for the point $P=(3,1,-2)$ onto $\\mathbf{n}\\text{.}$ This gives the vector<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\xx = \\left(\\frac{\\mathbf{p}\\dotp\\mathbf{n}}{\\len{\\mathbf{n}}^2}\\right)\\mathbf{n} = \\frac{-7}{21}\\bbm 1\\\\-2\\\\4\\ebm =\\bbm-1\/3\\\\2\/3\\\\-4\/3\\ebm\\text{.}\n\\end{equation*}\n<\/div><p id=\"p-425\">Now, this vector is <em class=\"emphasis\">parallel<\/em> to $\\mathbf{n}\\text{,}$ so it's perpendicular to the plane. Subtracting it from $\\mathbf{p}$ gives a vector parallel to the plane, and this is the position vector for the point we seek.<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\mathbf{q}=\\mathbf{p}-\\xx=\\bbm 3\\\\1\\\\-2\\ebm-\\bbm -1\/3\\\\-2\/3\\\\-4\/3\\ebm = \\bbm 10\/3\\\\1\/3\\\\-2\/3\\ebm\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so the closest point is $Q=\\bigl(\\frac{10}{3},\\frac13,-\\frac{2}{3}\\bigr)\\text{.}$ We weren't asked for it, but note that if we wanted the distance from the point $P$ to the plane, this is given by $\\len{\\xx}=\\frac13\\sqrt{21}\\text{.}$<\/p><\/div><\/div><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-28\" id=\"solution-28\"><span class=\"type\">Solution<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">2<\/span><span class=\"space\"> <\/span><span class=\"title\">Using orthogonal projection<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-28\"><div class=\"solution solution-like\"><p id=\"p-426\">Let's solve the same problem using orthogonal projection. First, we have to deal with the fact that the vectors $\\uu$ and $\\vv$ are probably not orthogonal. To get around this, we replace $\\vv$ with<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ww = \\vv-\\left(\\frac{\\vv\\dotp\\uu}{\\len{\\uu}^2}\\right)\\uu = \\bbm -4\\\\0\\\\1\\ebm+\\frac 85\\bbm 2\\\\1\\\\0\\ebm = \\bbm -4\/5\\\\8\/5\\\\1\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">We now set<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\mathbf{q} \\amp =\\left(\\frac{\\mathbf{p}\\dotp\\uu}{\\len{\\uu}^2}\\right)\\uu-\\left(\\frac{\\mathbf{p}\\dotp\\ww}{\\len{\\ww}^2}\\right)\\ww\\\\\n\\amp = \\frac{7}{5}\\bbm 2\\\\1\\\\0\\ebm +\\frac{-14}{105}\\bbm -4\\\\8\\\\5\\ebm \\\\\n\\amp = \\bbm 10\/3\\\\1\/3\\\\-2\/3\\ebm\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">Lo and behold, we get the same answer as before.<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-427\">The only problem with <a href=\"sec-ortho-projection.ipynb#def-ortho-projection\" class=\"internal\" title=\"Definition 3.2.4\">Definition 3.2.4<\/a> is that it appears to depend on the choice of orthogonal basis. To see that it doesn't, we need one more definition.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-ortho-comp\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.6<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-428\">For any subspace $U$ of $\\R^n\\text{,}$ we define the <dfn class=\"terminology\">orthogonal complement<\/dfn> of $U\\text{,}$ denoted $U^\\bot\\text{,}$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\nU^\\bot = \\{\\xx\\in\\R^n \\,|\\, \\xx\\dotp\\yy = 0 \\text{ for all } \\yy\\in U\\}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-429\">The term “complement” comes from terminology we mentioned early on, but didn't spend much time on. <a href=\"sec-dimension.ipynb#thm-construct-complement\" class=\"internal\" title=\"Theorem 1.6.20\">Theorem 1.6.20<\/a> told us that for any subspace $U$ of a vector space $V\\text{,}$ it is possible to construct another subspace $W$ of $V$ such that $V = U\\oplus W\\text{.}$ The subspace $W$ is known as a complement of $U\\text{.}$ A complement is not unique, but the orthogonal complement is. As you might guess from the name, $U^\\bot$ is also a subspace of $\\R^n\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-36\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.7<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-430\">Show that $U^\\bot$ is a subspace of $\\R^n\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-projection\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.8<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Projection Theorem.<\/span><\/h6><p id=\"p-431\">Let $U$ be a subspace of $\\R^n\\text{,}$ let $\\xx$ be any vector in $\\R^n\\text{,}$ and let $\\mathbf{p}=\\proj{U}{\\xx}\\text{.}$ Then:<\/p><ol class=\"decimal\"><li id=\"li-99\"><p id=\"p-432\">$\\mathbf{p}\\in U\\text{,}$ and $\\xx-\\mathbf{p}\\in U^\\bot\\text{.}$<\/p><\/li><li id=\"li-100\"><p id=\"p-433\">$\\mathbf{p}$ is the <em class=\"emphasis\">closest<\/em> vector in $U$ to the vector $\\xx\\text{,}$ in the sense that the distance $d(\\mathbf{p},\\xx)$ is minimal among all vectors in $U\\text{.}$ That is, for all $\\uu\\neq \\mathbf{p}\\in U\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\xx-\\mathbf{p}}\\lt\\len{\\xx-\\yy}\\text{.}\n\\end{equation*}\n<\/div><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-37\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.9<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-434\">Show that $U\\cap U^\\bot = \\{\\mathbf{0}\\}\\text{.}$ Use this fact to show that <a href=\"sec-ortho-projection.ipynb#def-ortho-projection\" class=\"internal\" title=\"Definition 3.2.4\">Definition 3.2.4<\/a> does not depend on the choice orthogonal basis.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref hint-knowl original\" data-refid=\"hk-hint-2\" id=\"hint-2\"><span class=\"type\">Hint<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-hint-2\"><div class=\"hint solution-like\"><p id=\"p-435\">Suppose we find vectors $\\mathbf{p}$ and $\\mathbf{p}'$ using basis $B$ and $B'\\text{.}$ Note that $\\mathbf{p}-\\mathbf{p}'\\in U\\text{,}$ but also that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\mathbf{p}-\\mathbf{p}' = (\\mathbf{p}-\\xx)-(\\mathbf{p}'-\\xx)\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Now use <a href=\"sec-ortho-projection.ipynb#thm-projection\" class=\"internal\" title=\"Theorem 3.2.8: Projection Theorem\">Theorem 3.2.8<\/a><\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-436\">Finally, we note one more useful fact. The process of sending a vector to its orthogonal projection defines an operator on $\\R^n\\text{,}$ and yes, it's linear.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-proj-operator\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.10<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-437\">Let $U$ be a subspace of $\\R^n\\text{,}$ and define a function $P_U:\\R^n\\to \\R^n$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\nP_U(\\xx) = \\proj{U}{\\xx} \\text{ for any } \\xx\\in\\R^n\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Then $T$ is a linear operator such that $U=\\im P_U$ and $U^\\bot = \\ker P_U\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-438\">Note: it follows from this result and the <a href=\"sec-kernel-image.ipynb#thm-dimension-lintrans\" class=\"internal\" title=\"Theorem 2.2.7: Dimension Theorem\">Dimension Theorem<\/a> that<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\dim U + \\dim U^\\bot = n\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and since $U\\cap U^\\bot = \\{\\mathbf{0}\\}\\text{,}$ $U^\\bot$ is indeed a complement of $U$ in the sense introduced in <a href=\"sec-dimension.ipynb#thm-construct-complement\" class=\"internal\" title=\"Theorem 1.6.20\">Theorem 1.6.20<\/a>. It's also fairly easy to see that $\\dim U + \\dim U^\\bot = n$ directly. If $\\ww\\in U^\\bot\\text{,}$ and $\\{\\uu_1,\\ldots, \\uu_k\\}$ is a basis for $U\\text{,}$ then we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ww\\dotp \\uu_1= 0, \\ldots, \\ww\\dotp \\uu_k=0\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and for an unknown $\\ww\\text{,}$ this is simply a homogeneous system of $k$ equations with $n$ variables. Moreover, they are <em class=\"emphasis\">independent<\/em> equations, since the $\\uu_i$ form a basis. We thus expect $n-k$ free parameters in the general solution.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-38\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">3.2.11<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-439\">Let $U = \\{(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c)\\,|\\, a,b,c\\in\\R\\}\\subseteq \\R^5\\text{.}$ Determine a basis for $U^\\bot\\text{.}$<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-29\" id=\"solution-29\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-29\"><div class=\"solution solution-like\"><p id=\"p-440\">First, we note that for a general element of $U\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c) = a(1,2,0,4,1)+b(-1,1,0,-1,0)+c(3,0,3,3,-4)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">If $\\ww = (x_1,x_2,x_3,x_4,x_5)\\in U^\\bot\\text{,}$ then we must have<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\ww\\dotp (1,2,0,4,1) \\amp = x_1+2x_2+4x_4+x_5=0 \\\\\n\\ww\\dotp (-1,1,0,-1,0) \\amp =-x_1+x_2-x_4=0\\\\\n\\ww\\dotp (3,0,3,3,-4) \\amp =3x_1+3x_3+3x_4-4x_5 = 0\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">To find a basis for $U^\\bot\\text{,}$ we simply need to find the nullspace of the coefficient matrix for this system, which we do below.<\/p>{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA = Matrix(3,5,[1,2,0,4,1,-1,1,0,-1,0,3,0,3,3,-4])\nA.nullspace()"], "outputs":[]}<\/div><\/div><\/div><\/article><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-ortho-projection.ipynb"}
}