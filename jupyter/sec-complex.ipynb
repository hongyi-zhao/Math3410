{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\renewcommand{\\C}{\\mathbb{C}}\n\\newcommand{\\im}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\diag}{\\operatorname{diag}}\n\\newcommand{\\tr}{\\operatorname{tr}}\n\\newcommand{\\dotp}{\\!\\boldsymbol{\\cdot}\\!}\n\\newcommand{\\len}[1]{\\lVert #1\\rVert}\n\\newcommand{\\abs}[1]{\\lvert #1\\rvert}\n\\newcommand{\\proj}[2]{\\operatorname{proj}_{#1}{#2}}\n\\newcommand{\\bz}{\\overline{z}}\n\\newcommand{\\zz}{\\mathbf{z}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\vv}{\\mathbf{v}}\n\\newcommand{\\ww}{\\mathbf{w}}\n\\newcommand{\\xx}{\\mathbf{x}}\n\\newcommand{\\yy}{\\mathbf{y}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\basis}[2]{\\{\\mathbf{#1}_1,\\mathbf{#1}_2,\\ldots,\\mathbf{#1}_{#2}\\}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Section<\/span> <span class=\"codenumber\">4.4<\/span> <span class=\"title\">Diagonalization of complex matrices<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-510\">Recall that when we first defined vector spaces, we mentioned that a vector space can be defined over any <em class=\"emphasis\">field<\/em> $\\mathbb{F}\\text{.}$ To keep things simple, we've mostly assumed $\\mathbb{F}=\\mathbb{R}\\text{.}$ But most of the theorems and proofs we've encountered go through unchanged if we work over a general field. (This is not quite true: over a <em class=\"emphasis\">finite<\/em> field things can get more complicated. For example, if $\\mathbb{F}=\\mathbb{Z}_2=\\{0,1\\}\\text{,}$ then we get weird results like $\\vv+\\vv=\\mathbf{0}\\text{,}$ since $1+1=0\\text{.}$)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-511\">In fact, if we replace $\\R$ by $\\C\\text{,}$ about the only thing we'd have to go back and change is the definition of the dot product. The reason for this is that although the complex numbers seem computationally more complicated, (which might mostly be because you don't use them often enough) they follow the exact same algebraic rules as the real numbers. In other words, the <em class=\"emphasis\">arithmetic<\/em> might be different, but the <em class=\"emphasis\">algebra<\/em> is the same. There is one key difference between the two fields: over the complex numbers, every polynomial can be factored. This is important if you're interested in finding eigenvalues.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Subsection<\/span> <span class=\"codenumber\">4.4.1<\/span> <span class=\"title\">Review of complex numbers<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-512\">Let's quickly review some basic facts about complex numbers that are typically covered in an earlier course. First, we define the set of complex numbers by<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\C = \\{x+iy \\,|\\, x,y\\in \\R\\},\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">where $i=\\sqrt{-1}\\text{.}$ We have a bijection $\\C \\to \\R^2$ given by $x+iy\\mapsto (x,y)\\text{;}$ because of this, we often picture $\\C$ as the <em class=\"emphasis\">complex plane<\/em>, with a “real” $x$ axis, and an “imaginary” $y$ axis.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-513\">Arithmetic with complex numbers is defined by<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{align*}\n(x_1+iy_1)+(x_2+iy_2) \\amp = (x_1+x_2)+i(y_1+y_2) \\\\\n(x_1+iy_1)(x_2+iy_2) \\amp = (x_1x_2-y_1y_2)+i(x_1y_2+x_2y_1)\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">The multiplication rule looks complicated, but it's really just “<abbr class=\"initialism\">FOIL<\/abbr>”, along with the fact that $i^2=-1\\text{.}$ Note that if $c=c+i0$ is real, we have $c(x+iy)=(cx)+i(cy)\\text{,}$ so that $\\C$ has the structure of a two dimensional vector space over $\\R$ (isomorphic to $\\R^2$).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-514\">Subtraction is defined in the obvious way. Division is less obvious. To define division, it helps to first introduce the <dfn xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"terminology\">complex conjugate<\/dfn>. Given a complex number $z=x+iy\\text{,}$ we define $\\overline{z}=x-iy\\text{.}$ The importance of the conjugate is that we have the identity<\/p><div class=\"displaymath\">\n\\begin{equation*}\nz\\bz = (x+iy)(x-iy)=x^2+y^2\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">So $z\\bz$ is <em class=\"emphasis\">real<\/em>, and <em class=\"emphasis\">non-negative<\/em>. This lets us define the <dfn class=\"terminology\">modulus<\/dfn> of $z$ by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\abs{z} = \\sqrt{z\\bz} = \\sqrt{x^2+y^2}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">This gives a measure of the magnitude of a complex number, in the same way as the vector norm on $\\R^2\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-515\">Now, given $z=x+iy$ and $w=s+it\\text{,}$ we have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\frac{z}{w}=\\frac{z\\bar{w}}{w\\bar{w}} = \\frac{(x+iy)(s-it)}{s^2+t^2} = \\frac{xs-yt}{s^2+t^2}+i\\frac{xt+ys}{s^2+t^2}\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">And of course, we have $w\\bar{w}\\neq 0$ unless $w=0\\text{,}$ and as usual, we don't divide by zero.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-516\">An important thing to keep in mind when working with complex numbers is that they follow the same algebraic rules as real numbers. For example, given $a,b,z,w$ all complex, and $a\\neq 0\\text{,}$ where $az+b=w\\text{,}$ if we want to solve for $z\\text{,}$ the answer is $z=\\frac1a(w-b)\\text{,}$ as it would be in $\\R\\text{.}$ The difference between $\\R$ and $\\C$ only really materializes when we want to <em class=\"emphasis\">compute<\/em> $z\\text{,}$ by plugging in values for $a,b$ and $w\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-517\">One place where $\\C$ is <em xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"emphasis\">computationally<\/em> more complicated is finding powers and roots. For this, it is often more convenient to write our complex numbers in <dfn class=\"terminology\">polar form<\/dfn>. The key to the polar form for complex numbers is <em class=\"emphasis\">Euler's identity<\/em>. For a <em class=\"emphasis\">unit<\/em> complex number $z$ (that is with $\\abs{z}=1$), we can think of $z$ as a point on the unit circle, and write<\/p><div class=\"displaymath\">\n\\begin{equation*}\nz = \\cos(\\theta)+i\\sin(\\theta)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">If $\\abs{z}=r\\text{,}$ we simply change the radius of our circle, so in general, $z = r(\\cos(\\theta)+i\\sin(\\theta))\\text{.}$ Euler's identity states that<\/p><div class=\"displaymath\">\n\\begin{equation}\n\\cos(\\theta)+i\\sin(\\theta)=e^{i\\theta}\\text{.}\\label{eq-euler}\\tag{4.4.1}\n\\end{equation}\n<\/div><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-518\">This idea of putting a complex number in an exponential function seems odd at first. If you take a course in complex variables, you'll get a better understanding of why this makes sense. But for now, we can take it as a convenient piece of notation. The reason it's convenient is that the rules for complex arithmetic turn out to align quite nicely with properties of the exponential function. For example, de Moivre's Theorem states that<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n(\\cos(\\theta)+i\\sin(\\theta))^n = \\cos(n\\theta)+i\\sin(n\\theta)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">This can be proved by induction (and the proof is not even all that bad), but it seems perfectly obvious in exponential notation:<\/p><div class=\"displaymath\">\n\\begin{equation*}\n(e^{i\\theta})^n = e^{in\\theta}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">since you multiply exponents when you raise a power to a power.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-519\">Similarly, if we want to multiply two unit complex numbers, we have<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{align*}\n(\\cos\\alpha+i\\sin\\alpha)(\\cos\\beta+i\\sin\\beta) \\amp = (\\cos\\alpha\\cos\\beta-\\sin\\alpha\\sin\\beta)\\\\\n\\amp \\quad\\quad +i(\\sin\\alpha\\cos\\beta+\\cos\\alpha\\sin\\beta)\\\\\n\\amp = \\cos(\\alpha+\\beta)+i\\sin(\\alpha+\\beta)\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">But in exponential notation, this is simply<\/p><div class=\"displaymath\">\n\\begin{equation*}\ne^{i\\alpha}e^{i\\beta} = e^{i(\\alpha+\\beta)}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which makes sense, since when you multiply exponentials, you add the exponents.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-520\">Generally, problems involving addition and subtraction are best handled in “rectangular” ($x+iy$) form, while problems involving multiplication and powers are best handled in polar form.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Subsection<\/span> <span class=\"codenumber\">4.4.2<\/span> <span class=\"title\">Complex vectors<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-521\">A complex vector space is simply a vector space where the scalars are elements of $\\C$ rather than $\\R\\text{.}$ Examples include polynomials with complex coefficients, complex-valued functions, and $\\C^n\\text{,}$ which is defined exactly how you think it should be. In fact, one way to obtain $\\C^n$ is to start with the exact same standard basis we use for $\\R^n\\text{,}$ and then take linear combinations using complex scalars.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-522\">We'll write elements of $\\C^n$ as $\\zz = (z_1,z_2,\\ldots, z_n)\\text{.}$ Notice that we've dropped the arrow notation for vectors in favour of a bold font. The reason is that we'll want to consider complex conjugates, and things get cluttered if we try to fit an arrow and a bar over our vector. The complex conjugate of $\\zz$ is given by<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\bar{\\zz} = (\\bz_1,\\bz_2,\\ldots, \\bz_n)\\text{.}\n\\end{equation*}\n<\/div><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-523\">The standard inner product on $\\C^n$ looks a lot like the dot product on $\\R^n\\text{,}$ with one important difference: we apply a complex conjugate to the second vector.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-complex-inner\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.1<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-524\">The <dfn class=\"terminology\">standard inner product<\/dfn> on $\\C^n$ is defined as follows: given $\\zz=(z_1,z_2,\\ldots, z_n)$ and $\\ww=(w_1,w_2,\\ldots, w_n)\\text{,}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle \\zz,\\ww\\rangle = \\zz\\dotp\\bar{\\ww} = z_1\\bar{w}_1+z_2\\bar{w}_2+\\cdots + z_n\\bar{w}_n\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-525\">If $\\zz,\\ww$ are real, this is just the usual dot product. The reason for using the complex conjugate is to ensure that we still have a positive-definite inner product on $\\C^n\\text{:}$<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\langle \\zz,\\zz\\rangle = z_1\\bz_1+z_2\\bz_2+\\cdots + z_n\\bz_n = \\abs{z_1}^2+\\abs{z_2}^2+\\cdots + \\abs{z_n}^2\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which shows that $\\langle \\zz,\\zz\\rangle \\geq 0\\text{,}$ and $\\langle \\zz,\\zz\\rangle = 0$ if and only if $\\zz=\\mathbf{0}\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-41\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.2<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-526\">Compute the dot product of $\\zz = (2-i, 3i, 4+2i)$ and $\\ww = (3i,4-5i,-2+2i)\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-527\">This isn't hard to do by hand, but it's useful to know how to ask the computer to do it, too. Unfortunately, the dot product in SymPy does not include the complex conjugate. One likely reason for this is that while most mathematicians take the complex conjugate of the <em class=\"emphasis\">second<\/em> vector, some mathematicians, and most physicists, put the conjugate on the first vector. So they may have decided to remain agnostic about this choice. We can manually apply the conjugate, using <code class=\"code-inline tex2jax_ignore\">Z.dot(W.H)<\/code>.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nZ = Matrix([2-I,3*I,4+2*I])\nW = Matrix([3*I,4-5*I,-2+2*I])\nZ, W, Z.dot(W.H)"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-528\">Again, you might want to wrap that last term in <code class=\"code-inline tex2jax_ignore\">simplify()<\/code> (in which case you'll get $-22-6i$ for the dot product). Above, we saw that the complex inner product is designed to be positive definite, like the real inner product. The remaining properties of the complex inner product are given as follows.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-complex-inner-props\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.3<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-529\">For any vectors $\\zz_1,\\zz_2,\\zz_3$ and any complex number $\\alpha\\text{,}$<\/p><ol class=\"decimal\"><li id=\"li-110\"><p id=\"p-530\">$\\langle \\zz_1+\\zz_2,\\zz_3\\rangle = \\langle \\zz_1,\\zz_3\\rangle + \\langle zz_2,\\zz_3\\rangle$ and $\\langle \\zz_1,\\zz_2+\\zz_3\\rangle = \\langle \\zz_1,\\zz_2\\rangle + \\langle zz_1,\\zz_3\\rangle\\text{.}$<\/p><\/li><li id=\"li-111\"><p id=\"p-531\">$\\langle \\alpha\\zz_1,\\zz_2\\rangle = \\alpha\\langle\\zz_1,\\zz_2\\rangle$ and $\\langle \\zz_1,\\alpha\\zz_2\\rangle=\\bar{\\alpha}\\langle \\zz_1,\\zz_2\\rangle\\text{.}$<\/p><\/li><li id=\"li-112\"><p id=\"p-532\">$\\displaystyle \\langle \\zz_2,\\zz_1\\rangle = \\overline{\\langle \\zz_1,\\zz_2\\rangle}$<\/p><\/li><li id=\"li-113\"><p id=\"p-533\">$\\langle \\zz_1,\\zz_1\\rangle\\geq 0\\text{,}$ and $\\langle \\zz_1,\\zz_1\\rangle =0$ if and only if $\\zz_1=\\mathbf{0}\\text{.}$<\/p><\/li><\/ol><\/article><article class=\"proof\" id=\"proof-21\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><ol id=\"p-534\" class=\"decimal\"><li id=\"li-114\"><p id=\"p-535\">Using the distributive properties of matrix multiplication and the transpose,<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\langle \\zz_1+\\zz_2,\\zz_3\\rangle \\amp= (\\zz_1+\\zz_2)^T\\bar{\\zz_3}\\\\\n\\amp =(\\zz_1^T+\\zz_2^T)\\bar{\\zz_3}\\\\\n\\amp =\\zz_1^T\\bar{\\zz_3}+\\zz_2^T\\bar{\\zz_3}\\\\\n\\amp =\\langle \\zz_1,\\zz_3\\rangle + \\langle \\zz_2,\\zz_3\\rangle\\text{.}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">The proof is similar when addition is in the second component. (But not identical -- you'll need the fact that the complex conjugate is distributive, rather than the transpose.)<\/p><\/li><li id=\"li-115\"><p id=\"p-536\">These again follow from writing the inner product as a matrix product.<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle \\alpha\\zz_1,\\zz_2\\rangle = (\\alpha \\zz_1)^T\\bar{\\zz_2} = \\alpha(\\zz_1^T\\bar{\\zz_2}) = \\alpha\\langle\\zz_1,\\zz_2\\rangle\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle \\zz_1,\\alpha\\zz_2\\rangle = \\zz_1^T\\overline{\\alpha \\zz_2} = \\zz_1^T(\\bar{\\alpha}\\bar{\\zz_2}) = \\bar{\\alpha}(\\zz_1^T\\zz_2)=\\alpha\\langle \\zz_1,\\zz_2\\rangle\\text{.}\n\\end{equation*}\n<\/div><\/li><li id=\"li-116\"><p id=\"p-537\">Note that for any vectors $\\zz,\\ww\\text{,}$ $\\zz^T\\ww$ is a number, and therefore equal to its own transpose. Thus, we have $\\zz^T\\ww = (\\zz^T\\ww)^T=\\ww^T\\zz\\text{,}$ and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\overline{\\langle \\zz_1,\\zz_2\\rangle} = \\overline{\\zz_1^T\\bar{\\zz_2}} = \\overline{\\bar{\\zz_2}^T\\zz_1} = \\zz_2^T\\overline{\\zz_1}=\\langle \\zz_2,\\zz_1\\rangle\\text{.}\n\\end{equation*}\n<\/div><\/li><li id=\"li-117\"><p id=\"p-538\">This was already demonstrated above.<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-complex-norm\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.4<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-539\">The <dfn class=\"terminology\">norm<\/dfn> of a vector $\\zz = (z_1,z_2,\\ldots, z_n)$ in $\\C^n$ is given by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\zz} = \\sqrt{\\langle \\zz,\\zz\\rangle} = \\sqrt{\\abs{z_1}^2+\\abs{z_2}^2+\\cdots +\\abs{z_n}^2}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-540\">Note that much like the real norm, the complex norm satisfies $\\len{\\alpha\\zz}=\\abs{\\alpha}\\len{\\zz}$ for any (complex) scalar $\\alpha\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"type\">Subsection<\/span> <span class=\"codenumber\">4.4.3<\/span> <span class=\"title\">Complex matrices<\/span><\/h6><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-541\">Linear transformations are defined in exactly the same way, and a complex matrix is simply a matrix whose entries are complex numbers. There are two important operations defined on complex matrices: the conjugate, and the conjugate transpose (also known as the hermitian transpose).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-conjugate-transpose\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.5<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-542\">The <dfn class=\"terminology\">conjugate<\/dfn> of a matrix $A=[a_{ij}]\\in M_{mn}(\\C)$ is the matrix $\\bar{A}=[\\bar{a}_{ij}]\\text{.}$ The <dfn class=\"terminology\">conjugate transpose<\/dfn> of $A$ is the matrix $A^H$ defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA^H = (\\bar{A})^T=\\overline{(A^T)}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-543\">Note that many textbooks use the notation $A^\\dagger$ for the conjugate transpose.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-hermitian-unitary\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.6<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-544\">An $n\\times n$ matrix $A\\in M_{nn}(\\C)$ is called <dfn class=\"terminology\">hermitian<\/dfn> if $A^H = A\\text{,}$ and <dfn class=\"terminology\">unitary<\/dfn> if $A^H = A^{-1}\\text{.}$ (A matrix is <dfn class=\"terminology\">skew-hermitian<\/dfn> if $A^H=-A\\text{.}$)<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-545\">Hermitian and unitary matrices (or more accurately, linear operators) are very important in quantum mechanics. Indeed, hermitian matrices represent “observable” quantities, in part because their eigenvalues are real, as we'll soon see. For us, hermitian and unitary matrices can simply be viewed as the complex counterparts of symmetric and orthogonal matrices, respectively. In fact, a real symmetric matrix <em class=\"emphasis\">is<\/em> hermitian, since the conjugate has no effect on it, and similarly, a real orthogonal matrix is technically unitary. As with orthogonal matrices, a unitary matrix can also be characterized by the property that its rows and columns both form orthonormal bases.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-42\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.7<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-546\">Show that the matrix $A = \\bbm 4\\amp 1-i\\amp -2+3i\\\\1+i\\amp 5 \\amp 7i\\\\-2-3i\\amp -7i\\amp -4\\ebm$ is hermitian, and that the matrix $B = \\frac12\\bbm 1+i\\amp \\sqrt{2}\\\\1-i\\amp\\sqrt{2}i\\ebm$ is unitary.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-33\" id=\"solution-33\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-33\"><div class=\"solution solution-like\"><p id=\"p-547\">We have $\\bar{A}=\\bbm 4\\amp 1+i\\amp -2-3i\\\\1-i\\amp 5 \\amp -7i\\\\-2+3i\\amp 7i\\amp -4\\ebm\\text{,}$ so<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA^H = (\\bar{A})^T = \\bbm 4\\amp 1-i\\amp -2+3i\\\\1+i\\amp 5\\amp 7i\\\\-2-3i\\amp -7i\\amp -4\\ebm = A\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">and<\/p><div class=\"displaymath\">\n\\begin{align*}\nBB^H \\amp =\\frac14\\bbm 1+i\\amp \\sqrt{2}\\\\1-i\\amp\\sqrt{2}i\\ebm\\bbm 1-i\\amp 1+i\\\\\\sqrt{2}\\amp-\\sqrt{2}i\\ebm \\\\\n\\amp =\\frac14\\bbm (1+i)(1-i)+2\\amp (1+i)(1+i)-2i\\\\(1-i)(1-i)+2i\\amp (1-i)(1+i)+2\\ebm\\\\\n\\amp =\\frac14\\bbm 4\\amp 0\\\\0\\amp 4\\ebm = \\bbm 1\\amp 0\\\\0\\amp 1\\ebm\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">so that $B^H = B^{-1}\\text{.}$<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-548\">When using SymPy, the hermitian conjugate of a matrix <code class=\"code-inline tex2jax_ignore\">A<\/code> is executed using <code class=\"code-inline tex2jax_ignore\">A.H<\/code>. (There appears to also be an equivalent operation named <code class=\"code-inline tex2jax_ignore\">Dagger<\/code> coming from <code class=\"code-inline tex2jax_ignore\">sympy.physics.quantum<\/code>, but I've had more success with <code class=\"code-inline tex2jax_ignore\">.H<\/code>.) The complex unit is entered as <code class=\"code-inline tex2jax_ignore\">I<\/code>. So for the exercise above, we can do the following.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A = Matrix(3,3,[4,1-I,-2+3*I,1+I,5,7*I,-2-3*I,-7*I,-4])\nA == A.H"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-549\">The last line verifies that $A=A^H\\text{.}$ We could also replace it with <code class=\"code-inline tex2jax_ignore\">A,A.H<\/code> to explicitly see the two matrices side by side. Now, let's confirm that $B$ is unitary.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["B = Matrix(2,2,[1\/2+1\/2*I, sqrt(2)\/2,1\/2-1\/2*I,(sqrt(2)\/2)*I])\nB,B*B.H"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-550\">Hmm... That doesn't look like the identity on the right. Maybe try replacing <code class=\"code-inline tex2jax_ignore\">B\\*B.H<\/code> with <code class=\"code-inline tex2jax_ignore\">simplify(B\\*B.H)<\/code>. Or you could try <code class=\"code-inline tex2jax_ignore\">B.H, B\\*\\*-1<\/code> to compare results. Actually, what's interesting is that in a Sage cell, <code class=\"code-inline tex2jax_ignore\">B.H == B\\*\\*-1<\/code> yields <code class=\"code-inline tex2jax_ignore\">False<\/code>, but <code class=\"code-inline tex2jax_ignore\">B.H == simplify(B\\*\\*-1)<\/code> yields <code class=\"code-inline tex2jax_ignore\">True<\/code>!<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-551\">As mentioned above, hermitian matrices are the complex analogue of symmetric matrices. Recall that a key property of a symmetric matrix is its symmetry with respect to the dot product. For a symmetric matrix $A\\text{,}$ we had $\\mathbf{x}\\dotp (A\\mathbf{y})=(A\\mathbf{x})\\dotp \\mathbf{y}\\text{.}$ Hermtian matrices exhibit the same behaviour with respect to the complex inner product.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-hermitian-symmetry\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.8<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-552\">An $n\\times n$ complex matrix $A$ is Hermitian if and only if<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle A\\zz,\\ww\\rangle = \\langle \\zz, A\\ww\\rangle\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">for any $\\zz,\\ww\\in\\C^n$<\/p><\/article><article class=\"proof\" id=\"proof-22\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-553\">Note that the property $A^H=A$ is equivalent to $A^T=\\bar{A}\\text{.}$ This gives us<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle A\\zz,\\ww\\rangle = (A\\zz)^T\\bar{\\ww} = (\\zz^TA^T)\\bar{\\ww} = (\\zz^T\\bar{A})\\bar{\\ww}=\\zz^T(\\overline{A\\ww}) = \\langle \\zz,\\ww\\rangle\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Conversely, suppose $\\langle A\\zz,\\ww\\rangle = \\langle \\zz, A\\ww\\rangle$ for all $\\zz,\\ww\\in \\C^n\\text{,}$ and let $\\basis{e}{n}$ denote the standard basis for $\\C^n\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\na_{ji}=\\langle A\\mathbf{e}_i,\\mathbf{e}_j\\rangle = \\langle \\mathbf{e}_i,A\\mathbf{e}_j\\rangle = \\overline{a_{ij}}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">which shows that $A^T=\\bar{A}\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-554\">Next, we've noted that one advantage of doing linear algebra over $\\C$ is that every polynomial can be completely factored, including the characteristic polynomial. This means that we can always find eigenvalues for a matrix. When that matrix is Hermitian, we get a surprising result.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-hermitian-eigen-real\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.9<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-555\">For any hermitian matrix $A\\text{,}$<\/p><ol class=\"decimal\"><li id=\"li-118\"><p id=\"p-556\">The eigenvalues of $A$ are real.<\/p><\/li><li id=\"li-119\"><p id=\"p-557\">Eigenvectors corresponding to distinct eigenvalues are orthogonal.<\/p><\/li><\/ol><\/article><article class=\"proof\" id=\"proof-23\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><ol id=\"p-558\" class=\"decimal\"><li id=\"li-120\"><p id=\"p-559\">Suppose $A\\zz = \\lambda\\zz$ for some $\\lambda\\in\\C$ and $\\zz\\neq \\mathbf{0}\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\lambda \\langle \\zz,\\zz\\rangle  = \\langle \\lambda\\zz,\\zz\\rangle = \\langle A\\zz,\\zz \\rangle = \\langle \\zz, A\\zz\\rangle = \\langle \\zz,\\lambda,\\zz\\rangle = \\bar{\\lambda}\\langle \\zz,\\zz\\rangle\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Thus, $(\\lambda-\\bar{\\lambda})\\len{\\zz}^2=0\\text{,}$ and since $\\len{z}\\neq 0\\text{,}$ we must have $\\bar{\\lambda}=\\lambda\\text{,}$ which means $\\lambda\\in\\R\\text{.}$<\/p><\/li><li id=\"li-121\"><p id=\"p-560\">Similarly, suppose $\\lambda_1,\\lambda_2$ are eigenvalues of $A\\text{,}$ with corresonding eigenvectors $\\zz,\\ww\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\lambda_1\\langle \\zz,\\ww\\rangle = \\langle \\lambda_1\\zz,\\ww\\rangle = \\langle A\\zz,\\ww\\rangle =\\langle \\zz,A\\ww\\rangle = \\langle \\zz,\\lambda_2\\ww\\rangle = \\bar{\\lambda_2}\\langle\\zz,\\ww\\rangle\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">This gives us $(\\lambda_1-\\bar{\\lambda_2})\\langle \\zz,\\ww\\rangle=0\\text{.}$ And since we already know $\\lambda_2$ must be real, and $\\lambda_1\\neq \\lambda_2\\text{,}$ we must have $\\langle, \\zz,\\ww\\rangle = 0\\text{.}$<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-561\">In light of <a href=\"sec-complex.ipynb#thm-hermitian-eigen-real\" class=\"internal\" title=\"Theorem 4.4.9\">Theorem 4.4.9<\/a>, we realize that diagonalization of hermitian matrices will follow the same script as for symmetric matrices. Indeed, <a href=\"sec-ortho-projection.ipynb#thm-gram-schmidt\" class=\"internal\" title=\"Theorem 3.2.2: Gram-Schmidt Orthonormalization Algorithm\">Gram-Schmidt Orthonormalization Algorithm<\/a> applies equally well in $\\C^n\\text{,}$ as long as we replace the dot product with the complex inner product. This suggests the following.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-complex-spectral\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.10<\/span><span class=\"period\">.<\/span><span class=\"space\"> <\/span><span class=\"title\">Spectral Theorem.<\/span><\/h6><p id=\"p-562\">If $A$ is an $n\\times n$ hermitian matrix, then there exists an orthonormal basis of $\\C^n$ consisting of eigenvectors of $A\\text{.}$ Moreover, the matrix $U$ whose columns consist of those eigenvectors is unitary, and the matrix $U^HAU$ is diagonal.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"exercise exercise-like\" id=\"exercise-43\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Exercise<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.11<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-563\">Confirm that the matrix $A = \\bbm 4 \\amp 3-i\\\\3+i\\amp 1\\ebm$ is hermitian. Then, find the eigenvalues of $A\\text{,}$ and a unitary matrix $U$ such that $U^HAU$ is diagonal.<\/p><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref solution-knowl original\" data-refid=\"hk-solution-34\" id=\"solution-34\"><span class=\"type\">Solution<\/span><\/a><div class=\"hidden-content tex2jax_ignore\" id=\"hk-solution-34\"><div class=\"solution solution-like\"><p id=\"p-564\">Confirming that $A^H=A$ is almost immediate. We will use the computer below to compute the eigenvalues and eigenvectors of $A\\text{,}$ but it's useful to attempt this at least once by hand. We have<\/p><div class=\"displaymath\">\n\\begin{align*}\n\\det(zI-A) \\amp = \\det\\bbm z-4 \\amp -3+i\\\\-3-i\\amp z-1\\ebm\\\\\n\\amp (z-4)(z-1)-(-3-i)(-3+i)\\\\\n\\amp z^2-5z+4-10\\\\\n\\amp (z+1)(z-6)\\text{,}\n\\end{align*}\n<\/div><p data-braille=\"continuation\">so the eigenvalues are $\\lambda_1=-1$ and $\\lambda_2=6\\text{,}$ which are both real, as expected.<\/p><p id=\"p-565\">Finding eigenvectors can seem trickier than with real numbers, mostly because it is no longer immediately apparent when one row or a matrix is a multiple of another. But we know that the rows of $A-\\lambda I$ must be parallel for a $2\\times 2$ matrix, which lets proceed nonetheless.<\/p><p id=\"p-566\">For $\\lambda_1=-1\\text{,}$ we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA + I =\\bbm 5 \\amp 3-i\\\\3+i\\amp 2\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">There are two ways one can proceed from here. We could use row operations to get to the reduced row-echelon form of $A\\text{.}$ If we take this approach, we multiply row 1 by $\\frac15\\text{,}$ and then take $-3-i$ times the new row 1 and add it to row 2, to create a zero, and so on.<\/p><p id=\"p-567\">Easier is to realize that if we haven't made a mistake calculating our eigenvalues, then the above matrix can't be invertible, so there must be some nonzero vector in the kernel. If $(A+I)\\bbm a\\\\b\\ebm=\\bbm0\\\\0\\ebm\\text{,}$ then we must have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n5a+(3-i)b=0\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">when we multiply by the first row of $A\\text{.}$ This suggests that we take $a=3-i$ and $b=-5\\text{,}$ to get $\\zz = \\bbm 3-i\\\\-5\\ebm$ as our first eigenvector. To make sure we've done things correctly, we multiply by the second row of $A+I\\text{:}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\n(3+i)(3-i)+2(-5) = 10-10 = 0\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">Success! Now we move onto the second eigenvalue.<\/p><p id=\"p-568\">For $\\lambda_2=6\\text{,}$ we get<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA-6I = \\bbm -2\\amp 3-i\\\\3+i\\amp -5\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">If we attempt to read off the answer like last time, the first row of $A-6I$ suggests the vector $\\ww = \\bbm 3-i\\\\2\\ebm\\text{.}$ Checking the second row to confirm, we find:<\/p><div class=\"displaymath\">\n\\begin{equation*}\n(3+i)(3-i)-5(2) = 10-10=0\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">as before.<\/p><p id=\"p-569\">Finally, we note that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\langle \\zz, \\ww\\rangle = (3-i)\\overline{(3-i)}+(-5)(2) = (3-i)(3+i)-10 = 0\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so the two eigenvectors are orthogonal, as expected. We have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\len{\\zz}=\\sqrt{10+25}=\\sqrt{35} \\quad \\text{ and } \\quad \\len{\\ww}=\\sqrt{10+4}=\\sqrt{14}\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so our orthogonal matrix is<\/p><div class=\"displaymath\">\n\\begin{equation*}\nU = \\bbm \\frac{3-i}{\\sqrt{35}}\\amp \\frac{3-i}{\\sqrt{14}}\\\\-\\frac{5}{\\sqrt{35}}\\amp \\frac{2}{\\sqrt{14}}\\ebm\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">With a bit of effort, we can finally confirm that<\/p><div class=\"displaymath\">\n\\begin{equation*}\nU^HAU = \\bbm -1\\amp 0\\\\0\\amp 6\\ebm\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">as expected.<\/p><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-570\">To do the above exercise using SymPy, we first define $A$ and ask for the eigenvectors.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A = Matrix(2,2,[4,3-I,3+I,1])\nA.eigenvects()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-571\">We can now manually determine the matrix $U\\text{,}$ as we did above, and input it:<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["U = Matrix([[(3-I)\/sqrt(35),(3-I)\/sqrt(14)],\n            [-5\/sqrt(35),2\/sqrt(14)]])"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-572\">To confirm it's unitary, add the line <code class=\"code-inline tex2jax_ignore\">U\\*U.H<\/code> to the above, and confirm that you get the identity matrix as output. You might need to use <code class=\"code-inline tex2jax_ignore\">simplify(U\\*U.H)<\/code> if the result is not clear. Now, to confirm that $U^HAU$ really is diagonal, go back to the cell above, and enter it. Try <code class=\"code-inline tex2jax_ignore\">(U.H)\\*A\\*U<\/code>, just to remind yourself that adding the <code class=\"code-inline tex2jax_ignore\">simplify<\/code> command is often a good idea.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-573\">If you want to cut down on the manual labour involved, we can make use of some of the other tools SymPy provides. In the next cell, we're going to assign the output of <code class=\"code-inline tex2jax_ignore\">A.eigenvects()<\/code> to a list. The only trouble is that the output of the eigenvector command is a list of lists. Each list item is a list <code class=\"code-inline tex2jax_ignore\">(eigenvalue, multiplicity, [eigenvectors])<\/code>.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["L = A.eigenvects()\nL"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-574\">Try the above modifications, in sequence. First, replacing the second line by <code class=\"code-inline tex2jax_ignore\">L[0]<\/code> will give the first list item, which is another list:<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\n\\left(-1,1,\\left[\\bbm -\\frac35+\\frac{i}{5}\\ebm\\right]\\right)\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">We want the third item in the list, so try <code class=\"code-inline tex2jax_ignore\">(L[0])[2]<\/code>. But note the extra set of brackets! There could (in theory) be more than one eigenvector, so this is a list with one item. To finally get the vector out, try <code class=\"code-inline tex2jax_ignore\">((L[0])[2])[0]<\/code>. (There is probably a better way to do this. Someone who is more fluent in Python is welcome to advise.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-575\">Now that we know how to extract the eigenvectors, we can normalize them, and join them to make a matrix. The norm of a vector is simnply <code class=\"code-inline tex2jax_ignore\">v.norm()<\/code>, and to join column vectors <code class=\"code-inline tex2jax_ignore\">u1<\/code> and <code class=\"code-inline tex2jax_ignore\">u2<\/code> to make a matrix, we can use the command <code class=\"code-inline tex2jax_ignore\">u1.row\\_join(u2)<\/code>. We already defined the matrix <code class=\"code-inline tex2jax_ignore\">A<\/code> and list <code class=\"code-inline tex2jax_ignore\">L<\/code> above, but here is the whole routine in one cell, in case you didn't run all the cells above.<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA = Matrix(2,2,[4,3-I,3+I,1])\nL = A.eigenvects()\nv = ((L[0])[2])[0]\nw = ((L[1])[2])[0]\nu1 = (1\/v.norm())*v\nu2 = (1\/w.norm())*w\nU = u1.row_join(u2)\nu1, u2, U, simplify(U.H*A*U)"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-576\">Believe me, you want the simplify command on that last matrix.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-577\">While <a href=\"sec-complex.ipynb#thm-complex-spectral\" class=\"internal\" title=\"Theorem 4.4.10: Spectral Theorem\">Theorem 4.4.10<\/a> guarantees that any hermitian matrix can be “unitarily diagonalized”, there are also non-hermitian matrices for which this can be done as well. A classic example of this is given in Nicholson's book, so we do not repeat the details here: the matrix $\\bbm 0\\amp 1\\\\-1\\amp 0\\ebm$ is a real matrix with complex eigenvalues $\\pm i\\text{,}$ and while it is neither symmetric nor hermitian, it can be orthogonally diagonalized. This should be contrasted with the real spectral theorem, where any matrix that can be orthogonally diagonalized is necessarily symmetric.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-578\">This suggests that perhaps hermitian matrices are not quite the correct class of matrix for which the spectral theorem should be stated. Indeed, it turns out there is a somewhat more general class of matrix: the <em class=\"emphasis\">normal<\/em> matrices.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition definition-like\" id=\"def-normal-matrix\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Definition<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.12<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-579\">An $n\\times n$ matrix $A$ is <dfn class=\"terminology\">normal<\/dfn> if $A^HA = AA^H\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-580\">It turns out that a matrix $A$ is normal if and only if $A=UDU^H$ for some unitary matrix $U$ and diagonal matrix $D\\text{.}$ A further generalization is known as <em class=\"emphasis\">Schur's Theorem<\/em>.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-schurr\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.13<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-581\">For <em class=\"emphasis\">any<\/em> complex $n\\times n$ matrix $A\\text{,}$ there exists a unitary matrix $U$ such that $U^HAU = T$ is upper-triangular, and such that the diagonal entries of $T$ are the eigenvalues of $A\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" id=\"p-582\">Using Schur's Theorem, we can obtain a famous result, known as the Cayley-Hamilton Theorem, for the case of complex matrices. (It is true for real matrices as well, but we don't yet have the tools to prove it.) The Cayley-Hamilton Theorem states that substituting any matrix into its characteristic polynomial results in the zero matrix. To understand this result, we should first explain how to define a polynomial of a matrix.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-583\">Given a polynomial $p(x) = a_0+a_1x+\\cdots + a_nx_n\\text{,}$ we define $p(A)$ as<\/p><div xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"displaymath\">\n\\begin{equation*}\np(A) = a_0I+a_1A+\\cdots + a_nA^n\\text{.}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">(Note the presence of the identity matrix in the first term, since it does not make sense to add a scalar to a matrix.) Note further that since $(P^{-1}AP)^n = P^{-1}A^nP$ for any invertible matrix $P$ and positive integer $n\\text{,}$ we have $p(U^HAU)=U^Hp(A)U$ for any polynomial $p$ and unitary matrix $U\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem theorem-like\" id=\"thm-cayley-hamilton-c\"><h6 xmlns:svg=\"http:\/\/www.w3.org\/2000\/svg\" class=\"heading\"><span class=\"type\">Theorem<\/span><span class=\"space\"> <\/span><span class=\"codenumber\">4.4.14<\/span><span class=\"period\">.<\/span><\/h6><p id=\"p-584\">Let $A$ be an $n\\times n$ complex matrix, and let $c_A(x)$ denote the characteristic polynomial of $A\\text{.}$ Then we have $c_A(A)=0\\text{.}$<\/p><\/article><article class=\"proof\" id=\"proof-24\"><h6 class=\"heading\"><span class=\"type\">Proof<span class=\"period\">.<\/span><\/span><\/h6><p id=\"p-585\">By <a href=\"sec-complex.ipynb#thm-schurr\" class=\"internal\" title=\"Theorem 4.4.13\">Theorem 4.4.13<\/a>, there exists a unitary matrix $U$ such that $A = UTU^H\\text{,}$ where $T$ is upper triangular, and has the eigenvalues of $A$ as diagonal entries. Since $c_A(A)=c_A(UTU^H)=Uc_A(T)U^H\\text{,}$ and $c_A(x)=c_T(x)$ (since $A$ and $T$ are similar) it suffices to show that $c_A(A)=0$ when $A$ is upper-triangular. (If you like, we are showing that $C_T(T)=0\\text{,}$ and deducing that $c_A(A)=0\\text{.}$) But if $A$ is upper-triangular, so is $xI_A\\text{,}$ and therefore, $\\det(xI-A)$ is just the product of the diagonal entries. That is,<\/p><div class=\"displaymath\">\n\\begin{equation*}\nc_A(x) = (x-\\lambda_1)(x-\\lambda_2)\\cdots (x-\\lambda_n)\\text{,}\n\\end{equation*}\n<\/div><p data-braille=\"continuation\">so<\/p><div class=\"displaymath\">\n\\begin{equation*}\nc_A(A) = (A-\\lambda_1I)(A-\\lambda_2I)\\cdots (A-\\lambda_nI)\\text{.}\n\\end{equation*}\n<\/div><p id=\"p-586\">Since the first column of $A$ is $\\bbm \\lambda_1\\amp 0 \\amp \\cdots \\amp 0\\ebm^T\\text{,}$ the first column of $A-\\lambda_1I$ is identically zero. The second column of $A-\\lambda_2I$ similarly has the form $\\bbm k\\amp 0\\amp\\cdots\\amp 0\\ebm$ for some number $k\\text{.}$<\/p><p id=\"p-587\">It follows that the first two columns of $(A-\\lambda_1I)(A-\\lambda_2I)$ are identically zero. Since only the first two entries in the third column of $(A-\\lambda_3I)$ can be nonzero, we find that the first three columns of $(A-\\lambda_1I)(A-\\lambda_2I)(A-\\lambda_3I)$ are zero, and so on.<\/p><\/article><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-complex.ipynb"}
}