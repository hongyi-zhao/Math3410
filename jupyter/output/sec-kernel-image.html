<!DOCTYPE html SYSTEM "about:legacy-compat">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-10-07T23:53:42-06:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Kernel and Image</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script src="https://sagecell.sagemath.org/embedded_sagecell.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script>$(function () {
    // Make *any* div with class 'sagecell-sage' an executable Sage cell
    // Their results will be linked, only within language type
    sagecell.makeSagecell({inputLocation: 'div.sagecell-sage',
                           linked: true,
                           languages: ['sage'],
                           evalButtonText: 'Evaluate (Sage)'});
});
</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.12/pretext.js"></script><script src="https://pretextbook.org/js/0.12/pretext_add_on.js"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/toc.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/colors_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/setcolors.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.2/features.css" rel="stylesheet" type="text/css">
<script>var logged_in = false;
var role = 'student';
var guest_access = true;
var login_required = false;
var js_version = 0.12;
</script>
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(\newcommand{\spn}{\operatorname{span}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Img}{\operatorname{im}}
\newcommand{\nll}{\operatorname{null}}
\newcommand{\csp}{\operatorname{col}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="linalg-notes-with-computations.html"><span class="title">Lecture Notes for Math 3410, with Computational Examples</span></a></h1>
<p class="byline">Sean Fitzpatrick</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="sec-lin-tran-intro.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="ch-linear-trans.html" title="Up">Up</a><span id="nextbutton" class="next-button button toolbar-item disabled">Next</span></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="sec-lin-tran-intro.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="ch-linear-trans.html" title="Up">Up</a><span class="next-button button toolbar-item disabled">Next</span>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link">
<a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a><ul><li><a href="preface-1.html" data-scroll="preface-1">Preface</a></li></ul>
</li>
<li class="link">
<a href="ch-computation.html" data-scroll="ch-computation"><span class="codenumber">1</span> <span class="title">Computational Tools</span></a><ul>
<li><a href="section-jupyter.html" data-scroll="section-jupyter">Jupyter</a></li>
<li><a href="sec-python-basics.html" data-scroll="sec-python-basics">Python basics</a></li>
<li><a href="sec-sympy.html" data-scroll="sec-sympy">SymPy for linear algebra</a></li>
</ul>
</li>
<li class="link">
<a href="ch-vector-space.html" data-scroll="ch-vector-space"><span class="codenumber">2</span> <span class="title">Vector spaces</span></a><ul>
<li><a href="sec-vec-sp.html" data-scroll="sec-vec-sp">Abstract vector spaces</a></li>
<li><a href="sec-span.html" data-scroll="sec-span">Span</a></li>
<li><a href="sec-independence.html" data-scroll="sec-independence">Linear Independence</a></li>
<li><a href="sec-dimension.html" data-scroll="sec-dimension">Basis and dimension</a></li>
</ul>
</li>
<li class="link">
<a href="ch-linear-trans.html" data-scroll="ch-linear-trans"><span class="codenumber">3</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="sec-lin-tran-intro.html" data-scroll="sec-lin-tran-intro">Definition and examples</a></li>
<li><a href="sec-kernel-image.html" data-scroll="sec-kernel-image" class="active">Kernel and Image</a></li>
</ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="sec-kernel-image"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">3.2</span> <span class="title">Kernel and Image</span>
</h2>
<a href="sec-kernel-image.html" class="permalink">Â¶</a><p id="p-187">Given any linear transformation \(T:V\to W\) we can associate two important subspaces: the <dfn class="terminology">kernel</dfn> of \(T\) (also known as the <dfn class="terminology">nullspace</dfn>), and the <dfn class="terminology">image</dfn> of \(T\) (also known as the <dfn class="terminology">range</dfn>).</p>
<article class="definition-like" id="def-kernel-image"><h6 class="heading">
<span class="type">Definition</span> <span class="codenumber">3.2.1</span>.</h6>
<p id="p-188">Let \(T:V\to W\) be a linear transformation. The <dfn class="terminology">kernel</dfn> of \(T\text{,}\) denoted \(\ker T\text{,}\) is defined by</p>
<div class="displaymath">
\begin{equation*}
\ker T = \{\vec{v}\in V \,|\, T(\vec{v}=\vec{0})\}\text{.}
\end{equation*}
</div>
<p>The <dfn class="terminology">image</dfn> of \(T\text{,}\) denoted \(\Img T\text{,}\) is defined by</p>
<div class="displaymath">
\begin{equation*}
\Img T = \{(T\vec{v}) \,|\, \vec{v}\in V\}\text{.}
\end{equation*}
</div></article><p id="p-189">Note that the kernel of \(T\) is just the set of all vectors \(T\) sends to zero. The image of \(T\) is the range of \(T\) in the usual sense of the range of a function.</p>
<article class="theorem-like" id="thm-ker-img-subspace"><h6 class="heading">
<span class="type">Theorem</span> <span class="codenumber">3.2.2</span>.</h6>
<p id="p-190">For any linear transformation \(T:V\to W\text{,}\)</p>
<ol class="decimal">
<li id="li-36"><p id="p-191">\(\ker T\) is a subspace of \(V\text{.}\)</p></li>
<li id="li-37"><p id="p-192">\(\Img T\) is a subspace of \(W\text{.}\)</p></li>
</ol></article><article class="hiddenproof" id="proof-8"><a data-knowl="" class="id-ref" data-refid="hk-proof-8"><h6 class="heading"><span class="type">Proof.</span></h6></a></article><div id="hk-proof-8" class="hidden-content tex2jax_ignore"><article class="hiddenproof"><ol id="p-193" class="decimal">
<li id="li-38">
<p id="p-194">To show that \(\ker T\) is a subspace, first note that \(\vec{0}\in \ker T\text{,}\) since \(T(\vec{0})=\vec{0}\) for any linear transformation \(T\text{.}\) If \(\vec{v},\vec{w}\in \ker T\text{,}\) then \(T(\vec{v})=\vec{0}\) and \(T(\vec{w})=0\text{,}\) and therefore,</p>
<div class="displaymath">
\begin{equation*}
T(\vec{v}+\vec{w})=T(\vec{v})+T(\vec{w})=\vec{0}+\vec{0}=\vec{0}\text{.}
\end{equation*}
</div>
<p>Similarly, for any scalar \(c\) and \(\vec{v}\in \ker T\text{,}\)</p>
<div class="displaymath">
\begin{equation*}
T(c\vec{v})=cT(\vec{v})=c\vec{0}=\vec{0}\text{.}
\end{equation*}
</div>
<p>By the subspace test, \(\ker T\) is a subspace.</p>
</li>
<li id="li-39">
<p id="p-195">Again, since \(T(\vec{0})=\vec{0}\text{,}\) we see that \(\vec{0}\in \Img T\text{,}\) so \(\Img T\) is nonempty. If \(\vec{w}_1,\vec{w}_2\in \Img T\text{,}\) then there exist \(\vec{v}_1,\vec{v}_2\in V\) such that \(T(\vec{v}_1)=\vec{w}_1\) and \(T(\vec{v}_2)=\vec{w}_2\text{.}\) It follows that</p>
<div class="displaymath">
\begin{equation*}
\vec{w}_1+\vec{w}_2 = T(\vec{v}_1)+T(\vec{v}_2) = T(\vec{v}_1+\vec{v}_2)\text{,}
\end{equation*}
</div>
<p>so \(\vec{w}_1+\vec{w}_2\in \Img T\text{.}\) Similarly, if \(c\) is any scalar and \(\vec{w}=T(\vec{v})\in\Img T\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
c\vec{w}=cT(\vec{v})=T(c\vec{v})\text{,}
\end{equation*}
</div>
<p>so \(c\vec{w}\in \Img T\text{.}\)</p>
</li>
</ol></article></div>
<p id="p-196">A familiar setting that you may already have encontered in a previous linear algebra course is that of a matrix transformation. Let \(A\) be an \(m\times n\) matrix. Then we can define \(T:\R^n\to \R^m\) by \(T(\vec{x})=A\vec{x}\text{,}\) where elements of \(\R^n,\R^m\) are considered as column vectors. We then have</p>
<div class="displaymath">
\begin{equation*}
\ker T = \nll(A) = \{\vec{x}\in \R^n \,|\, A\vec{x}=\vec{0}\}
\end{equation*}
</div>
<p>and</p>
<div class="displaymath">
\begin{equation*}
\Img T = \csp(A) = \{A\vec{x}\,|\, \vec{x|\in \R^n}\text{,}
\end{equation*}
</div>
<p>where \(\csp(A)\) denotes the <dfn class="terminology">column space</dfn> of \(A\text{.}\) Recall further that if we write \(A\) in terms of its columns as</p>
<div class="displaymath">
\begin{equation*}
A = \bbm C_1 \amp C_2 \amp \cdots \amp C_n\ebm
\end{equation*}
</div>
<p>and a vector \(\vec{x}\in \R^n\) as \(\vec{x}=\bbm x_1\\x_2\\\vodts \\x_n\ebm\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
A\vec{x} = x_1C_1+x_2C_2+\cdots +x_nC_n\text{.}
\end{equation*}
</div>
<p>Thus, any element of \(\csp(A)\) is a linear combination of its columns, explaining the name <em class="emphasis">column space</em>.</p>
<p id="p-197">Determining \(\nll(A)\) and \(\csp(A)\) for a given matrix \(A\) is, unsurprisingly, a matter of reducing \(A\) to row-echelon form. Finding \(\nll(A)\) is simply a matter of describing the set of all solutions to the homogeneous system \(A\vec{x}=\vec{0}\text{.}\) Finding \(\csp(A)\) relies on the following theorem.</p>
<article class="theorem-like" id="thm-colspace"><h6 class="heading">
<span class="type">Theorem</span> <span class="codenumber">3.2.3</span>.</h6>
<p id="p-198">Let \(A\) be an \(m\times n\) matrix with columns \(C_1,C_2,\ldots, C_n\text{.}\) If the reduced row-echelon form of \(A\) has leading ones in columns \(j_1,j_2,\ldots, j_k\text{,}\) then \(\{C_{j_1},C_{j_2},\ldots, c_{j_k}\}\) is a basis for \(\csp(A)\text{.}\)</p></article><p id="p-199">The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson. To see why it works, we need to remember a few basic facts from elementary linear algebra. First, recall that performing an elementary row operation on a matrix \(A\) is equivalent to multiplying on the left by an elementary matrix \(E\) defined using the same row operation.</p>
<p id="p-200">Since every elementary matrix is invertible, and any product of invertible matrices is invertible, and we can transform \(A\) into a row-echelon matrix \(R\) using elementary row operations, it follows that \(R = UA\) for an invertible matrix \(U\text{;}\) indeed, we have \(U = E_kE_{k-1}\cdots E_2E_1\text{,}\) where \(E_1,\ldots, E_k\) are the elemetnary matrices corresponding to the row operations used to carry \(A\) to \(R\text{.}\)</p>
<p id="p-201">A basis for \(\csp(R)\) is given by the columns of \(R\) containing the leading ones. The reason for this is as follows. First, recall that each nonzero row begins with a leading one. So if the leading ones of \(R\) are in columns \(i_1,\ldots, i_k\text{,}\) then there are \(k\) nonzero rows. Since all rows of zeros go at the bottom, each column in \(R\) has its last \(m-k\) entries identically zero. Thus,</p>
<div class="displaymath">
\begin{equation*}
\csp(R)\subseteq \left\{\bbm a_1\\\vdots \\a_k\\0\\\vdots 0\ebm\in \R^m \,|\, a_1,\ldots, a_k\in\R\right\}\text{,}
\end{equation*}
</div>
<p>so \(\dim \csp(R)\leq k\text{.}\) But the columns containing leading ones are easily shown to be independent, so they form a basis of \(\csp(R)\text{,}\) which therefore has dimension \(k=\operatorname{rank}(A)\text{.}\)</p>
<p id="p-202">Next, since \(R=UA\text{,}\) where \(U\) is invertible, if \(R=\bbm Y_1\amp Y_2\amp \cdots \amp Y_n\ebm\) and \(A = \bbm C_1\amp C_2\amp \cdots \amp C_n\ebm\text{,}\) then \(C_i = U^{-1}Y_i\) for each \(i\text{.}\) It follows from the fact that \(U\) is invertible and that the columns containing leading ones in \(R\) form a basis for \(\csp(R)\) that the corresponding columns in \(A\) form a basis for \(\csp(A)\text{.}\) (For details, see Section 5.4 in Nicholson.)</p>
<p id="p-203">For example, consider the linear transformation \(T:\R^4\to \R^3\) defined by the matrix</p>
<div class="displaymath">
\begin{equation*}
A = \bbm 1 \amp 3 \amp 0 \amp -2\\
-2 \amp -1 \amp 2 \amp 0\\
1 \amp 8 \amp 2 \amp -6\ebm\text{.}
\end{equation*}
</div>
<p>Let's determine the RREF of \(A\text{:}\)</p>
<div class="sagecell-sage" id="sage-46"><script type="text/x-sage">from sympy import *
init_printing()
A=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])
A.rref()
</script></div>
<p id="p-204">We see that there are leading ones in the first and second column. Therefore, \(\csp(A) = \Img(T) = \spn\left\{\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right\}\text{.}\) Indeed, note that</p>
<div class="displaymath">
\begin{equation*}
\bbm 0\\2\\2\ebm = -\frac65\bbm 1\\-2\\1\ebm + \frac25\bbm 3\\-1\\8\ebm
\end{equation*}
</div>
<p>and</p>
<div class="displaymath">
\begin{equation*}
\bbm -2\\0\\-6\ebm = \frac25\bbm 1\\-2\1\ebm -\frac45\bbm 3\\-1\\8\ebm\text{,}
\end{equation*}
</div>
<p>so that indeed, the third and fourth columns are in the span of the first and second.</p>
<p id="p-205">Furthermore, we can determine the nullspace: if \(A\vec{x}=\vec{0}\) where \(\vec{x}=\bbm x_1\\x_2\\x_3\\x_4\ebm\text{,}\) then we must have</p>
<div class="displaymath">
\begin{align*}
x_1 \amp =\frac65 x_3-\frac25 x_4\\
x_2 \amp =-\frac25 x_3+\frac 45 x_4\text{,}
\end{align*}
</div>
<p>so</p>
<div class="displaymath">
\begin{equation*}
\vec{x} = \bbm \frac65x_3-\frac25x_4\\ -\frac25x_3+\frac45x_4\\x_3\\x_4\ebm = \frac{x_3}{5}\bbm 6\\-2\\5\\0\ebm + \frac{x_4}{5}\bbm -2\\4\\0\\5\ebm\text{.}
\end{equation*}
</div>
<p>It follows that a basis for \(\nll(A)=\ker T\) is \(\left\{\bbm 6\\-2\\5\\0\ebm, \bbm -2\\4\\0\\5\ebm\right\}\text{.}\)</p>
<p id="p-206">Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space. But it's probably worth your while to know how to determine these from the RREF of a matrix, without additional help from the computer. That said, let's see how the computer's output compares to what we found:</p>
<div class="sagecell-sage" id="sage-47"><script type="text/x-sage">A.nullspace()
</script></div>
<div class="sagecell-sage" id="sage-48"><script type="text/x-sage">A.columnspace()
</script></div>
<p id="p-207">Note that the output from the computer simply states the basis for each space. Of course, for computational purposes, this is typically good enough.</p>
<p id="p-208">An important result that comes out while trying to show that the âpivot columnsâ of a matrix (the ones that end up with leading ones in the RREF) are a basis for the column space is that the column rank (defined as the dimesion of \(\csp(A)\)) and the row rank (the dimension of the space spanned by the columns of \(A\)) are equal. One can therefore speak unabmiguously about the <dfn class="terminology">rank</dfn> of a matrix \(A\text{,}\) and it is just as it's defined in a first course in linear algebra: the number of leading ones in the RREF of \(A\text{.}\)</p>
<p id="p-209">For a general linear transformation, we can't necessarily speak in terms of rows and columns, but if \(T:V\to W\) is linear, and either \(V\) or \(W\) is finite-dimensional, then we can define the rank of \(T\) as follows.</p>
<article class="definition-like" id="def-rank-transformation"><h6 class="heading">
<span class="type">Definition</span> <span class="codenumber">3.2.4</span>.</h6>
<p id="p-210">Let \(T:V\to W\) be a linear transformation. Then the <dfn class="terminology">rank</dfn> of \(T\) is defined by</p>
<div class="displaymath">
\begin{equation*}
\operatorname{rank} T = \dim \Img T\text{,}
\end{equation*}
</div>
<p>and the <dfn class="terminology">nullity</dfn> of \(T\) is defined by</p>
<div class="displaymath">
\begin{equation*}
\operatorname{nullity} T = \dim \ker T\text{.}
\end{equation*}
</div></article><p id="p-211">Note that if \(W\) is finite-dimensional, then so is \(\Img T\text{,}\) since it's a subspace of \(W\text{.}\) On the other hand, if \(V\) is finite-dimensional, then we can find a basis \(\{\vec{v}_1,\ldots, \vec{v}_n}}\) of \(V\text{,}\) and the set \(\{T(\vec{v}_1),\ldots, T(\vec{v}_n\})\}\) will span \(\Img T\text{,}\) so again the image is finite-dimensional, so the rank of \(T\) is finite. It is possible for either the rank or the nullity of a transformation to be infinite.</p>
<p id="p-212">Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces. From the textbook, we have the following nice example.</p>
<article class="example-like" id="example-2"><h6 class="heading">
<span class="type">Example</span> <span class="codenumber">3.2.5</span>.</h6>
<p id="p-213">Let \(T:M_{nn}\to M_{nn}\) be defined by \(T(A)=A-A^T\text{.}\) Then</p>
<ol class="decimal">
<li id="li-40"><p id="p-214">\(T\) is a linear map.</p></li>
<li id="li-41"><p id="p-215">\(\ker T\) is equal to the set of all symmetric matrices.</p></li>
<li id="li-42"><p id="p-216">\(\Img T\) is equal to the set of all skew-symmetric matrices.</p></li>
</ol>
<div class="solutions">
<a data-knowl="" class="id-ref" data-refid="hk-solution-8" id="solution-8"><span class="type">Solution</span></a><div id="hk-solution-8" class="hidden-content tex2jax_ignore"><div class="solution"><ol id="p-217" class="decimal">
<li id="li-43">
<p id="p-218">We have \(T(0)=0\) since \(0^T=0\text{.}\) Using proerties of the transpose and matrix algebra, we have</p>
<div class="displaymath">
\begin{equation*}
T(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)
\end{equation*}
</div>
<p>and</p>
<div class="displaymath">
\begin{equation*}
T(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)\text{.}
\end{equation*}
</div>
</li>
<li id="li-44"><p id="p-219">It's clear that if \(A^T=A\text{,}\) then \(T(A)=0\text{.}\) On the other hand, if \(T(A)=0\text{,}\) then \(A-A^T=0\text{,}\) so \(A=A^T\text{.}\) Thus, the kernel consists of all symmetric matrices.</p></li>
<li id="li-45">
<p id="p-220">If \(B=T(A)=A-A^T\text{,}\) then</p>
<div class="displaymath">
\begin{equation*}
B^T = (A-A^T)^T = A^T-A = -B\text{,}
\end{equation*}
</div>
<p>so certainly every matrix in \(\Img A\) is skew-symmetric. On the other hand, if \(B\) is skew-symmetric, then \(B=T(\frac12 B)\text{,}\) since</p>
<div class="displaymath">
\begin{equation*}
T\Bigl(\frac12 B\Bigr) = \frac12 T(B) = \frac12(B-B^T) = \frac12(B-(-B))= B\text{.}
\end{equation*}
</div>
</li>
</ol></div></div>
</div></article><p id="p-221">You'll recall from a course like Math 2000 that in the study of functions, the properties of being injective (one-to-one) and surjective (onto) are important. They're important for linear transformations as well, and defined in exactly the same way.</p>
<p id="p-222">It's clear that being surjective is closely tied to image. Indeed, by definition, \(T:V\to W\) is onto if \(\Img T = W\text{.}\) What might not be immediately obvious is that the kernel tells us if a linear map is injective.</p>
<article class="theorem-like" id="thm-injective-kernel"><h6 class="heading">
<span class="type">Theorem</span> <span class="codenumber">3.2.6</span>.</h6>
<p id="p-223">Let \(T:V\to W\) be a linear transformation. Then \(T\) is injective if and only if \(\ker T = \{\vec{0}\}\text{.}\)</p></article><article class="hiddenproof" id="proof-9"><a data-knowl="" class="id-ref" data-refid="hk-proof-9"><h6 class="heading"><span class="type">Proof.</span></h6></a></article><div id="hk-proof-9" class="hidden-content tex2jax_ignore"><article class="hiddenproof"><p id="p-224">Suppose \(T\) is injective, and let \(\vec{v}\in \ker T\text{.}\) Then \(T(\vec{v})=\vec{0}\text{.}\) On the other hand, we know that \(T(\vec{0})=\vec{0}\text{,}\) and since \(T\) is injective, we must have \(\vec{v}=\vec{0}\text{.}\) Conversely, suppose that \(\ker T = \{0\}\) and that \(T(\vec{v}_1)=T(\vec{v}_2)\) for some \(\vec{v}_1,\vec{v}_2\in V\text{.}\) Then</p>
<div class="displaymath">
\begin{equation*}
\vec{0} = T(\vec{v}_1)-T(\vec{v}_2) = T(\vec{v}_1-\vec{v}_2)\text{,}
\end{equation*}
</div>
<p>so \(\vec{v}_1-\vec{v}_2\in \ker T\text{.}\) Therefore, we must have \(\vec{v}_1-\vec{v}_2=\vec{0}\text{,}\) so \(\vec{v}_1=\vec{v}_2\text{,}\) and it follows that \(T\) is injective.</p></article></div>
<p id="p-225">Let us return to the case of a matrix transformation \(T_A:\R^n\to \R^m\text{.}\) Notice that \(\ker T_A\) is simply the set of all solutions to \(A\vec{x}=\vec{0}\text{,}\) while \(\Img T_A\) is the set of all \(\vec{y}\in\R^m\) for which \(A\vec{x}=\vec{y}\) <em class="emphasis">has</em> a solution.</p>
<p id="p-226">Recall from the discussion above that \(\rank A = \dim \csp(A) = \dim \Img T_A\text{.}\) It follows that \(T_A\) is surjective if and only if \(\rank A = m\text{.}\) On the other hand, \(T_A\) is injective if and only if \(\rank A = n\text{,}\) because we know that the system \(A\vec{x}=\vec{0}\) has a unique solution if and only if each column of \(A\) contains a leading one.</p>
<p id="p-227">This has some interesting consequences. If \(m=n\) (that is, if \(A\) is square), then each increase in \(\dim \nll(A)\) produces a corresponding decrease in \(\dim \csp(A)\text{,}\) since both correspond to the âlossâ of a leading one. Moreover, if \(\rank A = n\text{,}\) then \(T_A\) is both injective and surjective. Recall that a function is invertible if and only if it is both injective and surjective. It should come as no surprise that invertibility of \(T_A\) (as a function) is equivalent to invertibility of \(A\) (as a matrix).</p>
<p id="p-228">Also, note that if \(m \lt n\text{,}\) then \(\rank A\leq m \lt n\text{,}\) so \(T_A\) could be surjective, but can't possibly be injective. On the other hand, if \(m\gt n\text{,}\) then \(\rank A\leq n \lt m\text{,}\) so \(T_A\) could be injective, but can't possibly be surjective. These results generalize to linear transformations between any finite-dimensional vector spaces. The first step towards this is the following fundamental theorem.</p>
<article class="theorem-like" id="thm-dimension-lintrans"><h6 class="heading">
<span class="type">Theorem</span> <span class="codenumber">3.2.7</span>. <span class="title">Dimension Theorem (Fundamental Theorem of Linear Transformations).</span>
</h6>
<p id="p-229">Let \(T:V\to W\) be any linear transformation such that \(\ker T\) and \(\Img T\) are finite-dimensional. Then \(V\) is finite-dimensional, and</p>
<div class="displaymath">
\begin{equation*}
\dim V = \dim \ker T + \dim \Img T\text{.}
\end{equation*}
</div></article><p id="p-230">This is sometimes known as the <em class="emphasis">Rank-Nullity Theorem</em>, since it can be stated in the form</p>
<div class="displaymath">
\begin{equation*}
\dim V = \rank T + \operatorname{nullity} T\text{.}
\end{equation*}
</div></section></div></main>
</div>
<div class="login-link"><span id="loginlogout" class="login">login</span></div>
<script src="https://pretextbook.org/js/0.12/login.js"></script>
</body>
</html>
