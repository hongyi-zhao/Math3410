<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-orthogonality">
  <title>Orthogonality and Applications</title>
  <section xml:id="sec-orthogonal-sets">
    <title>Orthogonal sets of vectors</title>
    <introduction>
      <p>
        You may recall from elementary linear algebra, or a calculus class,
        that vectors in <m>\R^2</m> or <m>\R^3</m> are considered to be quantities with both <em>magnitude</em> and <em>direction</em>.
        Interestingly enough, neither of these properties is inherent to a general vector space.
        The vector space axioms specify only algebra; they say nothing about geometry.
        (What, for example, should be the <q>angle</q> between two polynomials?)
      </p>

      <p>
        Because vector algebra is often introduced as a consequence of geometry (like the <q>tip-to-tail</q> rule),
        you may not have thought all that carefully about what, exactly,
        is responsible for making the connection between algebra and geometry.
        It turns out that the missing link is the humble dot product.
        This should be plausible after a bit of thought.
        After all, you probably encountered the following result, perhaps as a consequence of the law of cosines:
        for any two vectors <m>\uu,\vv\in\R^2</m>,
        <me>
          \uu\dotp\vv = \len{\uu}\,\len{\vv}\cos\theta
        </me>,
        where <m>\theta</m> is the angle between <m>\uu</m> and <m>\vv</m>.
        Here we see both magnitude and direction (encoded by the angle) defined in terms of the dot product.
      </p>

      <p>
        While it is possible to generalize the idea of the dot product to something called an <em>inner product</em>,
        we will first focus on the basic dot product in <m>\R^n</m>.
        Once we have a good understanding of things in that setting, we can move on to consider the abstract counterpart.
      </p>
    </introduction>

    <subsection xml:id="subsec-dot-basics">
      <title>Basic definitions and properties</title>
      <p>
        For most of this chapter (primarily for typographical reasons) we will denote elements of <m>\R^n</m>
        as ordered <m>n</m>-tuples <m>(x_1,\ldots, x_n)</m> rather than as column vectors.
      </p>

      <definition xml:id="def-dot-prod-norm">
        <statement>
          <p>
            Let <m>\xx=(x_1,x_2,\ldots, x_n)</m> and <m>\yy=(y_1,y_2,\ldots, y_n)</m>
            be vectors in <m>\R^n</m>. The <term>dot product</term> of <m>\xx</m> and <m>\yy</m>,
            denoted by <m>\xx\dotp\yy</m> is the scalar defined by
            <me>
              \xx\dotp \yy = x_1y_1+x_2y_2+\cdots + x_ny_n
            </me>.
            The <term>norm</term> of a vector <m>\xx</m> is denoted <m>\len{\xx}</m> and defined by
            <me>
              \len{\xx} = \sqrt{x_1^2+x_2^2+\cdots + x_n^2}
            </me>.
          </p>
        </statement>
      </definition>

      <p>
        Note that both the dot product and the norm produce <em>scalars</em>.
        Through the Pythagorean Theorem, we recognize the norm as the length of <m>\xx</m>.
        The dot product can still be thought of as measuring the angle between vectors,
        although the simple geometric proof used in two dimensions is not that easily translated to <m>n</m> dimensions.
        At the very least, the dot product lets us extend the notion of right angles to higher dimensions.
      </p>

      <definition xml:id="def-orthogonal">
        <statement>
          <p>
            We say that two vectors <m>\xx,\yy\in\R^n</m>
            are <term>orthogonal</term> if <m>\xx\dotp\yy = 0</m>.
          </p>
        </statement>
      </definition>

      <p>
        It should be no surprise that all the familiar properties of the dot product work just as well in any dimension.
        The folowing properties are all easily confirmed by routine computation.
      </p>

      <theorem xml:id="thm-dot-props">
        <statement>
          <p>
            For any vectors <m>\xx,\yy,\zz\in\R^n</m>,
            <ol>
              <li><m>\xx\dotp\yy = \yy\dotp\xx</m></li>
              <li><m>\xx\dotp(\yy+\zz)=\xx\dotp\yy+\xx\dotp\zz</m></li>
              <li>
                <p>
                  For any scalar <m>c</m>, <m>\xx\dotp(c\yy) = (c\xx)\dotp\yy=c(\xx\dotp\yy)</m>
                </p>
              </li>
              <li>
                <p>
                  <m>\xx\dotp\xx\geq 0</m>, and <m>\xx\dotp\xx=0</m> if and only if <m>\xx=\mathbf{0}</m>
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>

      <p>
        The above properties, when properly abstracted, become the defining properties of a (real) inner product.
        (A complex inner product also involves complex conjugates.)
        For a general inner product, the requirement <m>\xx\dotp\xx\geq 0</m>
        is referred to as being <em>positive-definite</em>,
        and the property that only the zero vector produces zero when dotted with itself is called <em>nondegenerate</em>.
        Note that we have the following connection between norm and dot product:
        <me>
          \len{\xx}^2 = \xx\dotp \xx
        </me>.
        For a general inner product, this can be used as a <em>definition</em> of the norm associated to an inner product.
      </p>

      <exercise>
        <statement>
          <p>
            Given that <m>\len{\xx}=3, \len{\yy}=1</m>, and <m>\xx\dotp\yy=-2</m>,
            compute <m>(4\xx-3\yy)\dotp (\xx+5\yy)</m>.
          </p>
        </statement>
        <solution>
          <p>
            Note that the distributive property, together with symmetry,
            let us handle this dot product using what is essentially <q><init>FOIL</init></q>:
            <md>
              <mrow> (4\xx-3\yy)\dotp (\xx+5\yy)\amp = (4\xx)\dotp \xx+(4\xx)\dotp(5\yy)+(-3\yy)\dotp \xx+(-3\yy)\dotp(5\yy)</mrow>
              <mrow> \amp = 4(\xx\dotp\xx)+(4\cdot 5)(\xx\dotp \yy)-3(\yy\dotp \xx)+(-3\cdot 5)(\yy\dotp\yy)</mrow>
              <mrow> \amp = 4\len{\xx}^2+20\xx\dotp\yy-3\xx\dotp\yy-15\len{\yy}^2</mrow>
              <mrow> \amp = 4(9)+17(-2)-15(1) = -13</mrow>
            </md>.
          </p>
        </solution>
      </exercise>

      <exercise xml:id="ex-norm-sum-square">
        <statement>
          <p>
            Show that for any vectors <m>\xx,\yy\in\R^n</m>, we have
            <me>
              \len{\xx+\yy}^2 = \len{\xx}^2+2\xx\dotp\yy+\len{\yy}^2
            </me>.
          </p>
        </statement>
        <solution>
          <p>
            This is simply an exercise in properties of the dot product. We have
            <md>
              <mrow>\len{\xx+\yy}^2 \amp = (\xx+\yy)\dotp (\xx+\yy) </mrow>
              <mrow> \amp = \xx\dotp \xx+\xx\dotp\yy+\yy\dotp\xx+\yy\dotp\yy</mrow>
              <mrow>  \amp =\len{\xx}^2+2\xx\dotp\yy+\len{\yy}^2</mrow>
            </md>.
          </p>
        </solution>
      </exercise>

      <exercise>
        <statement>
          <p>
            Suppose <m>\mathbb{R}^n=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}</m>.
            Prove that <m>\xx=\mathbf{0}</m> if and only if <m>\xx\dotp \vv_i=0</m> for each <m>i=1,2,\ldots, k</m>.
          </p>
        </statement>
        <solution>
          <p>
            If <m>\xx=\mathbf{0}</m>, then the result follows immediately from the dot product formula in <xref ref="def-dot-prod-norm"/>.
            Conversely, suppose <m>\xx\dotp \vv_i=0</m> for each <m>i</m>.
            Since the <m>\vv_i</m> span <m>\R^n</m>, there must exist scalars <m>c_1,c_2,\ldots, c_k</m>
            such that <m>\xx=c_1\vv_1+c_2\vv_2+\cdots+c_k\vv_k</m>. But then
            <md>
              <mrow>\xx\dotp\xx \amp = \xx\dotp (c_1\vv_1+c_2\vv_2+\cdots+c_k\vv_k) </mrow>
              <mrow> \amp = c_1(\xx\dotp \vv_1)+ c_2(\xx\dotp \vv_2)+\cdots +c_k(\xx\dotp \vv_k)</mrow>
              <mrow>  \amp = c_1(0)+c_2(0)+\cdots + c_k(0)=0</mrow>
            </md>.
          </p>
        </solution>
      </exercise>

      <p>
        There are two important inequalities associated to the dot product and norm.
        We state them both in the following theorem.
      </p>

      <theorem xml:id="thm-cauchy-triangle">
        <statement>
          <p>
            Let <m>\xx,\yy</m> be any vectors in <m>\R^n</m>.
            Then
            <ol>
              <li>
                <m>\lvert \xx\dotp \yy\rvert \leq \len{\xx}\len{\yy}</m>
              </li>
              <li>
                <m>\len{\xx+\yy}\leq \len{\xx}+\len{\yy}</m>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>

      <p>
        The first of the above inequalities is called the <em>Cauchy-Schwarz inequality</em>, which be viewed as a manifestation of the formula
        <me>
          \xx\dotp \yy = \len{\xx}\len{\yy}\cos\theta
        </me>,
        since after all, <m>\lvert \cos\theta\rvert\leq 1</m> for any angle <m>\theta</m>.
        For a direct proof, we note that for any scalars <m>r,s</m> we have
        <me>
          \len{r\xx\pm s\yy}^2 = r^2\len{\xx}^2\pm 2rs\xx\dotp\yy+s^2\len{\yy}^2
        </me>.
        Putting <m>r=\len{\yy}, s=\len{\xx}</m> gives
        <me>
          \len{s\xx\pm r\yy}^2 = 2rs(rs\pm\xx\dotp\yy)
        </me>.
        Since the left-hand side is non-negative, we must have
        <me>
          \xx\yy\leq \len{\xx}\len{\yy} \quad \text{ and } -\len{\xx}\len{\yy}\leq \xx\dotp\yy
        </me>,
        and the result follows.
      </p>

      <p>
        The second result, called the  <em>triangle inequality</em>,
        follows immediately from the Cauchy-Scwarz inequality and <xref ref="ex-norm-sum-square"/>.
        The triangle inequality gets its name from the <q>tip-to-tail</q> picture for vector addition.
        Essentially, it tells us that the length of any side of a triangle must be less than the sum of the lengths of the other two sides.
        The importance of the triangle inequality is that it tells us that the norm can be used to define distance.
      </p>

      <definition xml:id="def-vector-distance">
        <statement>
          <p>
            For any vectors <m>\xx,\yy\in \R^n</m>, the <term>distance</term>
            from <m>\xx</m> to <m>\yy</m> is denoted <m>d(\xx,\yy)</m>, and defined as
            <me>
              d(\xx,\yy) = \len{\xx-\yy}
            </me>.
          </p>
        </statement>
      </definition>

      <p>
        Using properties of the norm, we can show that this distance function meets the criteria of what's called a <em>metric</em>.
        A metric is any function that takes a pair of vectors (or points) as input, and returns a number as output,
        with the following properties:
        <ol>
          <li>
            <p>
              <m>d(\xx,\yy)=d(\yy,\xx)</m> for any <m>\xx,\yy</m>
            </p>
          </li>
          <li>
            <p>
              <m>d(\xx,\yy)\geq 0</m>, and <m>d(\xx,\yy)=0</m> if and only if <m>\xx=\yy</m>
            </p>
          </li>
          <li>
            <p>
              <m>d(\xx,\yy)\leq d(\xx,\zz)+d(\zz,\yy)</m> for any <m>\xx,\yy,\zz</m>
            </p>
          </li>
        </ol>
        We leave it as an exericse to confirm that the distance function defined above is a metric.
      </p>

      <p>
        In more advanced courses (<eg/> topology or analysis) you might go into detailed study of these structures.
        There are three interrelated structures: inner products, norms, and metrics.
        You might consider questions like: does every norm come from an inner product? Does every metric come from a norm?
        (No.) Things get even more interesting for infinite-dimensional spaces.
        Of special interest are spaces such as <em>Hilbert spaces</em> (a special type of infinite-dimensional inner product space)
        and <em>Banach spaces</em> (a special type of infinite-dimensional normed space).
      </p>
    </subsection>

    <subsection xml:id="subsec-ortho-sets">
      <title>Orthogonal sets of vectors</title>
      <p>
        In earlier chapters, we've seen that among different sets of vectors one could consider,
        independent sets and spanning sets are both worthy of study.
        One of the main themes of this chapter is that <em>orthogonal</em> sets are equally worthy,
        and in many cases, easier to work with.
      </p>

      <definition xml:id="def-ortho-set">
        <statement>
          <p>
            A set of vectors <m>\{\vv_1,\vv_2,\ldots, \vv_k\}</m> in <m>\R^n</m>
            is called <term>orthogonal</term> if:
            <ul>
              <li>
                <p>
                  <m>\vv_i\neq \mathbf{0}</m> for each <m>i=1,2\ldots, n</m>
                </p>
              </li>
              <li>
                <p>
                  <m>\vv_i\dotp\vv_j = 0</m> for all <m>i\neq j</m>
                </p>
              </li>
            </ul>
          </p>
        </statement>
      </definition>

      <exercise xml:id="ex-orthogonal-set">
        <statement>
          <p>
            Show that the following is an orthogonal subset of <m>\R^4</m>.
            <me>
              \{(1,0,1,0), (-1,0,1,1), (1,1,-1,2)\}
            </me>
            Can you find a fourth vector that is orthogonal to each vector in this set?
          </p>
        </statement>
        <solution>
          <p>
            Clearly, all three vectors are nonzero. To confirm the set is orthogonal, we simply compute dot products:
            <md>
              <mrow> (1,0,1,0)\dotp (-1,0,1,1)\amp =-1+0+1+0=0</mrow>
              <mrow> (-1,0,1,1)\dotp (1,1,-1,2)\amp =-1+0-1+2=0</mrow>
              <mrow> (1,0,1,0)\dotp (1,1,-1,2) \amp = 1+0-1+0=0</mrow>
            </md>.
          </p>

          <p>
            To find a fourth vector, we proceed as follows. Let <m>\xx=(a,b,c,d)</m>.
            We want <m>\xx</m> to be orthogonal to the three vectors in our set.
            Computing dot products, we must have:
            <md>
              <mrow>(a,b,c,d)\dotp (1,0,1,0) \amp = a+c=0 </mrow>
              <mrow>(a,b,c,d)\dotp (-1,0,1,1) \amp = -a+c+d=0 </mrow>
              <mrow>(a,b,c,d)\dotp (1,1,-1,2) \amp = a+b-c+2d=0</mrow>
            </md>.
            This is simply a homogeneous system of three equations in four variables.
            Using the Sage cell below, we find that our vector must satisfy
            <m>a=\frac12 d, b = -3d, c=-\frac12 d</m>.
            One possible nonzero solution is to take <m>d=2</m>, giving <m>\xx=(1,-6,-1,2)</m>.
            We'll leave the verification that this vector works as an easy exercise.
          </p>
        </solution>
      </exercise>

      <sage>
        <input>
          from sympy import *
          init_printing()
          A=Matrix(3,4,[1,0,1,0,-1,0,1,1,1,1,-1,2])
          A.rref()
        </input>
      </sage>

      <p>
        The requirement that the vectors in an orthogonal set be nonzero is partly because the alternative would be boring,
        and partly because it lets us state the following theorem.
      </p>

      <theorem xml:id="thm-ortho-independent">
        <statement>
          <p>
            Any orthogonal set of vectors is linearly independent.
          </p>
        </statement>
        <proof>
          <p>
            Suppose <m>S=\{\vv_1,\vv_2,\ldots, \vv_k\}</m> is orthogonal, and suppose
            <me>
              c_1\vv_1+c_2\vv_2+\cdots + c_k\vv_k = \mathbf{0}
            </me>
            for scalars <m>c_1,c_2,\ldots, c_k</m>.
            Taking the dot product of both sides of the above equation with <m>\vv_1</m> gives
            <md>
              <mrow>c_1(\vv_1\dotp \vv_1)+c_2(\vv_1\dotp \vv_2)+\cdots +c_k(\vv_1\dotp \vv_k) \amp =\vv_1\dotp \mathbf{0}</mrow>
              <mrow> c_1\len{\vv_1}^2+0+\cdots + 0\amp = 0 </mrow>
            </md>.
            Since <m>\len{\vv_1}^2\neq 0</m>, we must have <m>c_1=0</m>.
            We similarly find that all the remaining scalars are zero by taking the dot product with <m>\vv_2,\ldots, \vv_k</m>.
          </p>
        </proof>
      </theorem>

      <p>
        Another useful consequence of orthogonality: recall that in two dimensions,
        we have the Pythagorean Theorem for right-angled triangles,
        but have to settle for the Law of Cosines otherwise.
        In <m>n</m> dimensions, we have the following, which follows from the fact that all <q>cross terms</q>
        (dot products of different vectors) will vanish.
      </p>

      <theorem xml:id="thm-pythagoras">
        <statement>
          <p>
            For any orthogonal set of vectors <m>\{\xx_1,\ldots, \xx_k\}</m> we have
            <me>
              \len{\xx_1+\cdots +\xx_k}^2 = \len{\xx_1}^2+\cdots + \len{\xx_k}^2
            </me>.
          </p>
        </statement>
      </theorem>

      <p>
        Our final initial result about orthogonal sets of vectors relates to span.
        In general, we know that if <m>\yy\in\spn\{\xx_1,\ldots, \xx_k\}</m>,
        then it is possible to solve for scalars <m>c_1,\ldots, c_k</m>
        such that <m>\yy=c_1\xx_1+\cdots+ c_k\xx_k</m>.
        The trouble is that finding these scalars generally involves setting up,
        and then solving, a system of linear equations.
        The great thing about orthogonal sets of vectors is that we can provide explicit formulas for the scalars.
      </p>

      <theorem xml:id="thm-fourier-expansion">
        <title>Fourier expansion theorem</title>
        <statement>
          <p>
            Let <m>=\{\vv_1,\vv_2,\ldots, \vv_k\}</m> be an orthogonal set of vectors.
            For any <m>\yy\in \spn S</m>, we have
            <me>
              \yy = \left(\frac{\yy\dotp\mathbf{v}_1}{\vv_1\dotp\vv_1}\right)\vv_1+
              \left(\frac{\yy\dotp\mathbf{v}_2}{\vv_2\dotp\vv_2}\right)\vv_2+\cdots +
              \left(\frac{\yy\dotp\mathbf{v}_k}{\vv_k\dotp\vv_k}\right)\vv_k
            </me>.
          </p>
        </statement>
        <proof>
          <p>
            Let <m>\yy=c_1\vv_1+\cdots + c_k\vv_k</m>.
            Taking the dot product of both sides of this equation with <m>\vv_i</m>
            gives
            <me>
              \vv_i\dotp\yy = c_i(\vv_i\dotp\vv_i)
            </me>,
            since the dot product of <m>\vv_i</m> with <m>\vv_j</m> for <m>i\neq j</m> is zero.
          </p>
        </proof>
      </theorem>

      <p>
        One use of <xref ref="thm-fourier-expansion"/> is determining whether or not a given vector is in the span of an orthogonal set.
        If it is in the span, then its coefficients must satisfy the Fourier expansion formula.
        Therefore, if we compute the right hand side of the above formula and do not get our original vector, then that vector must not be in the span.
      </p>

      <exercise xml:id="ex-test-span">
        <statement>
          <p>
            Determine whether or not the vectors <m>\vv=(1,-4,3,-11), \ww=(3,1,-4,2)</m>
            belong to the span of the vectors <m>\xx_1=(1,0,1,0), \xx_2=(-1,0,1,1), \xx_3=(1,1,-1,2)</m>.
            (We confirmed that these vectors form an orthogonal set in <xref ref="ex-orthogonal-set"/>.)
          </p>
        </statement>
        <solution>
          <p>
            We compute
            <md>
              <mrow>\left(\frac{\vv\dotp\xx_1}{\len{\xx_1}^2}\right)\mathbf{x}_1
                \amp +\left(\frac{\vv\dotp\xx_2}{\len{\xx_2}^2}\right)\mathbf{x}_2
                +\left(\frac{\vv\dotp\xx_3}{\len{\xx_3}^2}\right)\mathbf{x}_3</mrow>
              <mrow> \amp = \frac{4}{2}\xx_1+\frac{-9}{3}\xx_2+\frac{-28}{7}</mrow>
              <mrow> \amp = 2(1,0,1,0)-3(-1,0,1,1)-4(1,1,-1,2)</mrow>
              <mrow> \amp = (1,-4,3,-11) = \vv</mrow>
            </md>,
            so <m>\vv\in\spn\{\xx_1,\xx_2,\xx_3\}</m>.
          </p>

          <p>
            On the other hand, repeating the same calculation with <m>\ww</m>, we find
            <md>
              <mrow>\left(\frac{\vv\dotp\xx_1}{\len{\xx_1}^2}\right)\xx_1
                \amp +\left(\frac{\vv\dotp\xx_2}{\len{\xx_2}^2}\right)\xx_2
                +\left(\frac{\vv\dotp\xx_3}{\len{\xx_3}^2}\right)\xx_3</mrow>
              <mrow> \amp =\frac12 (1,0,1,0)-\frac53 (-1,0,1,1) +\frac47 (1,1,-1,2)</mrow>
              <mrow> \amp = \left(\frac{73}{42},\frac47,-\frac{115}{42},-\frac{11}{21}\right)\neq \ww</mrow>
            </md>,
            so <m>\ww\notin\spn\{\xx_1,\xx_2,\xx_3\}</m>.
          </p>

          <p>
            Soon, we'll see that the quantity we computed when showing that <m>\ww\notin\spn\{\xx_1,\xx_2,\xx_3\}</m>
            is, in fact, the <em>orthogonal projection</em> of <m>\ww</m> onto the subspace <m>\spn\{\xx_1,\xx_2,\xx_3\}</m>.
          </p>
        </solution>
      </exercise>

      <p>
        The Fourier expansion is especially simple if our basis vectors have norm one,
        since the demoninators in each coefficient disappear.
        Recall (from elementary linear algebra) that for any nonzero vector <m>\vv</m>,
        a <em>unit vector</em> (that is, a vector of norm one) in the direction of <m>\vv</m> is given by
        <me>
          \hat{u} = \frac{1}{\len{\vv}}\vv
        </me>.
        We often say that the vector <m>\uu</m> is <em>normalized</em>.
        (The convention of using a <q>hat</q> for unit vectors is common but not universal.)
      </p>

      <definition xml:id="def-onb">
        <statement>
          <p>
            A basis <m>B</m> of <m>\R^n</m> is called an <term>orthonormal basis</term>
            if <m>B</m> is orthogonal, and all the vectors in <m>B</m> are unit vectors.
          </p>
        </statement>
      </definition>

      <example>
        <statement>
          <p>
            In <xref ref="ex-orthogonal-set"/> we saw that the set
            <me>
              \{(1,0,1,0), (-1,0,1,1), (1,1,-1,2),(1,-6,-1,2)\}
            </me>
            is orthogonal. Since it's orthogonal, it must be independent,
            and since it's a set of four independent vectors in <m>\R^4</m>,
            it must be a basis. To get an orthonormal basis, we normalize each vector:
            <md>
              <mrow>\hat{u}_1 \amp = \frac{1}{\sqrt{1^2+0^2+1^2+0^2}}(1,0,1,0) = \frac{1}{\sqrt{2}}(1,0,1,0)</mrow>
              <mrow>\hat{u}_2 \amp = \frac{1}{\sqrt{(-1)^2+0^2+1^2+1^2}}(-1,0,1,1,) = \frac{1}{\sqrt{3}}(-1,0,1,1)</mrow>
              <mrow>\hat{u}_3 \amp = \frac{1}{\sqrt{1^2+1^2+(-1)^2+2^2}}(1,1,-1,2) = \frac{1}{\sqrt{7}}(1,1,-1,2)</mrow>
              <mrow>\hat{u}_4 \amp = \frac{1}{\sqrt{1^2+(-6)^2+(-1)^2+2^2}}(1,-6,-1,2) = \frac{1}{\sqrt{42}}(1,-6,-1,2)</mrow>
            </md>.
            The set <m>\{\hat{u}_1,\hat{u}_2,\hat{u}_3,\hat{u}_4\}</m> is then an orthonormal basis of <m>\R^4</m>.
          </p>
        </statement>
      </example>

      <p>
        The process of creating unit vectors does typically introduce square root coefficients in our vectors.
        This can seem undesirable, but there remains value in having an orthonormal basis.
        For example, suppose we wanted to write the vector <m>\vv=(3,5,-1,2)</m> in terms of our basis.
        We can quickly compute
        <md>
          <mrow>\vv\dotp\hat{u}_1 \amp = \frac{3}{\sqrt{2}}-\frac{1}{\sqrt{2}}=\sqrt{2}</mrow>
          <mrow>\vv\dotp\hat{u}_2 \amp = -\frac{3}{\sqrt{3}}-\frac{1}{\sqrt{3}}+\frac{2}{\sqrt{3}}=-\frac{2}{\sqrt{3}}</mrow>
          <mrow>\vv\dotp\hat{u}_3 \amp = \frac{3}{\sqrt{7}}+\frac{5}{\sqrt{7}}+\frac{1}{\sqrt{7}}+\frac{4}{\sqrt{7}} = \frac{11}{\sqrt{7}}</mrow>
          <mrow>\vv\dotp\hat{u}_4 \amp = \frac{3}{\sqrt{42}}-\frac{30}{\sqrt{42}}+\frac{1}{\sqrt{42}}+\frac{4}{\sqrt{42}} = -\frac{22}{\sqrt{42}}</mrow>
        </md>,
        and so
        <me>
          \vv = \sqrt{2}\hat{u}_1-\frac{2}{\sqrt{3}}\hat{u}_2+\frac{11}{\sqrt{7}}\hat{u}_3-\frac{22}{\sqrt{42}}\hat{u}_4
        </me>.
        There's still work to be done, but it is comparatively simpler than solving the corresponding system of equations.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-ortho-projection">
    <title>Orthogonal Projection</title>
    <p>
      In <xref ref="ex-test-span">Exercise</xref>, we saw that <xref ref="thm-fourier-expansion" text="title"/>
      gives us an efficient way of testing whether or not a given vector belongs to the span of an orthogonal set.
      When the answer is <q>no</q>, the quantity we compute while testing turns out to be very useful:
      it gives the <em>orthogonal projection</em> of that vector onto the span of our orthogonal set.
      This turns out to be exactly the ingredient needed to solve certain minimum distance problems.
    </p>

    <p>
      You may recall the following from elementary linear algebra, or vector calculus.
      Given an nonzero vector <m>\uu</m> and a vector <m>\vv</m>,
      the <em>projection</em> of <m>\vv</m> onto <m>\uu</m> is given by
      <me>
        \proj{\uu}{\vv} = \left(\frac{\vv\dotp\uu}{\len{\uu}^2}\right)\uu
      </me>.
      Note that this looks just like one of the terms in <xref ref="thm-fourier-expansion" text="title"/>.
      Recall also that the vector <m>\vv-\proj{\uu}{\vv}</m> is orthogonal to <m>\uu</m>.
      Our next result is a generalization of this observation.
    </p>

    <theorem xml:id="thm-orthogonal-lemma">
      <title>Orthogonal Lemma</title>
      <statement>
        <p>
          Let <m>\{\vv_1,\vv_2,\ldots, \vv_m\}</m> be an orthogonal set of vectors in <m>\R^n</m>,
          and let <m>\xx</m> be any vector in <m>\R^n</m>. Define the vector <m>\vv_{m+1}</m> by
          <me>
            \vv_{m+1} = \xx-\left(\frac{\xx\dotp\vv_1}{\len{\vv_1}^2}\vv_1+\cdots + \frac{\xx\dotp\vv_m}{\len{\vv_m}^2}\vv_m\right)
          </me>.
          Then:
          <ol>
            <li>
              <p>
                <m>\vv_{m+1}\dotp \vv_i = 0</m> for each <m>i=1,\ldots, m</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>\xx\notin\spn\{\vv_1,\ldots, \vv_m\}</m>,
                then <m>\vv_{m+1}\neq \mathbf{0}</m>,
                and therefore, <m>\{\vv_1,\ldots, \vv_m,\vv_{m+1}\}</m> is an orthogonal set.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          <ol>
            <li>
              <p>
                For any <m>i=1,\ldots m</m>, we have
                <me>
                  \vv_{m+1}\dotp\vv_i = \xx\dotp\vv_i - \frac{\xx\dotp\vv_i}{\len{\vv_i}^2}(\vv_i\dotp\vv_i)=0
                </me>,
                since <m>\vv_i\dotp\vv_j = 0</m> for <m>i\neq j</m>.
              </p>
            </li>

            <li>
              <p>
                It follows from the <xref ref="thm-fourier-expansion" text="title"/> that <m>\vv_{m+1}=\mathbf{0}</m>
                if and only if <m>\xx\in\spn\{\vv_1,\ldots, \vv_m\}</m>,
                and the fact that <m>\{\vv_1,\ldots, \vv_m,\vv_{m+1}\}</m>
                is an orthogonal set then follows from the first part.
              </p>
            </li>
          </ol>
        </p>
      </proof>
    </theorem>

    <p>
      It follows from the <xref ref="thm-orthogonal-lemma" text="title"/> that for any subspace <m>U\subseteq \R^n</m>,
      any set of orthogonal vectors in <m>U</m> can be extended to an orthogonal basis of <m>U</m>.
      Since any set containing a single nonzero vector is orthogonal,
      it follows that every subspace has an orthogonal basis.
      (If <m>U=\{\mathbf{0}\}</m>, we consider the empty basis to be orthogonal.)
    </p>

    <p>
      The procedure for creating an orthogonal basis is clear.
      Start with a single nonzero vector <m>\vv_1\in U</m>.
      If <m>U\neq \spn\{\vv_1\}</m>, choose a vector <m>\xx_1\in U</m> with <m>\xx_1\notin\spn\{\vv_1\}</m>.
      The <xref ref="thm-orthogonal-lemma" text="title"/> then provides us with a vector
      <me>
        \vv_2 = \xx_1-\frac{\xx_1\dotp\vv_1}{\len{\vv_1}^2}\vv_1
      </me>
      such that <m>\{\vv_1,\vv_2\}</m> is orthogonal.
      If <m>U=\spn\{\vv_1,\vv_2\}</m>, we're done.
      Otherwise, we repeat the process, choosing <m>\xx_2\notin\spn\{\vv_1,\vv_2\}</m>,
      and then using the <xref ref="thm-orthogonal-lemma" text="title"/> to obtain <m>\vv_3</m>,
      and so on, until an orthogonal basis is obtained.
    </p>

    <p>
      With one minor modification, the above procedure provides us with a major result.
      Suppose <m>U</m> is a subspace of <m>\R^n</m>, and start with <em>any</em> basis <m>\{\xx_1,\ldots, \xx_m\}</m> of <m>U</m>.
      By choosing our <m>\xx_i</m> in the procedure above to be these basis vectors, we obtain the
      <em>Gram-Schmidt algorithm</em> for constructing an orthogonal basis.
    </p>

    <theorem xml:id="thm-gram-schmidt">
      <title>Gram-Schmidt Orthonormalization Algorithm</title>
      <statement>
        <p>
          Let <m>U</m> be a subspace of <m>\R^n</m>, and let <m>\{\xx_1,\ldots, \xx_m\}</m> be a basis of <m>U</m>.
          Define vectors <m>\vv_1,\ldots, \vv_m</m> in <m>U</m> as follows:
          <md>
            <mrow>\vv_1 \amp = \xx_1 </mrow>
            <mrow>\vv_2 \amp = \xx_2 - \frac{\xx_2\dotp\vv_1}{\len{\vv_1}^2}\vv_1</mrow>
            <mrow>\vv_3 \amp = \xx_3 - \frac{\xx_3\dotp\vv_1}{\len{\vv_1}^2}\vv_1-\frac{\xx_3\dotp\vv_2}{\len{\vv_2}^2}\vv_2</mrow>
            <mrow>\vdots \amp </mrow>
            <mrow>\vv_m \amp = \xx_m - \frac{\xx_m\dotp\vv_1}{\len{\vv_1}^2}\vv_1-\cdots - \frac{\xx_m\dotp\vv_{m-1}}{\len{\vv_{m-1}}^2}\vv_{m-1}</mrow>
          </md>.
          Then <m>\{\vv_1,\ldots, \vv_m\}</m> is an orthogonal basis for <m>U</m>.
          Moreover, for each <m>k=1,2,\ldots, m</m>, we have
          <me>
            \spn\{\vv_1,\ldots, \vv_k\} = \spn\{\xx_1,\ldots, \xx_k\}
          </me>.
        </p>
      </statement>
    </theorem>

    <p>
      Of course, once we've used Gram-Schmidt to find an orthogonal basis,
      we can normalize each vector to get an orthonormal basis.
      The Gram-Schmidt algorithm is ideal when we know how to find <em>a</em> basis for a subspace,
      but we need to know an orthogonal basis.
      For example, suppose we want an orthonormal basis for the nullspace of the matrix
      <me>
        A = \bbm 2 \amp -1 \amp 3 \amp 0 \amp 5\\0 \amp 2 \amp -3  \amp 1 \amp 4\\ -4 \amp 2 \amp -6 \amp 0 \amp -10\\ 2 \amp 1 \amp 0 \amp 1 \amp 9\ebm
      </me>.
      First, we find <em>any</em> basis for the nullspace.
    </p>

    <sage>
      <input>
        from sympy import *
        init_printing()
        A = Matrix(4,5,[2,-1,3,0,5,0,2,-3,1,4,-4,2,-6,0,-10,2,1,0,1,9])
        A.nullspace()
      </input>
    </sage>

    <p>
      Let's make that basis look a little nicer by using some scalar multiplication to clear fractions.
      <me>
        B=\left\{\xx_1=\bbm 3\\-6\\-4\\0\\0\ebm, \xx_2=\bbm 1\\2\\0\\-4\\0\ebm, \xx_3=\bbm 7\\4\\0\\0\\-2\ebm\right\}
      </me>
      Defnitely not an orthogonal basis. So we take <m>\vv_1=\xx_1</m>, and
      <md>
        <mrow>\vv_2 \amp = \xx_2-\left(\frac{\xx_2\dotp\vv_1}{\len{\vv_1}^2}\right)\vv_1</mrow>
        <mrow> \amp = \bbm 1\\2\\0\\-4\\0\ebm -\frac{-9}{61}\bbm 3\\-6\\-4\\-0\\0\ebm </mrow>
      </md>,
      which equals something I'm not sure I want to try to simplify. Finally, we find
      <me>
        \vv_3 = \xx_3-\left(\frac{\xx_3\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1-\left(\frac{\xx_3\dotp\vv_2}{\len{\vv_2}^2}\right)\vv_2
      </me>.
      And now you probably get about five minutes into the fractions and say something that shouldn't appear in print.
      This sounds like a job for the computer.
    </p>

    <sage>
      <input>
        B = A.nullspace()
        GramSchmidt(B)
      </input>
    </sage>

    <p>
      Oh wait, you wanted that normalized?
      Turns out the GramSchmidt function has an optional argument of true or false.
      The default is false, which is to not normalize. Setting it to true gives an orthonormal basis:
    </p>

    <sage>
      <input>
        GramSchmidt(B,true)
      </input>
    </sage>

    <p>
      OK, so that's nice, and fairly intimidating looking.
      Did it work? We can specify the vectors in our list by giving their positions, which are 0, 1, and 2, resepctively.
    </p>

    <sage>
      <input>
        L=GramSchmidt(B)
        L[0],L[1]
      </input>
    </sage>

    <p>
      Let's compute dot products:
    </p>

    <sage>
      <input>
        L[0].dot(L[1]),L[1].dot(L[2]),L[0].dot(L[2])
      </input>
    </sage>

    <p>
      Let's also confirm that these are indeed in the nullspace.
    </p>

    <sage>
      <input>
        A*L[0],A*L[1],A*L[2]
      </input>
    </sage>

    <p>
      Boom. Let's try another example. This time we'll keep the vectors a little smaller in case you want to try it by hand.
    </p>

    <exercise>
      <statement>
        <p>
          Confirm that the set <m>B=\{(1,-2,1), (3,0,-2), (-1,1,2)\}</m> is a basis for <m>\R^3</m>,
          and use the <xref ref="thm-gram-schmidt" text="title"/> to find an orthonormal basis.
        </p>
      </statement>
      <solution>
        <p>
          First, note that we can actually jump right into the Gram-Schmidt procedure.
          If the set <m>B</m> is not a basis, then it won't be independent,
          and when we attempt to construct the third vector in our orthonormal basis,
          its projection on the the subspace spanned by the first two will be the same as the original vector,
          and we'll get zero when we subtract the two.
        </p>

        <p>
          We let <m>\xx_1=(1,-2,1), \xx_2=(3,0,-2), \xx_3=(-1,1,2)</m>,
          and set <m>\vv_1=\xx_1</m>. Then we have
          <md>
            <mrow>\vv_2 \amp = \xx_2-\left(\frac{\xx_2\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1 </mrow>
            <mrow> \amp = (3,0,-2)-\frac{1}{6}(1,-2,1)</mrow>
            <mrow> \amp = \frac16(17,2,-3) </mrow>
          </md>.
        </p>
        <p>
          Next, we compute <m>\vv_3</m>.
          <md>
            <mrow>\vv_3 \amp = \xx_3-\left(\frac{\xx_3\dotp \vv_1}{\len{\vv_1}^2}\right)\vv_1 - \left(\frac{\xx_3\dotp \vv_2}{\len{\vv_2}^2}\right)\vv_2</mrow>
            <mrow> \amp = (-1,1,2)-\frac{-1}{6}(1,-2,1)-\cdot \frac{-21}{303}(17,2,-3)</mrow>
            <mrow> \amp = (-1,1,2)+\frac16(1,-2,1)+\frac{7}{101}(17,2,-3)</mrow>
            <mrow> \amp = \frac{1}{606}\bigl((-606,606,1212)+(101,-202,101)+(782,84,-126)\bigr)</mrow>
            <mrow> \amp = \frac{1}{606}(277,488,1187)</mrow>
          </md>.
        </p>
      </solution>
    </exercise>

    <aside>
      <p>
        You'll notice that I used <m>6\vv_2</m> rather than <m>\vv_2</m>
        in the calculation of <m>\vv_3</m>.
        This lets me avoid fractions (momentarily), and doesn't affect the answer,
        since for any nonzero scalar <m>c</m>,
        <md>
          <mrow>\left(\frac{c\vv\dotp \xx}{\len{c\vv}^2}\right)\amp(c\vv)</mrow>
          <mrow>\amp= \left(\frac{c(\vv\dotp\xx)}{c^2\len{\vv}^2}\right)(c\vv)</mrow>
          <mrow>\amp=\left(\frac{\vv\dotp\xx}{\len{\vv^2}}\right)\vv</mrow>
        </md>.
      </p>
    </aside>

    <p>
      OK. Now, given the frequency with which typos occur in this text,
      and the fact that I tried to do the above problem in my head while typing
      (with an occasional calculator check),
      there's a good chance there's a mistake somewhere. Let's check our work.
    </p>

    <sage>
      <input>
        L=(Matrix([1,-2,1]),Matrix([3,0,-2]),Matrix([-1,1,2]))
        GramSchmidt(L)
      </input>
    </sage>

    <paragraphs xml:id="pars-projections">
      <title>Projections</title>
      <p>
        We hinted above that the calculations we've been doing have a lot to do with projection.
        Since any single nonzero vector forms an orthogonal basis for its span,
        the projection
        <me>
          \proj{\uu}{\vv}=\left(\frac{\uu\dotp\vv}{\len{\uu}^2}\right)\uu
        </me>
        can be viewed as the orthogonal projection of the vector <m>\vv</m>,
        not onto the vector <m>\uu</m>, but onto the subspace <m>\spn\{\uu\}</m>.
        This is, after all, how we viewed projections in elementary linear algebra:
        we drop the perpendicular from the tip of <m>\vv</m> onto the <em>line</em> in the direction of <m>\uu</m>.
      </p>

      <p>
        Now that we know how to define an orthogonal basis for a subspace,
        we can define orthogonal projection onto subspaces of dimension greater than one.
      </p>

      <definition xml:id="def-ortho-projection">
        <statement>
          <p>
            Let <m>U</m> be a subspace of <m>\R^n</m> with orthogonal basis
            <m>\{\uu_1,\ldots, \uu_k\}</m>.
            For any vector <m>\vv\in \R^n</m>, we define the <term>orthogonal projection</term>
            of <m>\vv</m> onto <m>U</m> by
            <me>
              \proj{U}{\vv} = \sum_{i=1}^k\left(\frac{\uu_i\dotp\vv}{\len{\uu_i}^2}\right)\uu_i
            </me>.
          </p>
        </statement>
      </definition>

      <p>
        Note that <m>\proj{U}{\vv}</m> is indeed an element of <m>U</m>, since it's a linear combination of its basis vectors.
        In the case of the trivial subspace <m>U=\{\mathbf{0}\}</m>, we define orthogonal projection of any vector to be <m>\mathbf{0}</m>,
        since really, what other choice do we have? (This case isn't really of any interest, we just like being thorogh.)
      </p>

      <p>
        Let's see how this might be put to use in a classic problem: finding the distance from a point to a plane.
      </p>

      <aside>
        <p>
          One limitation of this approach to projection is that we must project onto a <em>subspace</em>.
          Given a plane like <m>x-2y+4z=4</m>, we would need to modify our approach.
          One way to do it would be to find a point on the plane,
          and then try to translate everything to the origin.
          It's interesting to think about how this might be accomplished
          (in particular, in what direction would the tranlation have to be performed?)
          but someone external to the questions we're interested in here.
        </p>
      </aside>

      <example>
        <statement>
          <p>
            Find the distance from the point <m>(3,1,-2)</m> to the plane <m>P</m>
            defined by <m>x-2y+4z=0</m>.
          </p>
        </statement>
        <solution>
          <title>Using projection onto a normal vector</title>

          <p>
            In an elementary linear algebra (or calculus) course, we would solve this problem as follows.
            First, we would need two vectors parallel to the plane.
            If <m>\bbm x\\y\\z\ebm</m> lies in the plane, then <m>x-2y+4z=0</m>, so <m>x=2y-4z</m>, and
            <me>
              \bbm x\\y\\z\ebm = \bbm 2y-4z\\y\\z\ebm = y\bbm 2\\1\\0\ebm + z\bbm -4\\0\\1\ebm
            </me>,
            so <m>\uu=\bbm 2\\1\\0\ebm</m> and <m>\vv\bbm -4\\0\\1\ebm</m> are parallel to the plane.
            We then compute the normal vector
            <me>
              \mathbf{n}=\uu\times\vv=\bbm 1\\-2\\4\ebm
            </me>,
            and compute the projection of the position vector <m>\mathbf{p}=\bbm 3,1,-2\ebm</m> for the point <m>P=(3,1,-2)</m> onto <m>\mathbf{n}</m>.
            This gives the vector
            <me>
              \xx = \left(\frac{\mathbf{p}\dotp\mathbf{n}}{\len{\mathbf{n}}^2}\right)\mathbf{n} = \frac{-7}{21}\bbm 1\\-2\\4\ebm =\bbm-1/3\\2/3\\-4/3\ebm
            </me>.
          </p>

          <p>
            Now, this vector is <em>parallel</em> to <m>\mathbf{n}</m>, so it's perpendicular to the plane.
            Subtracting it from <m>\mathbf{p}</m> gives a vector parallel to the plane, and this is the position vector for the point we seek.
            <me>
              \mathbf{q}=\mathbf{p}-\xx=\bbm 3\\1\\-2\ebm-\bbm -1/3\\-2/3\\-4/3\ebm = \bbm 10/3\\1/3\\-2/3\ebm
            </me>
            so the closest point is <m>Q=\bigl(\frac{10}{3},\frac13,-\frac{2}{3}\bigr)</m>.
            We weren't asked for it, but note that if we wanted the distance from the point <m>P</m> to the plane,
            this is given by <m>\len{\xx}=\frac13\sqrt{21}</m>.
          </p>
        </solution>
        <solution>
          <title>Using orthogonal projection</title>

          <p>
            Let's solve the same problem using orthogonal projection.
            First, we have to deal with the fact that the vectors <m>\uu</m> and <m>\vv</m> are probably not orthogonal.
            To get around this, we replace <m>\vv</m> with
            <me>
              \ww = \vv-\left(\frac{\vv\dotp\uu}{\len{\uu}^2}\right)\uu = \bbm -4\\0\\1\ebm+\frac 85\bbm 2\\1\\0\ebm = \bbm -4/5\\8/5\\1\ebm
            </me>.
            We now set
            <md>
              <mrow>\mathbf{q} \amp =\left(\frac{\mathbf{p}\dotp\uu}{\len{\uu}^2}\right)\uu-\left(\frac{\mathbf{p}\dotp\ww}{\len{\ww}^2}\right)\ww</mrow>
              <mrow> \amp = \frac{7}{5}\bbm 2\\1\\0\ebm +\frac{-14}{105}\bbm -4\\8\\5\ebm </mrow>
              <mrow> \amp = \bbm 10/3\\1/3\\-2/3\ebm</mrow>
            </md>.
            Lo and behold, we get the same answer as before.
          </p>
        </solution>
      </example>

      <p>
        The only problem with <xref ref="def-ortho-projection"/> is that it appears to depend on the choice of orthogonal basis.
        To see that it doesn't, we need one more definition.
      </p>

      <definition xml:id="def-ortho-comp">
        <statement>
          <p>
            For any subspace <m>U</m> of <m>\R^n</m>,
            we define the <term>orthogonal complement</term> of <m>U</m>, denoted <m>U^\bot</m>,
            by
            <me>
              U^\bot = \{\xx\in\R^n \,|\, \xx\dotp\yy = 0 \text{ for all } \yy\in U\}
            </me>.
          </p>
        </statement>
      </definition>

      <p>
        The term <q>complement</q> comes from terminology we mentioned early on,
        but didn't spend much time on. <xref ref="thm-construct-complement">Theorem</xref>
        told us that for any subspace <m>U</m> of a vector space <m>V</m>,
        it is possible to construct another subspace <m>W</m> of <m>V</m>
        such that <m>V = U\oplus W</m>.
        The subspace <m>W</m> is known as a complement of <m>U</m>.
        A complement is not unique, but the orthogonal complement is.
        As you might guess from the name, <m>U^\bot</m> is also a subspace of <m>\R^n</m>.
      </p>

      <exercise>
        <statement>
          <p>
            Show that <m>U^\bot</m> is a subspace of <m>\R^n</m>.
          </p>
        </statement>
      </exercise>


      <theorem xml:id="thm-projection">
        <title>Projection Theorem</title>
        <statement>
          <p>
            Let <m>U</m> be a subspace of <m>\R^n</m>, let <m>\xx</m> be any vector in <m>\R^n</m>,
            and let <m>\mathbf{p}=\proj{U}{\xx}</m>. Then:
            <ol>
              <li>
                <p>
                  <m>\mathbf{p}\in U</m>, and <m>\xx-\mathbf{p}\in U^\bot</m>.
                </p>
              </li>
              <li>
                <p>
                  <m>\mathbf{p}</m> is the <em>closest</em> vector in <m>U</m> to the vector <m>\xx</m>,
                  in the sense that the distance <m>d(\mathbf{p},\xx)</m> is minimal among all vectors in <m>U</m>.
                  That is, for all <m>\uu\neq \mathbf{p}\in U</m>, we have
                  <me>
                    \len{\xx-\mathbf{p}}\lt\len{\xx-\yy}
                  </me>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </theorem>

      <exercise>
        <statement>
          <p>
            Show that <m>U\cap U^\bot = \{\mathbf{0}\}</m>.
            Use this fact to show that <xref ref="def-ortho-projection"/> does not depend on the choice orthogonal basis.
          </p>
        </statement>
        <hint>
          <p>
            Suppose we find vectors <m>\mathbf{p}</m> and <m>\mathbf{p}'</m> using basis <m>B</m> and <m>B'</m>.
            Note that <m>\mathbf{p}-\mathbf{p}'\in U</m>, but also that
            <me>
              \mathbf{p}-\mathbf{p}' = (\mathbf{p}-\xx)-(\mathbf{p}'-\xx)
            </me>
            Now use <xref ref="thm-projection"/>
          </p>
        </hint>
      </exercise>


      <p>
        Finally, we note one more useful fact. The process of sending a vector to its orthogonal projection defines an operator on <m>\R^n</m>,
        and yes, it's linear.
      </p>

      <theorem xml:id="thm-proj-operator">
        <statement>
          <p>
            Let <m>U</m> be a subspace of <m>\R^n</m>, and define a function <m>P_U:\R^n\to \R^n</m> by
            <me>
              P_U(\xx) = \proj{U}{\xx} \text{ for any } \xx\in\R^n
            </me>.
            Then <m>T</m> is a linear operator such that <m>U=\im P_U</m> and <m>U^\bot = \ker P_U</m>.
          </p>
        </statement>
      </theorem>

      <p>
        Note: it follows from this result and the <xref ref="thm-dimension-lintrans" text="title"/> that
        <me>
          \dim U + \dim U^\bot = n
        </me>,
        and since <m>U\cap U^\bot = \{\mathbf{0}\}</m>, <m>U^\bot</m> is indeed a complement of <m>U</m>
        in the sense introduced in <xref ref="thm-construct-complement">Theorem</xref>.
        It's also fairly easy to see that <m>\dim U + \dim U^\bot = n</m> directly.
        If <m>\ww\in U^\bot</m>, and <m>\{\uu_1,\ldots, \uu_k\}</m>
        is a basis for <m>U</m>, then we have
        <me>
          \ww\dotp \uu_1= 0, \ldots, \ww\dotp \uu_k=0
        </me>,
        and for an unknown <m>\ww</m>, this is simply a homogeneous system of <m>k</m> equations with <m>n</m> variables.
        Moverer, they are <em>independent</em> equations, since the <m>\uu_i</m> form a basis.
        We thus expect <m>n-k</m> free parameters in the general solution.
      </p>

      <exercise>
        <statement>
          <p>
            Let <m>U = \{(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c)\,|\, a,b,c\in\R\}\subseteq \R^5</m>.
            Determine a basis for <m>U^\bot</m>.
          </p>
        </statement>
        <solution>
          <p>
            First, we note that for a general element of <m>U</m>, we have
            <me>
              (a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c) = a(1,2,0,4,1)+b(-1,1,0,-1,0)+c(3,0,3,3,-4)
            </me>.
            If <m>\ww = (x_1,x_2,x_3,x_4,x_5)\in U^\bot</m>, then we must have
            <md>
              <mrow>\ww\dotp (1,2,0,4,1) \amp = x_1+2x_2+4x_4+x_5=0 </mrow>
              <mrow>\ww\dotp (-1,1,0,-1,0) \amp =-x_1+x_2-x_4=0</mrow>
              <mrow>\ww\dotp (3,0,3,3,-4) \amp =3x_1+3x_3+3x_4-4x_5 = 0</mrow>
            </md>.
            To find a basis for <m>U^\bot</m>, we simply need to find the nullspace of the coefficient matrix for this system,
            which we do below.
          </p>
        </solution>
      </exercise>

      <sage>
        <input>
          A = Matrix(3,5,[1,2,0,4,1,-1,1,0,-1,0,3,0,3,3,-4])
          A.nullspace()
        </input>
      </sage>
    </paragraphs>
  </section>
</chapter>
