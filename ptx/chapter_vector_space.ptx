<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-vector-space">
  <title>Vector spaces</title>
  <section xml:id="sec-vec-sp">
    <title>Abstract vector spaces</title>

    <p>
      In your first course in linear algebra, you likely worked a lot with vectors in two and three dimensions,
      where they can be visualized geometrically as objects with magnitude and direction (and drawn as arrows).
      You probably extended your understanding of vectors to include <em>column vectors</em>;
      that is, <m>1\times n</m> matrices of the form <m>\vv=\bbm v_1\\v_2\\\vdots\\v_n\ebm</m>.
    </p>

    <p>
      Using either geometric arguments (in <m>\R^2</m> or <m>\R^3</m>) or the properties of matrix arithmetic,
      you would have learned that these vectors can be added, by adding corresponding components,
      and multiplied by <em>scalars</em> <mdash/> that is, real numbers <mdash/>
      by multiplying each component of the vector by the scalar.
    </p>

    <p>
      It's also likely, although you may not have spent too long thinking about it,
      that you looked at the properties obeyed by the addition and scalar multiplication of vectors
      (or, for that matter, matrices).
      For example, you may have made use of the fact that order of addition doesn't matter,
      or that scalar multiplication distributes over addition.
    </p>

    <p>
      It turns out that the algebraic properties satisfied by vector addition and scalar multiplication are not unique to vectors,
      as vectors were understood in your first course in linear algebra.
      In fact, many types of mathematical object exhibit similar behaviour.
      Examples include matrices, polynomials, and even functions.
    </p>

    <p>
      Linear algebra, as an abstract mathematical topic, begins with a realization of the importance of these properties.
      Indeed, these properties, established as theorems for vectors in <m>\R^n</m>,
      become the <em>axioms</em> for the abstract notion of a <em>vector space</em>.
      The advantage of abstracting these ideas is that any proofs we write that depend only on these axioms will automatically be valid for any set of objects satisfying those axioms.
      That is, a result that is true for vectors in <m>\R^2</m> is often also true for vectors in <m>\R^n</m>, and for matrices, and polynomials, and so on.
      Mathematicians like to be efficient, and prefer to establish a result once in an abstract setting,
      knowing that it will then apply to many concrete settings that fit into the framework of the abstract result.
    </p>

    <definition xml:id="def-vector-space">
      <statement>
        <p>
          A <term>real vector space</term> (or vector space over <m>\R</m>) is a nonempty set <m>V</m>, whose objects are called <term>vectors</term>,
          equipped with two operatations:
          <ol>
            <li>
              <p>
                <term>Addition</term>, which is a map from <m>V\times V</m> to <m>V</m>
                that associates each ordered pair of vectors <m>(\vv,\ww)</m> to a vector <m>\vv+\ww</m>,
                called the <term>sum</term> of <m>\vv</m> and <m>\ww</m>.
              </p>
            </li>

            <li>
              <p>
                <term>Scalar multiplication</term>, which is a map from <m>\R\times V</m> to <m>V</m>
                that associates each real number <m>c</m> and vector <m>\vv</m> to a vector <m>c\vv</m>.
              </p>
            </li>
          </ol>
        </p>

        <p>
          The operations of addition and scalar multiplication are required to satisfy the following <em>axioms</em>:
          <dl>
            <li>
              <title>A1.</title>
              <p>
                If <m>\uu,\vv\in V</m>, then <m>\uu+\vv\in V</m>. (Closure under addition)
              </p>
            </li>
            <li>
              <title>A2.</title>
              <p>
                For all <m>\uu,\vv\in V</m>, <m>\uu+\vv=\vv+\uu</m>. (Commutativity of addition)
              </p>
            </li>
            <li>
              <title>A3.</title>
              <p>
                For all <m>\uu,\vv,\ww\in V</m>, <m>\uu+(\vv+\ww)=(\uu+_\vv)+\ww</m>. (Associativity of addition)
              </p>
            </li>
            <li>
              <title>A4.</title>
              <p>
                There exists an element <m>\zer\in V</m> such that <m>\vv+\zer=\vv</m> for each <m>\vv\in V</m>. (Existence of a zero vector)
              </p>
            </li>
            <li>
              <title>A5.</title>
              <p>
                For each <m>\vv\in V</m>, there exists a vector <m>-\vv\in V</m> such that <m>\vv+(-\vv)=\zer</m>. (Existence of negatives)
              </p>
            </li>
            <li>
              <title>S1.</title>
              <p>
                If <m>\vv\in V</m>, then <m>c\vv\in V</m> for all <m>c\in\R</m>. (Closure under scalar multiplication)
              </p>
            </li>
            <li>
              <title>S2.</title>
              <p>
                For all <m>c\in \R</m> and <m>\vv,\ww\in V</m>, <m>c(\vv+\ww)=c\vv+c\ww</m>. (Distribution over vector addition)
              </p>
            </li>
            <li>
              <title>S3.</title>
              <p>
                For all <m>a,b\in\R</m> and <m>\vv\in V</m>, <m>(a+b)\vv=a\vv+b\vv</m>. (Distribution over scalar addition)
              </p>
            </li>
            <li>
              <title>S4.</title>
              <p>
                For all <m>a,b\in \R</m> and <m>\vv\in V</m>, <m>a(b\vv)=(ab)\vv</m>. (Associativity of scalar multiplication)
              </p>
            </li>
            <li>
              <title>S5.</title>
              <p>
                For all <m>\vv\in V</m>, <m>1\vv=\vv</m>. (Normalization of scalar multiplication)
              </p>
            </li>
          </dl>
        </p>
      </statement>
    </definition>

    <p>
      Note that a zero vector must exist in every vector space.
      This simple observation is a key component of many proofs and counterexamples in linear algebra.
      In general, we may define a vector space whose scalars belong to a <em>field</em> <m>\mathbb{F}</m>.
      A field is a set of objects whose algebraic properties are modelled after those of the real numbers.
      Fields have their own set of axioms, which we will not list here.
      While it is possible to study linear algebra over <em>finite fields</em> (like the integers modulo a prime number)
      we will only consider two fields: the real numbers <m>\R</m>, and the complex numbers <m>\C</m>.
    </p>
    <p>
      A vector space whose scalars are complex numbers will be called a <em>complex vector space</em>.
      While many students are initially intimidated by the complex numbers,
      most results in linear algebra work exactly the same over <m>\C</m> as they do over <m>\R</m>.
      And where the results differ, things are usually <em>easier</em> with complex numbers,
      owing in part to the fact that all complex polyomials can be completely factored.
    </p>

    <p>
      To help us gain familiarity with the abstract nature of <xref ref="def-vector-space"/>,
      let us consider some basic examples.
    </p>

    <example xml:id="ex-vector-spaces">
      <statement>
        <p>
          The following are examples of vector spaces. We leave verification of axioms as an exercise.
          <ol>
            <li>
              <p>
                The set <m>\R^n</m> of <m>n</m>-tuples <m>(x_1,x_2,\ldots, \x_n)</m> of real numbers,
                where we define
                <md>
                  <mrow>(x_1,x_2,\ldots, x_n)+(y_1,y_2,\ldots, y_n) \amp = (x_1+y+1,x_2+y_2,\ldots, x_n+y_n) </mrow>
                  <mrow> c(x_1,x_2,\ldots, x_n)\amp = (cx_1,cx_2,\ldots, cx_n)</mrow>
                </md>.
                We will also often use <m>\R^n</m> to refer to the vector space of <m>1\times n</m> column matrices <m>\bbm x_1\\x_2\\\vdots\\x_n\ebm</m>,
                where addition and scalar multiplication are defined as for matrices
                (and the same as the above, with the only difference being the way in which we choose to write our vectors).
                If the distinction between <m>n</m>-tuples and column matrices is ever important, it will be made clear.
              </p>
            </li>

            <li>
              <p>
                The set <m>\mathbf{M}_{mn}</m> of <m>m\times n</m> matrices, equipped with the usual matrix addition and scalar multiplication.
              </p>
            </li>

            <li>
              <p>
                The set <m>\mathbf{P}_n</m> of all polynomials
                <me>
                  p(x) = a_0+a_1x+\cdots + a_nx^n
                </me>
                of degree less than or equal to <m>n</m>, where, for
                <md>
                  <mrow>p(x) \amp = a_0+a_1x+\cdots + a_nx^n </mrow>
                  <mrow>q(x) \amp = b_0+b_1x+\cdots +b_nx^n</mrow>
                </md>
                we define
                <me>
                  p(x)+q(x)=(a_0+b_0)+(a_1+b_1)x+\cdots + (a_n+b_n)x^n
                </me>
                and
                <me>
                  cp(x) = ca_0+(ca_1)x+\cdots + (ca_n)x^n
                </me>.
                The zero vector is the polynomial <m>0=0+0x+\cdots + 0x^n</m>.
              </p>
            </li>

            <li>
              <p>
                The set <m>\mathbf{F}[a,b]</m> of all functions <m>f:[a,b]\to \R</m>,
                where we define <m>(f+g)(x)=f(x)+g(x)</m> and <m>(cf)(x)=c(f(x))</m>.
                The zero function is the function satsifying <m>0(x)=0</m> for all <m>x\in [a,b]</m>,
                and the negative of a function <m>f</m> is given by <m>(-f)(x)=-f(x)</m> for all <m>x\in [a,b]</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </example>

    <p>
      There are a number of other algebraic properties that are common to all vector spaces;
      for example, it is true that <m>0\vv = \zer</m> for all vectors <m>\vv</m> in any vector space <m>V</m>.
      The reason these are not included is that the ten axioms in <xref ref="def-vector-space"/>
      are the ones deemed <q>essential</q> <ndash /> all other properties can be deduced from the axioms.
      To demonstate, we next give the proof that <m>0\vv = \zer</m>.
    </p>

    <theorem xml:id="thm-zero-mult">
      <statement>
        <p>
          In any vector space <m>V</m>, we have <m>0\vv = \zer</m> for all <m>\vv\in V</m>
        </p>
      </statement>
      <proof>
        <p>
          Since <m>0+0=0</m>, we have <m>0\vv=(0+0)\vv</m>.
          Using the distributive axiom S3, this becomes
          <me>
            0\vv + 0\vv = 0\vv
          </me>.
           By axiom A5, there is an element <m>-0\vv\in V</m> such that <m>0\vv+(-0\vv)=\zer</m>.
           Adding this to both sides of the equation above, we get:
           <me>
             (0\vv+0\vv)+(-0\vv) = 0\vv+(-0\vv)
           </me>.
           Now, apply the associative property (A3) on the left, and A5 on the right, to get
           <me>
             0\vv + (0\vv+(-0\vv)) = \zer
           </me>.
           Using A5 again on the left, we get <m>0\vv+\zer = \zer</m>.
           Finally, axiom A4 guarantees <m>0\vv = 0\vv+\zer = \zer</m>.
        </p>
      </proof>

    </theorem>

    <p>
      Similar tactics can be used to establish the following results,
      which we leave as an exercise.
      Solutions are included, but it will be worth your while in the long run to wrestle with these.
    </p>

    <exercise xml:id="ex-more-props">
      <statement>
        <p>
          Show that the following properties are valid in any vector space <m>V</m>:
          <ol>
            <li>
              <p>
                If <m>\uu+\vv=\uu+\ww</m>, then <m>\vv=\ww</m>.
              </p>
            </li>
            <li>
              <p>
                For any scalar <m>c</m>, <m>c\zer=\zer</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>c\vv=\zer</m>, then either <m>c=0</m> or <m>\vv=\zer</m>.
              </p>
            </li>
            <li>
              <p>
                For any <m>\vv\in V</m>, <m>(-1)\vv=-\vv</m>.
              </p>
            </li>
            <li>
              <p>
                The zero vector is the unique vector such that <m>\vv+\zer=\vv</m> for all <m>\vv\in V</m>.
              </p>
            </li>
            <li>
              <p>
                The negative <m>-\vv</m> of any vector <m>\vv</m> is unique.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <solution>
        <p>
          <ol>
            <li>
              <p>
                Suppose <m>\uu+\vv=\uu+\ww</m>. By adding <m>-\uu</m> on the left of each side, we obtain:
                <md>
                  <mrow>-\uu+(\uu+\vv) \amp =-\uu+(\uu+\ww)</mrow>
                  <mrow>(-\uu+\uu)+\vv \amp =(-\uu+\uu)+\ww \quad \text{ by A3}</mrow>
                  <mrow> \zer+\vv \amp =\zer+\ww \quad \text{ by A5}</mrow>
                  <mrow> \vv \amp =\ww \quad \text{ by A4}</mrow>
                </md>.
              </p>
            </li>
            <li>
              <p>
                We have <m>c\zer = c(\zer+\zer) = c\zer +c\zer</m>, by A4 and S2, respectively.
                Adding <m>-c\zer</m> to both sides (and using axioms A3, A4, A5 as in the proof of <xref ref="thm-zero-mult"/>)
                we get <m>\zer = c\zer</m>.
              </p>
            </li>
            <li>
              <p>
                The main difficulty with this problem is getting the logic of the statement correct,
                and making sure you know what it is you're trying to prove.
                The desired conclusion is an <em>or</em> statement, which means it suffices to establish one part or the other.
                Typically, the way to proceed in these cases is to argue that if the first part is false,
                then the second must be true. This is how we proceed here.
              </p>

              <p>
                Suppose that <m>c\vv=\zer</m>. If <m>c=0</m>,
                then it's true that <m>c=0</m> or <m>\vv=\zer</m>,
                so there's nothing to prove.
                Suppose then that <m>c\neq 0</m>. Then there exists a real number <m>\frac1c</m> such that <m>c\bigl(\frac1c\bigr)=1</m>.
                Multiplying both sides of <m>c\vv=\zer</m> by <m>\frac1c</m>, we get
                <me>
                  \frac1c(c\vv)=\frac1c\zer
                </me>.
                By the previous problem, we know that <m>\frac1c\zer = \zer</m>,
                and by axioms S4 and S5, we have
                <me>
                  \frac1c(c\vv)=\bigl(\frac1c\cdot c\bigr)\vv = 1\vv = \vv
                </me>,
                showing that <m>\vv=\zer</m>, and thus <m>c=0</m> or <m>\vv=\zer</m>.
                (This is a proof by cases, relying on the tautology <m>c=0</m> or <m>c\neq 0</m>.)
              </p>
            </li>

            <li>
              <p>
                Suppose there are two vectors <m>\zer_1,\zer_2</m> that act as additive identities.
                Then
                <md>
                  <mrow>\zer_1 \amp = \zer_1+\zer_2 \quad \text{ since } \vv+\zer_2=\vv \text{ for any } \vv</mrow>
                  <mrow>  \amp =\zer_2+\zer_1 \quad \text{ by axiom A2}</mrow>
                  <mrow>  \amp \zer_2 \quad \text{ since } \vv+\zer_1=\vv \text{ for any } \vv</mrow>
                </md>
                So any two vectors satisfying the property in A4 must, in fact, be the same.
              </p>
            </li>
            <li>
              <p>
                Let <m>\vv\in V</m>, and suppose there are vectors <m>\ww_1,\ww_2\in V</m>
                such that <m>\vv+\ww_1=\zer</m> and <m>\vv+\ww_2=\zer</m>. Then
                <md>
                  <mrow>\ww_1 \amp = \ww_1+\zer \quad  \text{ by A4}</mrow>
                  <mrow> \amp = \ww_1+(\vv+\ww_2) \quad \text{ by assumption}</mrow>
                  <mrow> \amp = (\ww_1+\vv)+\ww_2 \quad \text{ by A3}</mrow>
                  <mrow> \amp = (\vv+\ww_1)+\ww_2 \quad \text{ by A2}</mrow>
                  <mrow> \amp = \zer+\ww_2 \quad \text{ by assumption}</mrow>
                  <mrow> \amp \ww_2 \quad \text{ by A4}</mrow>
                </md>.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </exercise>
  </section>

  <section xml:id="sec-subspace">
    <title>Subspaces</title>
    <p>
      We begin with a motivating example. Let <m>\vv</m> be a nonzero vector in some vector space <m>V</m>.
      Consider the set <m>S = \{c\vv\,|\, c\in \R\}</m>.
      Given <m>a\vv,b\vv\in S</m>, notice that <m>a\vv+b\vv=(a+b)\vv</m> is also an element of <m>S</m>,
      since <m>a+b</m> is again a real number.
      Moreover, for any real number <m>c</m>, <m>c(a\vv)=(ca)\vv</m> is an element of <m>S</m>.
    </p>

    <p>
      There are two important observations: one is that performing addition or scalar multiplication on elements of <m>S</m>
      produces a new element of <m>S</m>. The other is that this addition and multiplication is essentially that of <m>\R</m>.
      The vector <m>\vv</m> is just a placeholder. Addition simply involves the real number addition <m>a+b</m>.
      Scalar multiplication becomes the real number multiplication <m>ca</m>.
      So we expect that the rules for addition and scalar multiplication in <m>S</m> follow those in <m>\R</m>,
      so that <m>S</m> is like a <q>copy</q> of <m>\R</m> inside of <m>V</m>.
      In particular, addition and scalar multiplication in <m>S</m> will satisfy all the vector space axioms,
      so that <m>S</m> deserves to be considered a vector space in its own right.
    </p>

    <p>
      A similar thing happens if we consider a set <m>U=\{a\vv+b\ww\,|\, a,b\in\R\}</m>,
      where <m>\vv,\ww</m> are two vectors in a vector space <m>V</m>.
      Given two elements <m>a_1\vv+a_2\ww,b_1\uu+b_2\ww</m>, we have
      <me>
        (a_1\vv+a_2\ww)+(b_1\v+b_2\ww) = (a_1+b_1)\vv+(a_2+b_2)\ww
      </me>,
      which is again an element of <m>U</m>, and the addition rule looks an awful lot like the addition rule
      <m>(a_1,a_2)+(b_1,b_2)=(a_1+b_1,a_2+b_2)</m> in <m>\R^2</m>.
      Scalar multiplication follows a similar pattern.
    </p>

    <p>
      In general we are often interested in subsets of vectors spaces that behave like <q>copies</q>
      of smaller vector spaces contained within the larger space.
      The technical term for this is <em>subspace</em>.
    </p>

    <definition xml:id="def-subspace">
      <statement>
        <p>
          Let <m>V</m> be a vector space, and let <m>U\subseteq V</m> be a subset.
          We say that <m>U</m> is a <term>subspace</term> of <m>V</m> if <m>U</m>
          is itself a vector space when using the addition and scalar multiplication of <m>V</m>.
        </p>
      </statement>
    </definition>

    <p>
      If we were to follow the definition, then verifying that a subset <m>U</m> is a subspace would involve checking all ten vector space axioms.
      Fortunately, this is not necessary. Since the operations are those of the vector space <m>V</m>,
      most properties follow automatically, being inherited from those of <m>V</m>.
    </p>

    <theorem xml:id="thm-subspace-test">
      <statement>
        <p>
          Let <m>V</m> be a vector space and let <m>U\subseteq V</m> be a subset.
          Then <m>U</m> is a subspace of <m>V</m> if and only if the following conditions are satisfied:
          <ol>
            <li>
              <p>
                <m>\zer\in U</m>, where <m>\zer</m> is the zero vector of <m>V</m>.
              </p>
            </li>

            <li>
              <p>
                <m>U</m> is closed under addition. That is, for all <m>\uu_1,\uu_2\in U</m>, we have <m>\uu_1+\uu_2\in U</m>.
              </p>
            </li>

            <li>
              <p>
                <m>U</m> is closed under scalar multiplication. That is, for all <m>\uu\in U</m> and <m>c\in\R</m>, <m>c\uu\in U</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          If <m>U</m> is a vector space, then clearly the second and third conditions must hold.
          Since a vector space musy be nonempty, there is some <m>\uu\in U</m>,
          from which it follows that <m>\zer=0\uu\in U</m>.
        </p>
        <p>
          Conversely, if all three conditions hold, we have axioms A1, A4, and S1 by assumption.
          Axioms A2 and A3 hold since any vector in <m>U</m> is also a vector in <m>V</m>;
          the same reasoning shows that axioms S2, S3, S4, and S5 hold.
          Finally, axiom A5 holds because condition 3 ensures that <m>(-1)\uu\in U</m> for any <m>\uu\in U</m>,
          and we know that <m>(-1)\uu=-\uu</m> by <xref ref="ex-more-props">Exercise</xref>.
        </p>
      </proof>

    </theorem>

    <p>
      In some texts, the condition that <m>\zer\in U</m> is replaced by the requirement that <m>U</m> be nonempty.
      Existence of <m>\zer</m> then follows from the fact that <m>0\vv=\zer</m>.
      However, it is usually easy to check that a set contains the zero vector,
      so it's the first thing one typically looks for when confirming that a subset is nonempty.
    </p>

    <example>
      <statement>
        <p>
          For any vector space <m>V</m>, the set <m>\{\zer\}</m> is a subspace,
          known as the <em>trivial subspace</em>.
        </p>
        <p>
          If <m>V=\mathbf{P}</m> is the vector space of all polynomials,
          then for any natural number <m>n</m>, the subset <m>U</m> of all polynomials of degree less than or equal to <m>n</m>
          is a subspace of <m>V</m>. Another common type of polynomial subspace is the set of all polynomials with a given root.
          For example, the set <m>U=\{p(x)\in\mathbf{P}\,|\,p(1)=0\}</m> is easily confirmed to be a subspace.
          However, a condition such as <m>p(1)=2</m> would <em>not</em> define a subspace,
          since this condition is not satisfied by the zero polynomial.
        </p>
        <p>
          In <m>\R^n</m>, we can define a subspace using one or more homogeneous linear equations.
          For example, the set
          <me>
            \{(x,y,z)\,|\, 2x-3y+4z=0\}
          </me>
          is a subspace of <m>\R^3</m>. A non-homogeneous equation won't work, however, since it would exclude the zero vector.
          Of course, we should expect that any non-linear equation fails to define a subspace,
          although one is still expected to verify this by confirming the failure of one of the axioms.
          For example, the set <m>S=\{(x,y)\,|\,x=y^2\}</m> is not a subspace;
          although it contains the zero vector (since <m>0^2=0</m>), we have <m>(1,1)\in S</m>,
          but <m>2(1,1)=(2,2)</m> does not belong to <m>S</m>.
        </p>
      </statement>
    </example>

    <p>
      In the next section, we'll encounter perhaps the most fruitful source of subspaces: sets of linear combinations (or <em>spans</em>).
    </p>
  </section>

  <section xml:id="sec-span">
    <title>Span</title>
    <p>
      Recall that a <term>linear combination</term> of a set of vectors
      <m>\vv_1,\ldots, \vv_k</m> is a vector expression of the form
      <me>
        \ww=c_1\vv_1+c_2\vv_2+\cdots +c_k\vv_k,
      </me>
      where <m>c_1,\ldots, c_k</m> are scalars.
    </p>

    <p>
      The <term>span</term> of those same vectors is the set of all possible linear combinations:
      <me>
        \spn\{\vv_1,\ldots, \vv_k\} = \{c_1\vv_1+ \cdots + c_k\vv_k \,|\, c_1,\ldots, c_k \in \mathbb{F}\}.
      </me>
      Therefore, the questions <q>Is the vector <m>\ww</m> in <m>\spn\{\vv_1,\ldots, \vv_k\}</m>?</q>
      is really asking, <q>Can <m>\ww</m> be written as a linear combination of <m>\vv_1,\ldots, \vv_k</m>?</q>
    </p>

    <p>
      With the appropriate setup, all such questions become questions about solving systems of equations.
      Here, we will look at a few such examples.
    </p>

    <exercise>
      <statement>
        <p>
          Determine whether the vector <m>\bbm 2\\3\ebm</m> is in the span of the vectors <m>\bbm 1\\1\ebm,\bbm -1\\2\ebm</m>.
        </p>
      </statement>
    </exercise>

    <p>
      This is really asking: are there scalars <m>s,t</m> such that
      <me>
        s\bbm 1\\1\ebm + t\bbm -1\\2\ebm = \bbm 2\\3\ebm
      </me>?
      And this, in turn, is equivalent to the system
      <md>
        <mrow>s -t \amp=2 </mrow>
        <mrow>s+2t \amp=3 </mrow>
      </md>,
      which is the same as the matrix equation
      <me>
        \bbm 1\amp -1\\1\amp 2\ebm\bbm s\\t\ebm = \bbm 2\\3\ebm.
      </me>
      Solving the system confirms that there is indeed a solution, so the answer to our original question is yes.
    </p>

    <p>
      To confirm the above example (and see what the solution is), we can use the computer.
    </p>
    <sage>
      <input>
        from sympy import *
        init_printing()
      </input>
    </sage>

    <sage>
      <input>
        A = Matrix(2,3,[1,-1,2,1,2,3])
        A.rref()
      </input>
    </sage>

    <p>
      The above code produces the reduced row-echelon form of the augmented matrix for our system.
      Do you remember how to get the answer from here? Here's another approach.
    </p>

    <sage>
      <input>
        B = Matrix(2,2,[1,-1,1,2])
        B
      </input>
    </sage>

    <sage>
      <input>
        C = Matrix(2,1,[2,3])
        X = (B**-1)*C
        X
      </input>
    </sage>

    <p>
      Our next example involves polynomials.
      At first this looks like a different problem,
      but it's essentially the same once we set it up.
    </p>

    <exercise>
      <statement>
        <p>
          Determine whether <m>p(x)=1+x+4x^2</m> belongs to
          <m>\spn\{1+2x-x^2,3+5x+2x^2\}</m>.
        </p>
      </statement>
    </exercise>

    <p>
      We seek scalars <m>s,t</m> such that
      <me>
        s(1+2x-2x^2)+t(3+5x+2x^2)=1+x+4x^2
      </me>.
      On the left-hand side, we expand and gather terms:
      <me>
        (s+3t)+(2s+5t)x+(-2s+2t)x^2 = 1+x+4x^2
      </me>.
      These two polynomials are equal if and only if we can solve the system
      <md>
        <mrow>s+3t \amp = 1 </mrow>
        <mrow>2s+5t \amp =1</mrow>
        <mrow> -2s+2t \amp =4</mrow>
      </md>.
    </p>

    <p>
      We can solve this computationally using matrices again.
    </p>
    <sage>
      <input>
        M = Matrix(3,3,[1,3,1,2,5,1,-2,2,4])
        M.rref()
      </input>
    </sage>
    <p>
      So, what's the answer? Is <m>p(x)</m> in the span?
      Can we determine what polynomials are in the span?
      Let's consider a general polynomial <m>q(x)=a+bx+cx^2</m>.
      A bit of thought tells us that the coefficients <m>a,b,c</m>
      should replace the constants <m>1,1,4</m> above.
    </p>

    <sage>
      <input>
        a, b, c = symbols('a b c', real = True, constant = True)
        N = Matrix(3,3,[1,3,a,2,5,b,-2,2,c])
        N
      </input>
    </sage>

    <p>
      Asking the computer to reduce this matrix won't produce the desired result.
      (Maybe someone can figure out how to define the constants in a way that works?)
      But we can always specify row operations.
    </p>

    <sage>
      <input>
        N1 = N.elementary_row_op(op='n-&gt;n+km',row=1,k=-2,row2=0)
        N1
      </input>
    </sage>

    <p>
      In the <c>elementary_row_op</c> function called above,
      we are asking the computer to change row <m>1</m> (the second row)
      by adding <m>-2</m> times row <m>0</m> )the first row).
      See <xref ref="subsec-sympy-matrix">Section</xref> for complete details on this syntax.
    </p>

    <p>
      Now we repeat. Here is another cell to work with:
    </p>

    <sage>

    </sage>

    <p>
      One of the reasons we care about linear combinations and span is that it gives us an easy means of generating subspaces,
      as the following theorem suggests.
    </p>

    <theorem xml:id="thm-span-is-subspace">
      <statement>
        <p>
          Let <m>V</m> be a vector space, and let <m>\vv_1,\vv_2,\ldots, \vv_k</m> be vectors in <m>V</m>. Then:
          <ol>
            <li>
              <p>
                <m>U=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}</m> is a subspace of <m>V</m>.
              </p>
            </li>

            <li>
              <p>
                <m>U</m> is the <em>smallest</em> subspace of  <m>V</m> containing <m>\vv_1,\ldots, \vv_k</m>,
                in the sense that if <m>W\subseteq V</m> is a subspace and <m>\vv_1,\ldots, \vv_k\in W</m>,
                then <m>U\subeteq W</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          Let <m>U=\spn\{\vv_1,\vv_2,\ldots, \vv_k\}</m>. Then <m>0\in U</m>,
          since <m>0=0\vv_1+0\vv_2+\cdots + 0\vv_k</m>.
          If <m>\uu=a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k</m> and <m>\ww=b_1\vv_1+b_2\vv_2+\cdots +b_k\vv_k</m>
          are vectors in <m>U</m>, then
          <md>
            <mrow>\uu+\ww \amp =(a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k)+(b_1\vv_1+b_2\vv_2+\cdots +b_k\vv_k)</mrow>
            <mrow> \amp = (a_1+b_1)\vv_1+(a_2+b_2)\vv_2+\cdots + (a_k+b_k)\vv_k</mrow>
          </md>
          is in <m>U</m>, and
          <md>
            <mrow>c\uu \amp =c(a_1\vv_1+a_2\vv_2+\cdots +a_k\vv_k)</mrow>
            <mrow> \amp =(ca_1)\vv_1+(ca_2)\vv_2+\cdots + (ca_k)\vv_k</mrow>
          </md>
          is in <m>U</m>, so by <xref ref="thm-subspace-test">Theorem</xref>,
          <m>U</m> is a subspace.
        </p>

        <p>
          To see that <m>U</m> is the smallest subspace containing <m>\vv_1,\ldots, \vv_k</m>,
          we need only note that if <m>\vv_1,\ldots, \vv_k\in W</m>,
          where <m>W</m> is a subspace, then since <m>W</m> is closed under scalar multiplication,
          we know that <m>c_1\vv_1,\ldots, c_k\vv_k</m> for any scalars <m>c_1,\ldots, c_k</m>,
          and since <m>W</m> is closed under addition, <m>c_1\vv_1+\cdots+c_k\vv_k\in W</m>.
          Thus, <m>W</m> contains all linear combinations of <m>\vv_1,\ldots, \vv_k</m>,
          which is to say that <m>W</m> contains <m>U</m>.
        </p>
      </proof>

    </theorem>

    <p>
      We end this section with a few non-computational, but useful results,
      which will be left as exercises to be done in class, or by the reader.
    </p>

    <exercise>
      <statement>
        <p>
          Let <m>V</m> be a vector space, and let <m>X,Y\subseteq V</m>.
          Show that if <m>X\subseteq Y</m>, then <m>\spn X \subseteq \spn Y</m>.
        </p>
      </statement>
    </exercise>

    <exercise>
      <statement>
        <p>
          Can <m>\{(1,2,0), (1,1,1)</m> span <m>\{(a,b,0)\,|\, a,b \in\R\}</m>?
        </p>
      </statement>
    </exercise>

    <theorem xml:id="theorem-surplus-span">
      <statement>
        <p>
          Let <m>V</m> be a vector space, and let <m>\vv_1,\ldots, \vv_k\in V</m>.
          If <m>\uu\in \spn\{\vv_1,\ldots, \vv_k\}</m>, then
          <me>
            \spn\{\uu,\vv_1,\ldots, \vv_k\} = \spn\{\vv_1,\ldots, \vv_k\}
          </me>.
        </p>
      </statement>
    </theorem>

    <p>
      The moral of <xref ref="theorem-surplus-span">Theorem</xref>
      is that one vector in a set is a linear combination of the others,
      we can remove it from the set without affecting the span.
      This suggests that we might want to look for the most <q>efficient</q>
      spanning sets <ndash /> those in which no vector in the set can be written in terms of the others.
      Such sets are called <term>linearly independent</term>,
      and they are the subject of <xref ref="sec-independence">Section</xref>.
    </p>
  </section>

  <section xml:id="sec-independence">
    <title>Linear Independence</title>

    <p>
      In any vector space <m>V</m>, we say that a set of vectors
      <me>
        \{\vv_1,\ldots,\vv_2\}
      </me>
      is <term>linearly independent</term> if for any scalars <m>c_1,\ldots, c_k</m>
      <me>
        c_1\vv_1+\cdots + c_k\vv_k = \mathbf{0} \quad\Rightarrow\quad c_1=\cdots = c_k=0
      </me>.
    </p>

    <p>
      This means that no vector in the set can be written as a linear combination of the other vectors in that set.
      We will soon see that when looking for vectors that span a subspace,
      it is especially useful to find a spanning set that is also linearly independent.
      The following lemma establishes some basic properties of independent sets.
    </p>

    <lemma>
      <statement>
        <p>
          In any vector space <m>V</m>:
          <ol>
            <li>
              <p>
                If <m>\vv\neq\mathbf{0}</m>, then <m>\{\vv\}</m> is indenpendent.
              </p>
            </li>
            <li>
              <p>
                If <m>S\subseteq V</m> contains the zero vector, then <m>S</m> is dependent.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </lemma>

    <p>
      The definition of linear independence tells us that if <m>\{\vv_1,\ldots, \vv_k\}</m>
      is an independent set of vectors, then there is only one way to write <m>\mathbf{0}</m>
      as a linear combination of these vectors; namely,
      <me>
        \mathbf{0} = 0\vv_1+0\vv_2+\cdots +0\vv_k
      </me>.
      In fact, more is true: <em>any</em> vector in the span of a linearly independent set
      can be written in only one way as a linear combination of those vectors.
    </p>

    <p>
      Computationally, questions about linear independence are just questions
      about homogeneous systems of linear equations.
      For example, suppose we want to know if the vectors
      <me>
        \uu=\bbm 1\\-1\\4\ebm, \vv=\bbm 0\\2\\-3\ebm, \ww=\bbm 4\\0\\-3\ebm
      </me>
      are linearly independent in <m>\mathbb{R}^3</m>.
      This question leads to the vector equation
      <me>
        x\uu+y\vv+z\ww=\mathbf{0}
      </me>,
      which becomes the matrix equation
      <me>
        \bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm\bbm x\\y\\z\ebm = \bbm 0\\0\\0\ebm
      </me>.
    </p>

    <p>
      We now apply some basic theory from linear algebra.
      A unique (and therefore, trivial) solution to this system is guaranteed if the matrix
      <m>A = \bbm 1\amp0\amp4\\-1\amp2\amp0\\4\amp-3\amp-3\ebm</m> is invertible,
      since in that case we have <m>\bbm x\\y\\z\ebm = A^{-1}\mathbf{0} = \mathbf{0}</m>.
    </p>

    <p>
      This approach is problematic, however, since it won't work if we have 2 vectors, or 4.
      Instead, we look at the reduced row-echelon form.
      A unique solution corresponds to having a leading 1 in each column of <m>A</m>.
      Let's check this condition.
    </p>

    <sage>
      <input>
        from sympy import *
        init_printing()
      </input>
    </sage>

    <sage>
      <input>
        A = Matrix(3,3,[1,0,4,-1,2,0,4,-3,-3])
        A.rref()
      </input>
    </sage>

    <p>
      One observation is useful here, and will lead to a better understanding of independence.
      First, it would be impossible to have 4 or more linearly independent vectors in <m>\mathbb{R}^3</m>.
      Why? (How many leading ones can you have in a <m>3\times 4</m> matrix?)
      Second, having two or fewer vectors makes it more likely that the set is independent.
    </p>

    <p>
      The largest set of linearly independent vectors possible in <m>\mathbb{R}^3</m> contains three vectors.
      You might have also observed that the smallest number of vectors needed to span <m>\mathbb{R}^3</m> is 3.
      Hmm. Seems like there's something interesting going on here. But first, some more computation.
    </p>

    <exercise>
      <statement>
        <p>
          Determine whether the set <m>\left\{\bbm 1\\2\\0\ebm, \bbm -1\\0\\3\ebm,\bbm -1\\4\\9\ebm\right\}</m>
          is linearly independent in <m>\R^3</m>.
        </p>
      </statement>
    </exercise>

    <p>
      Again, we set up a matrix and reduce:
    </p>

    <sage>
      <input>
        A = Matrix(3,3,[1,-1,-1,2,0,4,0,3,9])
        A.rref()
      </input>
    </sage>

    <p>
      Notice that this time we don't get a unique solution, so we can conclude that these vectors are <em>not</em> independent.
      Furthermore, you can probably deduce from the above that we have <m>2\vv_1+3\vv_2-\vv_3=\mathbf{0}</m>.
      Now suppose that <m>\ww\in\spn\{\vv_1,\vv_2,\vv_3\}</m>.
      In how many ways can we write <m>\ww</m> as a linear combination of these vectors?
    </p>

    <exercise>
      <statement>
        <p>
          Which of the following subsets of <m>P_2(\mathbb{R})</m> are independent?
          <md>
            <mrow>\text{(a) } S_1 = \{x^2+1, x+1, x\}</mrow>
            <mrow>\text{(b) } S_2 = \{x^2-x+3, 2x^2+x+5, x^2+5x+1\}</mrow>
          </md>
        </p>
      </statement>
    </exercise>

    <p>
      In each case, we set up the defining equation for independence, collect terms,
      and then analyze the resulting system of equations.
      (If you work with polynomials often enough,
      you can probably jump straight to the matrix.
      For now, let's work out the details.)
    </p>

    <p>Suppose
      <me>
        r(x^2+1)+s(x+1)+tx = 0
      </me>.
      Then <m>rx^2+(s+t)x+(r+s)=0=0x^2+0x+0</m>, so
      <md>
        <mrow>r \amp =0</mrow>
        <mrow>s+t \amp =0</mrow>
        <mrow>r+s\amp =0</mrow>
      </md>.
      And in this case, we don't even need to ask the computer.
      The first equation gives <m>r=0</m> right away,
      and putting that into the third equation gives <m>s=0</m>,
      and the second equation then gives <m>t=0</m>.
    </p>

    <p>
      Since <m>r=s=t=0</m> is the only solution, the set is independent.
    </p>

    <p>
      Repeating for <m>S_2</m> leads to the equation
      <me>
        (r+2s+t)x^2+(-r+s+5t)x+(3r+5s+t)1=0.
      </me>
      This gives us:
    </p>

    <sage>
      <input>
        A = Matrix(3,3,[1,2,1,-1,1,5,3,5,1])
        A.rref()
      </input>
    </sage>

    <exercise>
      <statement>
        <p>
          Determine whether or not the set
          <me>
            \left\{\bbm -1\amp 0\\0\amp -1\ebm, \bbm 1\amp -1\\ -1\amp 1\ebm,
                   \bbm 1\amp 1\\1\amp 1\ebm, \bbm 0\amp -1\\-1\amp 0\ebm\right\}
          </me>
          is linearly independent in <m>M_2(\mathbb{R})</m>.
        </p>
      </statement>
    </exercise>

    <p>
      Again, we set a linear combination equal to the zero vector, and combine:
      <md>
        <mrow>a\bbm -1\amp 0\\0\amp -1\ebm +b\bbm 1\amp -1\\ -1\amp 1\ebm
          +c\bbm 1\amp 1\\1\amp 1\ebm +d \bbm 0\amp -1\\-1\amp 0\ebm = \bbm 0\amp 0\\ 0\amp 0\ebm</mrow>
        <mrow>\bbm -a+b+c\amp -b+c-d\\-b+c-d\amp -a+b+c\ebm = \bbm 0\amp 0\\0\amp 0\ebm</mrow>
      </md>.
    </p>

    <p>
      We could proceed, but we might instead notice right away that equations 1 and 4 are identical,
      and so are equations 2 and 3.
      With only two distinct equations and 4 unknowns, we're certain to find nontrivial solutions.
    </p>
  </section>

  <section xml:id="sec-dimension">
    <title>Basis and dimension</title>

    <p>
      Next, we begin with an important result, sometimes known as the <q>Fundamental Theorem</q>:
    </p>

    <theorem xml:id="theorem-steinitz">
      <title>Fundamental Theorem (Steinitz Exchange Lemma)</title>
      <statement>
        <p>
          Suppose <m>V = \spn\{\vv_1,\ldots, \vv_n\}</m>.
          If <m>\{\ww_1,\ldots, \ww_m\}</m> is a linearly independent set of vectors in <m>V</m>,
          then <m>m\leq n</m>.
        </p>
      </statement>
    </theorem>

    <p>
      If a set of vectors spans a vector space <m>V</m>, and it is not independent,
      we observed that it is possible to remove a vector from the set and still span <m>V</m>.
      This suggests that spanning sets that are also linearly independent are of particular importance,
      and indeed, they are important enough to have a name.
    </p>

    <definition xml:id="def-basis">
      <statement>
        <p>
          Let <m>V</m> be a vector space. A set <m>\mathcal{B}=\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m>
          is called a <term>basis</term> of <m>V</m> if <m>\mathcal{B}</m> is linearly independent,
          and <m>\operatorname{span}\mathcal{B} = V</m>.
        </p>
      </statement>
    </definition>

    <p>
      The importance of a basis is that vector vector <m>\vv\in V</m> can be written in terms of the basis,
      and this expression as a linear combination of basis vectors is <em>unique</em>.
      Another important fact is that every basis has the same number of elements.
    </p>

    <theorem xml:id="thm-invariance">
      <title>Invariance Theorem</title>

      <statement>
        <p>
          If <m>\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m> and <m>\{\mathbf{f}_1,\ldots, \mathbf{f}_m\}</m>
          are both bases of a vector space <m>V</m>, then <m>m=n</m>.
        </p>
      </statement>
    </theorem>

    <p>
      Suppose <m>V=\spn\{\vv_1,\ldots,\vv_n\}</m>.
      If this set is not linearly independent, <xref ref="theorem-surplus-span">Theorem</xref>
      tells us that we can remove a vector from the set, and still span <m>V</m>.
      We can repeat this procedure until we have a linearly independent set of vectors, which will then be a basis.
      These results let us make a definition.
    </p>

    <definition xml:id="def-dimension">
      <statement>
        <p>
          Let <m>V</m> be a vector space.
          If <m>V</m> can be spanned by a finite number of vectors,
          then we call <m>V</m> a <term>finite-dimensional</term> vector space.
          If <m>V</m> is finite-dimensional (and non-trivial), and <m>\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m>
          is a basis of <m>V</m>, we say that <m>V</m> has <term>dimension</term> <m>n</m>,
          and write
          <me>
            \dim V = n
          </me>.
          If <m>V</m> cannot be spanned by finitely many vectors, we say that <m>V</m>
          is <term>infinite-dimensional</term>.
        </p>
      </statement>
    </definition>

    <exercise>
      <statement>
        <p>
          Find a basis for <m>U=\{X\in M_{22} \,|\, XA = AX\}</m>, if <m>A = \bbm 1\amp 1\\0\amp 0\ebm</m>
        </p>
      </statement>
    </exercise>

    <exercise>
      <statement>
        <p>
          Show that the following are bases of <m>\R^3</m>:
          <ul>
            <li><m>\{(1,1,0),(1,0,1),(0,1,1)\}</m></li>
            <li><m>\{(-1,1,1),(1,-1,1),(1,1,-1)</m></li>
          </ul>
        </p>
      </statement>
    </exercise>

    <sage>
      <input>
        from sympy import *
        init_printing()
      </input>
    </sage>

    <sage>

    </sage>

    <exercise>
      <statement>
        <p>
          Show that the following is a basis of <m>M_{22}</m>:
          <me>
            \left\{\bbm 1\amp 0\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 1\\0\amp 1\ebm, \bbm 1\amp 0\\0\amp 0\ebm\right\}
          </me>.
        </p>
      </statement>
    </exercise>

    <sage>

    </sage>

    <exercise>
      <statement>
        <p>
          Show that <m>\{1+x,x+x^2,x^2+x^3,x^3\}</m> is a basis for <m>P_3</m>.
        </p>
      </statement>
    </exercise>

    <sage>

    </sage>

    <exercise>
      <statement>
        <p>
          Find a basis and dimension for the following subpaces of <m>P_2</m>:
          <ol label='a'>
            <li>
              <m>U_1 = \{a(1+x)+b(x+x^2)\,|\, a,b\in\R\}</m>
            </li>
            <li>
              <m>U_2=\{p(x)\in P_2 \,|\, p(1)=0\}</m>
            </li>
            <li>
              <m>U_3 = \{p(x)\in P_2 \,|\, p(x)=p(-x)\}</m>
            </li>
          </ol>
        </p>
      </statement>
      <solution>
        <p>
          <ol label='a'>
            <li>
              <p>
                By definition, <m>U_1 = \spn \{1+x,x+x^2\}</m>,
                and these vectors are independent, since neither is a scalar multiple of the other.
                Since there are two vectors in this basis, <m>\dim U_1 = 2</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>p(1)=0</m>, then <m>p(x)=(x-1)q(x)</m> for some polynomial <m>q</m>.
                Since <m>U_2</m> is a subspace of <m>P_2</m>, the degree of <m>q</m> is at most 2.
                Therefore, <m>q(x)=ax+b</m> for some <m>a,b\in\R</m>, and
                <me>
                  p(x) = (x-1)(ax+b) = a(x^2-x)+b(x-1)
                </me>.
                Since <m>p</m> was arbitrary, this shows that <m>U_2 = \spn\{x^2-x,x-1\}</m>.
              </p>

              <p>
                The set <m>\{x^2-x,x-1\}</m> is also independent,
                since neither vector is a scalar multiple of the other.
                Therefore, this set is a basis, and <m>\dim U_2=2</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>p(x)=p(-x)</m>, then <m>p(x)</m> is an even polynomial,
                and therefore <m>p(x)=a+bx^2</m> for <m>a,b\in\R</m>.
                (If you didn't know this it's easily verified: if
                <me>
                  a+bx+cx^2 = a+b(-x)+c(-x)^2
                </me>,
                we can immediately cancel <m>a</m> from each side,
                and since <m>(-x)^2=x^2</m>, we can cancel <m>cx^2</m> as well.
                This leaves <m>bx=-bx</m>, or <m>2bx=0</m>, which implies that <m>b=0</m>.)
              </p>

              <p>
                It follows that the set <m>\{1,x^2\}</m> spans <m>U_3</m>,
                and since this is a subset of the standard basis <m>\{1,x,x^2\}</m> of <m>P_2</m>,
                it must be independent, and is therefore a basis of <m>U_3</m>,
                letting us conclude that <m>\dim U_3=2</m>.
              </p>
            </li>
          </ol>
        </p>
      </solution>
    </exercise>

    <p>
      We've noted a few times now that if <m>\ww\in\spn\{\vv_1,\ldots, \vv_n\}</m>,
      then
      <me>
        \spn\{\ww,\vv_1,\ldots, \vv_n\}=\spn\{\vv_1,\ldots, \vv_n\}
      </me>
      If <m>\ww</m> is not in the span, we can make another useful observation:
    </p>

    <lemma xml:id="lemma-independent">
      <title>Independent Lemma</title>
      <statement>
        <p>
          Suppose <m>\{\vv_1,\ldots, \vv_n\}</m> is a linearly independent set of vectors in a vector space <m>V</m>.
          If <m>\uu\in V</m> but <m>\uu\notin \spn\{\vv_1,\ldots, \vv_n\}</m>,
          then <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is independent.
        </p>
      </statement>
      <proof>
        <p>
          Suppose <m>S=\{\vv_1,\ldots, \vv_n\}</m> is independent,
          and that <m>\uu\notin\spn S</m>. Suppose we have
          <me>
            a\uu+c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}
          </me>
          for scalars <m>a,c_1,\ldots, c_n</m>. We must have <m>a=0</m>;
          otherwise, we can multiply by <m>\frac1a</m> and rearrange to obtain
          <me>
            \uu = -\frac{c_1}{a}\vv_1-\cdots -\frac{c_n}{a}\vv_n
          </me>,
          but this would mean that <m>\uu\in \spn S</m>, contradicting our assumption.
        </p>

        <p>
          With <m>a=0</m> we're left with
          <me>
            c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}
          </me>,
          and since we assumed that the set <m>S</m> is independent,
          we must have <m>c_1=c_2=\cdots=c_n=0</m>. Since we already showed <m>a=0</m>,
          this shows that <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is independent.
        </p>
      </proof>

    </lemma>
    <p>
      This is, in fact, an <q>if and only if</q> result.
      If <m>\uu\in\spn\{\vv_1,\ldots, \vv_n\}</m>, then <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is not independent.
      Above, we argued that if <m>V</m> is finite dimensional,
      then any spanning set for <m>V</m> can be reduced to a basis.
      It probably won't surprise you that the following is also true.
    </p>

    <lemma xml:id="lem-enlarge-independent">
      <statement>
        <p>
          Let <m>V</m> be a finite-dimensional vector space,
          and let <m>U</m> be any subspace of <m>V</m>.
          Then any independent set of vectors <m>\{\uu_1,\ldots, \uu_k\}</m> in <m>U</m>
          can be enlarged to a basis of <m>U</m>.
        </p>
      </statement>
      <proof>
        <p>
          This follows from <xref ref="lemma-independent"/>.
          If our independent set of vectors spans <m>U</m>, then it's a basis and we're done.
          If not, we can find some vector not in the span,
          and add it to our set to obtain a larger set that is still independent.
          We can continue adding vectors in this fashion until we obtain a spanning set.
        </p>

        <p>
          Note that this process <em>must</em> terminate: <m>V</m> is finite-dimensional,
          so there is a finite spanning set for <m>V</m>, and therefore for <m>U</m>.
          By the Steinitz Exchange lemma, our independent set cannot get larger than this spanning set.
        </p>
      </proof>

    </lemma>

    <theorem xml:id="thm-basis-exist">
      <statement>
        <p>
          Any finite-dimensional vector space <m>V</m> has a basis.
          Moreover:
          <ol>
            <li>
              <p>
                If <m>V</m> can be spanned by <m>m</m> vectors,
                then <m>\dim V\leq m</m>.
              </p>
            </li>
            <li>
              <p>
                Given an independent set <m>I</m> in <m>V</m>,
                and a basis <m>\mathcal{B}</m> of <m>V</m>,
                we can enlarge <m>I</m> to a basis of <m>V</m> by adding elements of <m>\mathcal{B}</m>.
              </p>
            </li>
          </ol>
        </p>

        <p>
          If <m>U</m> is a subspace of <m>V</m>, then:
          <ol>
            <li>
              <p>
                <m>U</m> is finite-dimensional, and <m>\dim U\leq \dim V</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>\dim U = \dim V</m>, then <m>U=V</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
    </theorem>

    <exercise>
      <statement>
        <p>
          Find a basis of <m>M_{22}(\R)</m> that contains the vectors
          <me>
            \vv=\bbm 1\amp 1\\0\amp 0\ebm, \ww=\bbm 0\amp 1\\0\amp 1\ebm
          </me>.
        </p>
      </statement>
      <solution>
        <p>
          By the previous theorem, we can form a basis by adding vectors from the standard basis
          <me>
            \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}
          </me>.
          It's easy to check that <m>\bbm 1\amp 0\\0\amp 0\ebm</m> is not in the span of <m>\{\vv,\ww\}</m>.
          To get a basis, we need one more vector.
          Observe that all three of our vectors so far have a zero in the <m>(2,1)</m>-entry.
          Thus, <m>\bbm 0\amp 0\\1\amp 0\ebm</m> cannot be in the span of the first three vectors,
          and adding it gives us our basis.
        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Extend the set <m>\{1+x,x+x^2,x-x^3\}</m> to a basis of <m>P_3(\R)</m>.
        </p>
      </statement>
      <solution>
        <p>
          Again, we only need to add one vector from the standard basis
          <m>\{1,x,x^2,x^3\}</m>, and it's not too hard to check that any of them will do.
        </p>
      </solution>
    </exercise>

    <exercise>
      <statement>
        <p>
          Give two examples of infinite-dimensional vector spaces.
          Support your answer.
        </p>
      </statement>
    </exercise>

    <p>
      Let's recap our results so far:
      <ul>
        <li>
          <p>
            A basis for a vector space <m>V</m> is an independent set of vectors that spans <m>V</m>.
          </p>
        </li>
        <li>
          <p>
            The number of vectors in any basis of <m>V</m> is a constant, called the dimension of <m>V</m>.
          </p>
        </li>
        <li>
          <p>
            The number of vectors in any independent set is always less than or equal to the number of vectors in a spanning set.
          </p>
        </li>
        <li>
          <p>
            In a finite-dimensional vector space, any independent set can be enlarged to a basis,
            and any spanning set can be cut down to a basis by deleting vectors that are in the span of the remaining vectors.
          </p>
        </li>
      </ul>
      Another important aspect of dimension is that it reduces many problems,
      such as determining equality of subspaces, to counting problems.
    </p>

    <theorem xml:id="thm-subspace-dim">
      <statement>
        <p>
          Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
          <ol>
            <li>
              <p>
                If <m>U\subseteq W</m>, then <m>\dim U\leq \dim W</m>.
              </p>
            </li>
            <li>
              <p>
                If <m>U\subseteq W</m> and <m>\dim U=\dim W</m>, then <m>U=W</m>.
              </p>
            </li>
          </ol>
        </p>
      </statement>
      <proof>
        <p>
          <ol>
            <li>
              <p>
                Suppose <m>U\subseteq W</m>, and let <m>B=\{\uu_1,\ldots, \uu_k\}</m>
                be a basis for <m>U</m>. Since <m>B</m> is a basis, it's independent.
                And since <m>B\subseteq U</m> and <m>U\subseteq W</m>, <m>B\subseteq W</m>.
                Thus, <m>B</m> is an independent subset of <m>W</m>,
                and since any basis of <m>W</m> spans <m>W</m>,
                we know that <m>\dim U = k \leq \dim W</m>,
                by <xref ref="theorem-steinitz">Theorem</xref>.
              </p>
            </li>
            <li>
              <p>
                Suppose <m>U\subseteq W</m> and <m>\dim U = \dim W</m>.
                Let <m>B</m> be a basis for <m>U</m>.
                As above, <m>B</m> is an independent subset of <m>W</m>.
                If <m>W\neq U</m>, then there is some <m>\ww\in W</m> with <m>\ww\notin U</m>.
                But <m>U=\spn B</m>, so that would mean that <m>B\cup \{\ww\}</m>
                is a linearly independent set containing <m>\dim U+1</m> vectors.
                This is impossible, since <m>\dim W=\dim U</m>,
                so no independent set can contain more than <m>\dim U</m> vectors.
              </p>
            </li>
          </ol>
        </p>
      </proof>

    </theorem>

    <p>
      An even more useful counting result is the following:
    </p>

    <theorem xml:id="thm-half-the-work">
      <statement>
        <p>
          Let <m>V</m> be an <m>n</m>-dimensional vector space.
          If the set <m>S</m> contains <m>n</m> vectors,
          then <m>S</m> is independent if and only if <m>\spn S=V</m>.
        </p>
      </statement>
      <proof>
        <p>
          If <m>S</m> is independent, then it can be extended to a basis <m>B</m> with <m>S\subseteq B</m>.
          But <m>S</m> and <m>B</m> both contain <m>n</m> vectors (since <m>\dim V=n</m>),
          so we must have <m>S=B</m>.
        </p>

        <p>
          If <m>S</m> spans <m>V</m>, then <m>S</m> must contain a basis <m>B</m>,
          and as above, since <m>S</m> and <m>B</m> contain the same number of vectors,
          they must be equal.
        </p>
      </proof>

    </theorem>

    <paragraphs xml:id="pars-subspace-combine">
      <title>New subspaces from old</title>
      <p>
        On your first assignment, you showed that if <m>U</m> and <m>W</m> are subspaces of a vector space <m>V</m>,
        then the intersection <m>U\cap W</m> is also a subspace of <m>V</m>.
        You also showed that the union <m>U\cup W</m> is generally not a subspace,
        unless one subspace is contained in the other
        (in which case the union is just the larger of the two subspaces we already have).
      </p>

      <p>
        In class, we discussed the fact that the right way to define a subspace containing both <m>U</m> and <m>W</m>
        is using their <term>sum</term>: we define the sum <m>U+W</m> of two subspaces by
        <me>
          U+W = \{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}
        </me>.
        We proved that <m>U+W</m> is again a subspace of <m>V</m>.
      </p>

      <p>
        If <m>U\cap W = \{\mathbf{0}\}</m>, we say that the sum is a <term>direct sum</term>,
        and write it as <m>U\oplus W</m>.
        The following theorem might help us understand why direct sums are singled out for special attention:
      </p>
      <theorem xml:id="thm-sum-dimension">
        <statement>
          <p>
            Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
            Then <m>U+W</m> is finite-dimensional, and
            <me>
              \dim(U+W)=\dim U + \dim W - \dim(U\cap W)
            </me>.
          </p>
        </statement>

      </theorem>

      <p>
        If the sum is direct, then we have simply <m>\dim(U\oplus W) = \dim U + \dim W</m>.
        The other reason why direct sums are preferable, is that any <m>\vv\in U\oplus W</m>
        can be written <em>uniquely</em> as <m>\vv=\uu+\ww</m>
        where <m>\uu\in U</m> and <m>\ww\in W</m>.
      </p>

      <theorem xml:id="thm-direct-sum">
        <statement>
          <p>
            For any subspaces <m>U,W</m> of a vector space <m>V</m>,
            <m>U\cap W = \{\mathbf{0}\}</m> if and only if for every <m>\vv\in U+W</m>
            there exist unique <m>\uu\in U, \ww\in W</m> such that <m>\vv=\uu+\ww</m>.
          </p>
        </statement>
        <proof>
          <p>
            Suppose that <m>U\cap W = \{\mathbf{0}\}</m>, and suppose that we have
            <m>\vv = \uu_1+\ww_1 = \uu_2+\ww_2</m>,
            for <m>\uu_1,\uu_2\in U,\ww_1,\ww_2\in W</m>.
            Then <m>\mathbf{0}=(\uu_1-\uu_2)+(\ww_1-\ww_2)</m>,
            which implies that
            <me>
              \ww_1-\ww_2 = -(\uu_1-\uu_2)
            </me>.
            Now, <m>\uu=\uu_1-\uu_2\in U</m>,
            since <m>U</m> is a subspace, and similarly,
            <m>\ww=\ww_1-\ww_2\in W</m>.
            But we also have <m>\ww=-\uu</m>, which implies that <m>\ww\in U</m>.
            Therefore, <m>\ww\in U\cap W</m>, which implies that <m>\ww=\mathbf{0}</m>,
            so <m>\ww_1=\ww_2</m>.
            But we must also then have <m>\uu=\mathbf{0}</m>, so <m>\uu_1=\uu_2</m>.
          </p>

          <p>
            Conversely, suppose that every <m>\vv\in U+W</m> can be written uniquely as <m>\vv=vec{u}+\ww</m>,
            with <m>\uu\in U</m> and <m>\ww\in W</m>. Suppose that <m>\mathbf{a}\in U\cap W</m>.
            Then <m>\mathbf{a}\in U</m> and <m>\mathbf{a}\in W</m>, so we also have <m>-\mathbf{a}\in W</m>,
            since <m>W</m> is a subspace.
            But then <m>\mathbf{0}=\mathbf{a}+(-\mathbf{a})</m>, where <m>\mathbf{a}\in U</m> and <m>-\mathbf{a}\in W</m>.
            On the other hand, <m>\mathbf{0}=\mathbf{0}+\mathbf{0}</m>,
            and <m>\mathbf{0}</m> belongs to both <m>U</m> and <m>W</m>. It follows that <m>\mathbf{a}=\mathbf{0}</m>.
            Since <m>\mathbf{a}</m> was arbitrary, <m>U\cap W = \{\mathbf{0}\}</m>.
          </p>
        </proof>

      </theorem>

      <p>
        We end with one last application of the theory we've developed on the existence of a basis for a finite-dimensional vector space.
        As we continue on to later topics, we'll find that it is often useful to be able to decompose a vector space into a direct sum of subspaces.
        Using bases, we can show that this is always possible.
      </p>

      <theorem xml:id="thm-construct-complement">
        <statement>
          <p>
            Let <m>V</m> be a finite-dimensonal vector space, and let <m>U</m> be any subspace of <m>V</m>.
            Then there exists a subspace <m>W\subseteq V</m> such that <m>U\oplus W = V</m>.
          </p>
        </statement>
        <proof>
          <p>
            Let <m>\{\uu_1,\ldots, \uu_m\}</m> be a basis of <m>U</m>.
            Since <m>U\subseteq W</m>, the set <m>\{\uu_1,\ldots, \uu_m\}</m>
            is a linearly independent subset of <m>V</m>.
            Since any linearly independent set can be extended to a basis of <m>V</m>,
            there exist vectors <m>\ww_1,\ldots,\ww_n</m> such that
            <me>
              \{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}
            </me>
            is a basis of <m>V</m>.
          </p>

          <p>
            Now, let <m>W = \spn\{\ww_1,\ldots, \ww_n\}</m>.
            Then <m>W</m> is a subspace, and <m>\{\ww_1,\ldots, \ww_n\}</m>
            is a basis for <m>W</m>. (It spans, and must be independent since it's a subset of an independent set.)
          </p>

          <p>
            Clearly, <m>U+W=V</m>, since <m>U+W</m> contains the basis for <m>V</m> we've constructed.
            To show the sum is direct, it suffices to show that <m>U\cap W = \{\mathbf{0}\}</m>.
            To that end, suppose that <m>\vv\in U\cap W</m>.
            Since <m>\vv\in U</m>, we have
            <me>
              \vv=a_1\uu_1+\cdots +a_m\uu_m
            </me>
            for scalars <m>a_1,\ldots, a_m</m>. Since <m>\vv\in W</m>, we can write
            <me>
              \vv=b_1\ww_1+\cdots + b_n\ww_n
            </me>
            for scalars <m>b_1,\ldots, b_n</m>. But then
            <me>
              \mathbf{0}=\vv-\vv=a_1\uu_1+\cdots a_m\uu_m-b_1\ww_1-\cdots -b_n\ww_n.
            </me>
            Since <m>\{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}</m> is a basis for <m>V</m>,
            it's independent, and therefore, all of the <m>a_i,b_j</m> must be zero, and therefore, <m>\vv=\mathbf{0}</m>.

          </p>
        </proof>


      </theorem>

      <p>
        The subspace <m>W</m> constructed in the theorem above is called a <term>complement</term> of <m>U</m>.
        It is not unique; indeed, it depends on the choice of basis vectors.
        For example, if <m>U</m> is a one-dimensional subspace of <m>\R^2</m>; that is, a line,
        then any other non-parallel line through the origin provides a complement of <m>U</m>.
        Later we will see that an especially useful choice of complement is the <term>orthogonal complement</term>.
      </p>

    </paragraphs>


  </section>
  </chapter>
